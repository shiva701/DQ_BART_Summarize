Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pip in /home/sshukla7/.local/lib/python3.10/site-packages (23.1.2)
Defaulting to user installation because normal site-packages is not writeable
here!!
05/08/2023 18:46:56 - INFO - __main__ -   Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/08/2023 18:46:56 - WARNING - __main__ -   Namespace(dataset_name=None, dataset_config_name=None, train_file='eli5_train_traditional_chinese.csv', test_file='eli5_test_traditional_chinese.csv', validation_file='eli5_validation_traditional_chinese.csv', ignore_pad_token_for_loss=True, max_source_length=1024, source_prefix=None, preprocessing_num_workers=None, overwrite_cache=None, max_target_length=128, val_max_target_length=None, pad_to_max_length=False, model_name_or_path='facebook/bart-base', config_name=None, tokenizer_name=None, text_column=None, summary_column=None, use_slow_tokenizer=False, per_device_train_batch_size=8, per_device_eval_batch_size=4, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=4, max_train_steps=None, gradient_accumulation_steps=2, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.05, output_dir='./output_eli5Translated/2_8_1_1_4_5e-05_fp16', seed=42, model_type=None, teacher_model='facebook/bart-base', student_model='facebook/bart-base', pred_distill=True, intermediate_distill=True, weight_bits=2, input_bits=8, clip_val=2.5, length_penalty=1.0, max_length=160, min_length=50, num_beams=4, do_train=True, do_test=True, test_teacher=False, distill_encoder=1, distill_decoder=1, log_steps=20, local_rank=0, weighted=False, new_distill_map=False, task_weight=1, logits_weight=1, hid_weight=1)


the arguments are:  Namespace(dataset_name=None, dataset_config_name=None, train_file='eli5_train_traditional_chinese.csv', test_file='eli5_test_traditional_chinese.csv', validation_file='eli5_validation_traditional_chinese.csv', ignore_pad_token_for_loss=True, max_source_length=1024, source_prefix=None, preprocessing_num_workers=None, overwrite_cache=None, max_target_length=128, val_max_target_length=None, pad_to_max_length=False, model_name_or_path='facebook/bart-base', config_name=None, tokenizer_name=None, text_column=None, summary_column=None, use_slow_tokenizer=False, per_device_train_batch_size=8, per_device_eval_batch_size=4, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=4, max_train_steps=None, gradient_accumulation_steps=2, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.05, output_dir='./output_eli5Translated/2_8_1_1_4_5e-05_fp16', seed=42, model_type=None, teacher_model='facebook/bart-base', student_model='facebook/bart-base', pred_distill=True, intermediate_distill=True, weight_bits=2, input_bits=8, clip_val=2.5, length_penalty=1.0, max_length=160, min_length=50, num_beams=4, do_train=True, do_test=True, test_teacher=False, distill_encoder=1, distill_decoder=1, log_steps=20, local_rank=0, weighted=False, new_distill_map=False, task_weight=1, logits_weight=1, hid_weight=1)


the args output_dir is:  ./output_eli5Translated/2_8_1_1_4_5e-05_fp16
05/08/2023 18:46:57 - WARNING - datasets.builder -   Found cached dataset csv (/home/sshukla7/.cache/huggingface/datasets/csv/default-b601d1c6990e120c/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)


 from_pretrained,  args output dir:  facebook/bart-base
init quantize emb



 ***** The raw datset is: *****  DatasetDict({
    train: Dataset({
        features: ['query', 'answer', 'french_query', 'french_answer'],
        num_rows: 1000
    })
    validation: Dataset({
        features: ['query', 'answer', 'french_query', 'french_answer'],
        num_rows: 200
    })
    test: Dataset({
        features: ['query', 'answer', 'french_query', 'french_answer'],
        num_rows: 200
    })
})


 ***** datasets size, train: 1000, validate: 200, test: 200 *****
05/08/2023 18:47:03 - INFO - __main__ -   Sample 654 of the training set: {'input_ids': [0, 2264, 16, 5, 2759, 9, 5849, 116, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 179, 6732, 6, 63, 182, 543, 7, 120, 41, 6089, 346, 4, 1993, 9, 5, 11840, 13, 1402, 6732, 283, 19, 14, 3724, 6, 10, 2759, 9, 5849, 4, 3139, 5072, 5, 1280, 9, 678, 37832, 11, 5, 1948, 4, 98, 291, 207, 19, 10, 2759, 9, 5849, 9, 195, 207, 839, 5, 3031, 1948, 115, 28, 31, 379, 27868, 1244, 207, 53, 63, 2299, 6152, 11, 89, 4, 100, 1017, 3608, 2600, 62, 15, 2759, 9, 5849, 25, 24, 16009, 7, 2166, 19851, 4, 85, 18, 3013, 7, 1346, 8, 10, 319, 55, 847, 359, 3841, 1118, 7, 5, 3418, 8, 19958, 10147, 443, 9, 35638, 4, 318, 38, 146, 10, 2166, 20104, 6, 224, 14978, 5, 5933, 9, 140, 18, 6463, 2549, 634, 10, 2]}.
Yes, cuda is available, Found device: NVIDIA A100-SXM-80GB, n_gpu: 1
05/08/2023 18:47:04 - INFO - __main__ -   

***** Running training *****
05/08/2023 18:47:04 - INFO - __main__ -     Num examples = 1000
05/08/2023 18:47:04 - INFO - __main__ -     Num Epochs = 4
05/08/2023 18:47:04 - INFO - __main__ -     Instantaneous batch size per device = 8
05/08/2023 18:47:04 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16
05/08/2023 18:47:04 - INFO - __main__ -     Gradient Accumulation steps = 2
05/08/2023 18:47:04 - INFO - __main__ -     Total optimization steps = 252
05/08/2023 18:47:04 - INFO - __main__ -     student encoder layers = 1
05/08/2023 18:47:04 - INFO - __main__ -     student decoder layers = 1
05/08/2023 18:47:04 - INFO - __main__ -     student encoder layers [0] is mapped with teacher encoder layers [5]
05/08/2023 18:47:04 - INFO - __main__ -     student decoder layers [0] is mapped with teacher decoder layers [5]
05/08/2023 18:48:09 - INFO - __main__ -   evaluation result: {'eval/rouge1': 18.4855, 'eval/rouge2': 0.741, 'eval/rougeL': 12.732, 'eval/rougeLsum': 16.0595} 


 line 910, args output dir:  ./output_eli5Translated/2_8_1_1_4_5e-05_fp16
05/08/2023 18:49:06 - INFO - __main__ -   evaluation result: {'eval/rouge1': 18.3552, 'eval/rouge2': 0.8501, 'eval/rougeL': 13.2857, 'eval/rougeLsum': 16.6543} 


 line 910, args output dir:  ./output_eli5Translated/2_8_1_1_4_5e-05_fp16
05/08/2023 18:49:59 - INFO - __main__ -   evaluation result: {'eval/rouge1': 18.5113, 'eval/rouge2': 0.8604, 'eval/rougeL': 13.0067, 'eval/rougeLsum': 17.3165} 


 line 910, args output dir:  ./output_eli5Translated/2_8_1_1_4_5e-05_fp16
05/08/2023 18:50:52 - INFO - __main__ -   evaluation result: {'eval/rouge1': 18.9871, 'eval/rouge2': 0.9677, 'eval/rougeL': 13.1766, 'eval/rougeLsum': 17.6721} 


 line 910, args output dir:  ./output_eli5Translated/2_8_1_1_4_5e-05_fp16


 from_pretrained,  args output dir:  ./output_eli5Translated/2_8_1_1_4_5e-05_fp16


 ****** fetching pre-trained model config from json file ***** 




 ***** best model config in do_test: ***** 

 {
  "_commit_hash": null,
  "_name_or_path": "bart-base",
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_cross_attention": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bad_words_ids": null,
  "begin_suppress_tokens": null,
  "bos_token_id": 0,
  "chunk_size_feed_forward": 0,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.0,
  "clip_val": 2.5,
  "cross_attention_hidden_size": null,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 1,
  "decoder_start_token_id": 2,
  "diversity_penalty": 0.0,
  "do_sample": false,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 1,
  "encoder_no_repeat_ngram_size": 0,
  "eos_token_id": 2,
  "exponential_decay_length_penalty": null,
  "finetuning_task": null,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "input_bits": 8,
  "is_decoder": false,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 1024,
  "min_length": 0,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beam_groups": 1,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_scores": false,
  "pad_token_id": 1,
  "prefix": null,
  "problem_type": null,
  "pruned_heads": {},
  "quantize_act": true,
  "remove_invalid_values": false,
  "repetition_penalty": 1.0,
  "return_dict": true,
  "return_dict_in_generate": false,
  "scale_embedding": false,
  "sep_token_id": null,
  "suppress_tokens": null,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "temperature": 1.0,
  "tf_legacy_loss": false,
  "tie_encoder_decoder": false,
  "tie_word_embeddings": true,
  "tokenizer_class": null,
  "top_k": 50,
  "top_p": 1.0,
  "torch_dtype": "float32",
  "torchscript": false,
  "transformers_version": "4.12.0.dev0",
  "typical_p": 1.0,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 50265,
  "vocab_size_or_config_json_file": -1,
  "weight_bits": 2
}

init quantize emb
05/08/2023 18:51:35 - INFO - __main__ -   test result: {'test/rouge1': 18.3072, 'test/rouge2': 0.8509, 'test/rougeL': 12.8204, 'test/rougeLsum': 17.4561}
