Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pip in /home/sshukla7/.local/lib/python3.10/site-packages (23.1.2)
Defaulting to user installation because normal site-packages is not writeable
05/07/2023 05:14:27 - INFO - __main__ -   Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/07/2023 05:14:27 - WARNING - __main__ -   Namespace(dataset_name='ml6team/cnn_dailymail_nl', dataset_config_name='3.0.0', train_file=None, validation_file=None, ignore_pad_token_for_loss=True, max_source_length=1024, source_prefix=None, preprocessing_num_workers=None, overwrite_cache=None, max_target_length=128, val_max_target_length=None, pad_to_max_length=False, model_name_or_path='ainize/bart-base-cnn', config_name=None, tokenizer_name=None, text_column=None, summary_column=None, use_slow_tokenizer=False, per_device_train_batch_size=16, per_device_eval_batch_size=4, learning_rate=3e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=2, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.05, output_dir='./output_ml6team_cnn_dailymail_nl/2_8_1_1_20_3e-05_fp16', seed=28, model_type=None, teacher_model='ainize/bart-base-cnn', student_model='ainize/bart-base-cnn', pred_distill=True, intermediate_distill=True, weight_bits=2, input_bits=8, clip_val=2.5, length_penalty=2.0, max_length=160, min_length=56, num_beams=4, do_train=True, do_test=True, test_teacher=False, distill_encoder=1, distill_decoder=1, log_steps=20, local_rank=0, weighted=False, new_distill_map=False, task_weight=1, logits_weight=1, hid_weight=1)


the arguments are:  Namespace(dataset_name='ml6team/cnn_dailymail_nl', dataset_config_name='3.0.0', train_file=None, validation_file=None, ignore_pad_token_for_loss=True, max_source_length=1024, source_prefix=None, preprocessing_num_workers=None, overwrite_cache=None, max_target_length=128, val_max_target_length=None, pad_to_max_length=False, model_name_or_path='ainize/bart-base-cnn', config_name=None, tokenizer_name=None, text_column=None, summary_column=None, use_slow_tokenizer=False, per_device_train_batch_size=16, per_device_eval_batch_size=4, learning_rate=3e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=2, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.05, output_dir='./output_ml6team_cnn_dailymail_nl/2_8_1_1_20_3e-05_fp16', seed=28, model_type=None, teacher_model='ainize/bart-base-cnn', student_model='ainize/bart-base-cnn', pred_distill=True, intermediate_distill=True, weight_bits=2, input_bits=8, clip_val=2.5, length_penalty=2.0, max_length=160, min_length=56, num_beams=4, do_train=True, do_test=True, test_teacher=False, distill_encoder=1, distill_decoder=1, log_steps=20, local_rank=0, weighted=False, new_distill_map=False, task_weight=1, logits_weight=1, hid_weight=1)


the args output_dir is:  ./output_ml6team_cnn_dailymail_nl/2_8_1_1_20_3e-05_fp16
Downloading and preparing dataset cnn_dailymail_nl/3.0.0 to /home/sshukla7/.cache/huggingface/datasets/ml6team___cnn_dailymail_nl/3.0.0/0.0.0/73618cbc23f25331390bc0475f361e0531b47feee292c9cf84d396d6a3b9b608...
Dataset cnn_dailymail_nl downloaded and prepared to /home/sshukla7/.cache/huggingface/datasets/ml6team___cnn_dailymail_nl/3.0.0/0.0.0/73618cbc23f25331390bc0475f361e0531b47feee292c9cf84d396d6a3b9b608. Subsequent calls will reuse this data.
init quantize emb




 The raw datset is :  DatasetDict({
    train: Dataset({
        features: ['article', 'highlights', 'id'],
        num_rows: 287113
    })
    validation: Dataset({
        features: ['article', 'highlights', 'id'],
        num_rows: 13368
    })
    test: Dataset({
        features: ['article', 'highlights', 'id'],
        num_rows: 11490
    })
})
datasets size, train: 287113, validate: 13368, test: 11490
05/07/2023 05:16:25 - INFO - __main__ -   Sample 59222 of the training set: {'input_ids': [0, 495, 4623, 479, 7957, 1813, 479, 121, 405, 1899, 2802, 2161, 35, 479, 379, 35, 3818, 12936, 6, 158, 263, 47153, 1014, 479, 479, 479, 163, 2161, 23740, 13760, 90, 35, 479, 15171, 35, 4390, 12936, 6, 365, 263, 47153, 1014, 479, 3060, 12805, 225, 1928, 18, 449, 17424, 225, 2136, 225, 10, 611, 1334, 24612, 26511, 11, 364, 225, 39298, 118, 105, 459, 128, 29126, 784, 257, 967, 108, 11, 364, 225, 5620, 2629, 992, 324, 6944, 6455, 354, 13332, 271, 6977, 1899, 428, 15904, 225, 2136, 225, 4342, 329, 1957, 417, 1177, 5963, 571, 8557, 677, 90, 748, 4623, 7581, 324, 4, 926, 4342, 29530, 2794, 109, 366, 6, 821, 3623, 139, 25902, 1145, 364, 225, 449, 29245, 1177, 263, 34400, 6, 16, 11, 37, 90, 992, 324, 6944, 6455, 354, 3538, 312, 3351, 11, 4323, 462, 18787, 4, 3713, 196, 268, 449, 17424, 225, 26458, 15, 23740, 225, 6526, 761, 8663, 41, 5270, 991, 10, 611, 1334, 12805, 225, 11, 37, 90, 128, 30047, 16291, 3340, 242, 3934, 9, 1928, 6487, 967, 6, 1177, 263, 45747, 324, 6944, 16619, 225, 992, 2495, 13516, 37, 90, 15, 1899, 548, 254, 11901, 1942, 7321, 254, 228, 14001, 271, 2136, 90, 821, 3209, 2070, 967, 90, 4, 3060, 12805, 225, 35, 381, 225, 1928, 6487, 967, 11, 37, 90, 4323, 462, 18787, 1090, 312, 4, 3351, 992, 324, 6944, 6455, 354, 11, 5620, 2629, 1245, 4, 381, 225, 1928, 6487, 967, 16, 364, 225, 16415, 330, 13332, 271, 475, 23891, 1928, 18, 449, 17424, 225, 15549, 2590, 225, 6, 162, 990, 337, 6977, 1899, 428, 15904, 225, 6, 1177, 20025, 2955, 271, 41, 5270, 991, 10, 611, 1334, 12805, 225, 16780, 821, 3623, 2832, 225, 1177, 4342, 329, 1957, 417, 3055, 2136, 225, 4, 726, 4526, 35, 16195, 4533, 990, 705, 1988, 14531, 225, 784, 257, 967, 5921, 2161, 38585, 3335, 6439, 1177, 295, 625, 415, 364, 225, 20187, 3863, 26458, 1928, 741, 5246, 225, 37, 242, 2543, 5473, 2911, 102, 415, 620, 1177, 37, 90, 385, 5398, 2977, 21058, 6, 2171, 2399, 37, 90, 364, 225, 8054, 11, 37, 90, 992, 324, 6944, 6455, 354, 992, 1630, 415, 37, 90, 20187, 1264, 523, 37, 90, 761, 9864, 27906, 967, 6944, 4, 19802, 35, 926, 21950, 7282, 18474, 21, 821, 3209, 2070, 4348, 462, 18474, 11, 263, 475, 6502, 242, 7445, 257, 11760, 6, 9131, 271, 16, 15, 32373, 29, 15, 3624, 748, 257, 710, 821, 1951, 14900, 3538, 263, 468, 487, 4, 289, 594, 4533, 990, 705, 1069, 267, 24370, 225, 784, 257, 967, 5921, 2161, 38585, 1717, 405, 1177, 295, 625, 415, 364, 225, 20187, 3863, 992, 18787, 1928, 741, 5246, 225, 37, 242, 2543, 5473, 2911, 102, 415, 620, 1177, 22460, 196, 37, 90, 385, 5398, 6, 37, 90, 2171, 2399, 364, 225, 8054, 11, 37, 90, 992, 324, 6944, 6455, 354, 992, 1630, 415, 37, 90, 20187, 1264, 523, 37, 90, 761, 9864, 1021, 34527, 225, 4, 289, 594, 4279, 7363, 15, 1899, 548, 254, 11901, 1942, 9, 13911, 242, 5251, 19995, 748, 4623, 37, 90, 20187, 1264, 523, 16780, 741, 2161, 37, 90, 784, 257, 967, 3055, 449, 14900, 6, 992, 1630, 415, 263, 1021, 34612, 12358, 139, 3733, 326, 2161, 417, 37, 242, 2543, 16780, 15, 1899, 329, 2495, 52, 571, 3055, 5921, 5600, 225, 4, 96, 263, 109, 366, 16, 364, 225, 4315, 1597, 33566, 6607, 263, 7458, 11736, 211, 6439, 1090, 7241, 368, 1459, 405, 225, 992, 337, 992, 1957, 225, 748, 4623, 263, 1928, 748, 4623, 10, 8797, 52, 6944, 4, 289, 594, 13550, 6526, 991, 9864, 992, 18787, 1076, 29, 364, 225, 479, 128, 241, 13728, 6645, 263, 710, 108, 1076, 29, 7458, 196, 268, 1597, 3538, 821, 196, 1488, 3869, 4342, 463, 8663, 449, 17424, 225, 8470, 3252, 330, 14900, 16780, 26458, 1928, 18, 8470, 3252, 3055, 748, 3109, 8663, 741, 5246, 225, 11901, 1942, 9131, 463, 225, 4, 381, 611, 1334, 6, 1076, 29, 263, 1021, 34612, 295, 5810, 8470, 3252, 330, 1075, 90, 16780, 1145, 26458, 761, 3055, 2136, 225, 37, 2558, 1023, 417, 2750, 263, 11901, 1942, 9131, 463, 225, 992, 337, 37, 90, 2136, 225, 5963, 1899, 1529, 25902, 748, 4623, 7581, 324, 4, 926, 21950, 7282, 18474, 21, 821, 3209, 2070, 4348, 462, 18474, 11, 263, 475, 6502, 242, 7445, 257, 11760, 9131, 271, 16, 15, 32373, 29, 15, 3624, 748, 257, 710, 821, 1951, 14900, 3538, 263, 468, 487, 4, 926, 3295, 162, 4779, 90, 13516, 37, 90, 16, 3538, 604, 154, 13516, 761, 8663, 37, 90, 769, 8797, 37, 428, 5720, 16780, 3055, 7727, 225, 885, 324, 26458, 1021, 38953, 992, 18787, 1177, 13516, 37, 90, 769, 8797, 2136, 90, 15, 12527, 3733, 417, 10, 260, 761, 8663, 10, 611, 1334, 24612, 26511, 11, 37, 90, 1928, 6487, 967, 4, 4594, 16, 1021, 1638, 33010, 13516, 6, 3538, 1694, 1899, 263, 41, 261, 757, 1459, 405, 3538, 37, 90, 13550, 6526, 991, 6, 748, 16434, 3538, 761, 8663, 9, 8, 2816, 13403, 4715, 13587, 225, 263, 1928, 449, 17424, 225, 81, 10232, 10955, 11, 2968, 102, 2923, 3538, 7458, 196, 268, 4, 926, 468, 487, 12, 33479, 3006, 324, 748, 4623, 263, 1223, 611, 3869, 3538, 37, 90, 17161, 992, 3733, 90, 13516, 3335, 1928, 8304, 225, 11, 19728, 11608, 992, 18787, 6, 9131, 271, 5620, 2629, 1245, 37, 242, 2543, 37, 90, 162, 990, 1145, 6705, 11, 37, 90, 37, 459, 1212, 4, 6189, 225, 37, 242, 2543, 2248, 6, 7303, 267, 7529, 118, 105, 3550, 6, 3523, 1512, 2359, 973, 6, 26720, 677, 2161, 242, 545, 6, 17124, 1438, 11760, 10, 8797, 6, 85, 3644, 105, 10, 8797, 1177, 34132, 118, 105, 6, 234, 11736, 1245, 6, 525, 605, 2629, 254, 1245, 6, 468, 5183, 23081, 6, 896, 1177, 21361, 8539, 105, 37, 428, 5720, 22626, 1916, 337, 7935, 10598, 579, 2028, 29, 748, 34188, 14001, 271, 4, 468, 487, 35, 289, 594, 468, 487, 12, 14721, 21193, 748, 4623, 263, 1223, 611, 3869, 3538, 37, 90, 17161, 992, 3733, 90, 13516, 3335, 1928, 8304, 225, 11, 19728, 11608, 992, 18787, 6, 9131, 271, 5620, 2629, 1245, 37, 242, 2543, 37, 90, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 13365, 4342, 29530, 2794, 109, 366, 16, 11, 37, 90, 312, 3351, 12, 14807, 6944, 6455, 354, 3538, 4323, 462, 18787, 4, 3713, 196, 268, 449, 17424, 225, 26458, 15, 23740, 225, 6526, 761, 8663, 2955, 271, 41, 5270, 991, 10, 611, 1334, 12805, 225, 4, 7542, 761, 741, 5246, 225, 10, 8797, 52, 6944, 8470, 12967, 337, 225, 8, 268, 992, 337, 37, 90, 2136, 225, 821, 19928, 1517, 859, 25902, 4, 2]}.
Yes, cuda is available, Found device: NVIDIA A100-SXM-80GB, n_gpu: 1
05/07/2023 05:16:29 - INFO - __main__ -   ***** Running training *****
05/07/2023 05:16:29 - INFO - __main__ -     Num examples = 287113
05/07/2023 05:16:29 - INFO - __main__ -     Num Epochs = 20
05/07/2023 05:16:29 - INFO - __main__ -     Instantaneous batch size per device = 16
05/07/2023 05:16:29 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
05/07/2023 05:16:29 - INFO - __main__ -     Gradient Accumulation steps = 2
05/07/2023 05:16:29 - INFO - __main__ -     Total optimization steps = 179460
05/07/2023 05:16:29 - INFO - __main__ -     student encoder layers = 1
05/07/2023 05:16:29 - INFO - __main__ -     student decoder layers = 1
05/07/2023 05:16:29 - INFO - __main__ -     student encoder layers [0] is mapped with teacher encoder layers [5]
05/07/2023 05:16:29 - INFO - __main__ -     student decoder layers [0] is mapped with teacher decoder layers [5]
05/07/2023 08:46:55 - INFO - __main__ -   evaluation result: {'eval/rouge1': 18.4218, 'eval/rouge2': 2.0328, 'eval/rougeL': 12.5773, 'eval/rougeLsum': 16.8909} 
