
Lmod is automatically replacing "gnu9/9.3.0" with "gnu10/10.3.0-ya".


Inactive Modules:
  1) hwloc/2.1.0     2) libfabric     3) openmpi4     4) ucx

wandb: Tracking run with wandb version 0.14.0
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 38.88it/s]
https://huggingface.co/facebook/bart-large-xsum/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/sshukla7/.cache/huggingface/transformers/tmpaxfc7yru
Downloading:   0%|          | 0.00/1.48k [00:00<?, ?B/s]Downloading: 100%|██████████| 1.48k/1.48k [00:00<00:00, 2.47MB/s]
storing https://huggingface.co/facebook/bart-large-xsum/resolve/main/config.json in cache at /home/sshukla7/.cache/huggingface/transformers/cdbb6cd53d09520f44eef933744192f091643a08a0d65588427573d58b2aeabd.03b20c7da280ad3416e8d6ca3c304bd2f05bd11046ec8ce7880df7b04f8970c5
creating metadata file for /home/sshukla7/.cache/huggingface/transformers/cdbb6cd53d09520f44eef933744192f091643a08a0d65588427573d58b2aeabd.03b20c7da280ad3416e8d6ca3c304bd2f05bd11046ec8ce7880df7b04f8970c5
loading configuration file https://huggingface.co/facebook/bart-large-xsum/resolve/main/config.json from cache at /home/sshukla7/.cache/huggingface/transformers/cdbb6cd53d09520f44eef933744192f091643a08a0d65588427573d58b2aeabd.03b20c7da280ad3416e8d6ca3c304bd2f05bd11046ec8ce7880df7b04f8970c5
Model config BartConfig {
  "_name_or_path": "facebook/bart-large-xsum",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "eos_token_ids": [
    2
  ],
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_length": 62,
  "max_position_embeddings": 1024,
  "min_length": 11,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 6,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "prefix": " ",
  "replacing_rate": 0,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "student_decoder_layers": null,
  "student_encoder_layers": null,
  "task_specific_params": {},
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50264
}

https://huggingface.co/facebook/bart-large-xsum/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /home/sshukla7/.cache/huggingface/transformers/tmpsit37t_g
Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 35.5kB/s]
storing https://huggingface.co/facebook/bart-large-xsum/resolve/main/tokenizer_config.json in cache at /home/sshukla7/.cache/huggingface/transformers/8a7732bd43e87001f6ed9106dbe3dea5109a7506835de4498e5ce4a91db05fdf.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8
creating metadata file for /home/sshukla7/.cache/huggingface/transformers/8a7732bd43e87001f6ed9106dbe3dea5109a7506835de4498e5ce4a91db05fdf.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8
loading configuration file https://huggingface.co/facebook/bart-large-xsum/resolve/main/config.json from cache at /home/sshukla7/.cache/huggingface/transformers/cdbb6cd53d09520f44eef933744192f091643a08a0d65588427573d58b2aeabd.03b20c7da280ad3416e8d6ca3c304bd2f05bd11046ec8ce7880df7b04f8970c5
Model config BartConfig {
  "_name_or_path": "facebook/bart-large-xsum",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "eos_token_ids": [
    2
  ],
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_length": 62,
  "max_position_embeddings": 1024,
  "min_length": 11,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 6,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "prefix": " ",
  "replacing_rate": 0,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "student_decoder_layers": null,
  "student_encoder_layers": null,
  "task_specific_params": {},
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50264
}

https://huggingface.co/facebook/bart-large-xsum/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /home/sshukla7/.cache/huggingface/transformers/tmp4vhc5q3v
Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]Downloading: 100%|██████████| 878k/878k [00:00<00:00, 15.7MB/s]
storing https://huggingface.co/facebook/bart-large-xsum/resolve/main/vocab.json in cache at /home/sshukla7/.cache/huggingface/transformers/71d65b64a1aef3a1ac2c08506592a72541c4a12c0f82174c85bd8d51af62724f.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
creating metadata file for /home/sshukla7/.cache/huggingface/transformers/71d65b64a1aef3a1ac2c08506592a72541c4a12c0f82174c85bd8d51af62724f.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
https://huggingface.co/facebook/bart-large-xsum/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /home/sshukla7/.cache/huggingface/transformers/tmp5371bgci
Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]Downloading: 100%|██████████| 446k/446k [00:00<00:00, 36.3MB/s]
storing https://huggingface.co/facebook/bart-large-xsum/resolve/main/merges.txt in cache at /home/sshukla7/.cache/huggingface/transformers/96c5f8e99e629c84a0b7c8bf058851da38f738481baf5e72995c45acbeb8dafa.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
creating metadata file for /home/sshukla7/.cache/huggingface/transformers/96c5f8e99e629c84a0b7c8bf058851da38f738481baf5e72995c45acbeb8dafa.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
https://huggingface.co/facebook/bart-large-xsum/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/sshukla7/.cache/huggingface/transformers/tmpi3imq_qn
Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 46.5MB/s]
storing https://huggingface.co/facebook/bart-large-xsum/resolve/main/tokenizer.json in cache at /home/sshukla7/.cache/huggingface/transformers/8ec578f2712277533fd5623320cc779a28a4b396e8b506b2712b04f5575761ce.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
creating metadata file for /home/sshukla7/.cache/huggingface/transformers/8ec578f2712277533fd5623320cc779a28a4b396e8b506b2712b04f5575761ce.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
loading file https://huggingface.co/facebook/bart-large-xsum/resolve/main/vocab.json from cache at /home/sshukla7/.cache/huggingface/transformers/71d65b64a1aef3a1ac2c08506592a72541c4a12c0f82174c85bd8d51af62724f.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05
loading file https://huggingface.co/facebook/bart-large-xsum/resolve/main/merges.txt from cache at /home/sshukla7/.cache/huggingface/transformers/96c5f8e99e629c84a0b7c8bf058851da38f738481baf5e72995c45acbeb8dafa.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
loading file https://huggingface.co/facebook/bart-large-xsum/resolve/main/tokenizer.json from cache at /home/sshukla7/.cache/huggingface/transformers/8ec578f2712277533fd5623320cc779a28a4b396e8b506b2712b04f5575761ce.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
loading file https://huggingface.co/facebook/bart-large-xsum/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/facebook/bart-large-xsum/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/facebook/bart-large-xsum/resolve/main/tokenizer_config.json from cache at /home/sshukla7/.cache/huggingface/transformers/8a7732bd43e87001f6ed9106dbe3dea5109a7506835de4498e5ce4a91db05fdf.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8
loading configuration file https://huggingface.co/facebook/bart-large-xsum/resolve/main/config.json from cache at /home/sshukla7/.cache/huggingface/transformers/cdbb6cd53d09520f44eef933744192f091643a08a0d65588427573d58b2aeabd.03b20c7da280ad3416e8d6ca3c304bd2f05bd11046ec8ce7880df7b04f8970c5
Model config BartConfig {
  "_name_or_path": "facebook/bart-large-xsum",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "eos_token_ids": [
    2
  ],
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_length": 62,
  "max_position_embeddings": 1024,
  "min_length": 11,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 6,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "prefix": " ",
  "replacing_rate": 0,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "student_decoder_layers": null,
  "student_encoder_layers": null,
  "task_specific_params": {},
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50264
}

https://huggingface.co/facebook/bart-large-xsum/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/sshukla7/.cache/huggingface/transformers/tmpaqqwe6nd
Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]Downloading:   1%|          | 10.1M/1.51G [00:00<00:15, 106MB/s]Downloading:   1%|▏         | 20.4M/1.51G [00:00<00:14, 107MB/s]Downloading:   2%|▏         | 30.6M/1.51G [00:00<00:14, 107MB/s]Downloading:   3%|▎         | 40.8M/1.51G [00:00<00:14, 107MB/s]Downloading:   3%|▎         | 51.0M/1.51G [00:00<00:14, 106MB/s]Downloading:   4%|▍         | 61.1M/1.51G [00:00<00:14, 105MB/s]Downloading:   5%|▍         | 71.5M/1.51G [00:00<00:14, 106MB/s]Downloading:   5%|▌         | 81.6M/1.51G [00:00<00:14, 105MB/s]Downloading:   6%|▌         | 92.0M/1.51G [00:00<00:14, 107MB/s]Downloading:   7%|▋         | 102M/1.51G [00:01<00:14, 107MB/s] Downloading:   7%|▋         | 112M/1.51G [00:01<00:14, 105MB/s]Downloading:   8%|▊         | 123M/1.51G [00:01<00:14, 106MB/s]Downloading:   9%|▊         | 133M/1.51G [00:01<00:13, 106MB/s]Downloading:   9%|▉         | 143M/1.51G [00:01<00:13, 107MB/s]Downloading:  10%|▉         | 153M/1.51G [00:01<00:13, 106MB/s]Downloading:  11%|█         | 163M/1.51G [00:01<00:13, 106MB/s]Downloading:  11%|█         | 174M/1.51G [00:01<00:13, 106MB/s]Downloading:  12%|█▏        | 184M/1.51G [00:01<00:13, 107MB/s]Downloading:  13%|█▎        | 194M/1.51G [00:01<00:13, 106MB/s]Downloading:  13%|█▎        | 204M/1.51G [00:02<00:13, 105MB/s]Downloading:  14%|█▍        | 214M/1.51G [00:02<00:13, 105MB/s]Downloading:  14%|█▍        | 224M/1.51G [00:02<00:13, 106MB/s]Downloading:  15%|█▌        | 235M/1.51G [00:02<00:13, 106MB/s]Downloading:  16%|█▌        | 245M/1.51G [00:02<00:12, 107MB/s]Downloading:  16%|█▋        | 255M/1.51G [00:02<00:12, 107MB/s]Downloading:  17%|█▋        | 266M/1.51G [00:02<00:12, 107MB/s]Downloading:  18%|█▊        | 276M/1.51G [00:02<00:12, 107MB/s]Downloading:  18%|█▊        | 286M/1.51G [00:02<00:12, 107MB/s]Downloading:  19%|█▉        | 296M/1.51G [00:02<00:12, 106MB/s]Downloading:  20%|█▉        | 307M/1.51G [00:03<00:12, 107MB/s]Downloading:  20%|██        | 317M/1.51G [00:03<00:12, 107MB/s]Downloading:  21%|██        | 327M/1.51G [00:03<00:11, 107MB/s]Downloading:  22%|██▏       | 337M/1.51G [00:03<00:11, 107MB/s]Downloading:  22%|██▏       | 348M/1.51G [00:03<00:11, 107MB/s]Downloading:  23%|██▎       | 358M/1.51G [00:03<00:11, 107MB/s]Downloading:  24%|██▎       | 368M/1.51G [00:03<00:11, 106MB/s]Downloading:  24%|██▍       | 378M/1.51G [00:03<00:11, 103MB/s]Downloading:  25%|██▌       | 389M/1.51G [00:03<00:11, 105MB/s]Downloading:  26%|██▌       | 399M/1.51G [00:03<00:11, 105MB/s]Downloading:  26%|██▋       | 409M/1.51G [00:04<00:11, 106MB/s]Downloading:  27%|██▋       | 419M/1.51G [00:04<00:11, 104MB/s]Downloading:  28%|██▊       | 429M/1.51G [00:04<00:11, 104MB/s]Downloading:  28%|██▊       | 439M/1.51G [00:04<00:11, 103MB/s]Downloading:  29%|██▉       | 449M/1.51G [00:04<00:11, 103MB/s]Downloading:  30%|██▉       | 459M/1.51G [00:04<00:10, 104MB/s]Downloading:  30%|███       | 469M/1.51G [00:04<00:10, 104MB/s]Downloading:  31%|███       | 479M/1.51G [00:04<00:10, 105MB/s]Downloading:  32%|███▏      | 489M/1.51G [00:04<00:10, 105MB/s]Downloading:  32%|███▏      | 499M/1.51G [00:04<00:10, 105MB/s]Downloading:  33%|███▎      | 509M/1.51G [00:05<00:10, 104MB/s]Downloading:  34%|███▎      | 520M/1.51G [00:05<00:10, 106MB/s]Downloading:  34%|███▍      | 530M/1.51G [00:05<00:10, 105MB/s]Downloading:  35%|███▍      | 540M/1.51G [00:05<00:09, 107MB/s]Downloading:  36%|███▌      | 550M/1.51G [00:05<00:09, 107MB/s]Downloading:  36%|███▌      | 561M/1.51G [00:05<00:09, 106MB/s]Downloading:  37%|███▋      | 571M/1.51G [00:05<00:09, 106MB/s]Downloading:  37%|███▋      | 581M/1.51G [00:05<00:09, 106MB/s]Downloading:  38%|███▊      | 591M/1.51G [00:05<00:09, 106MB/s]Downloading:  39%|███▉      | 601M/1.51G [00:05<00:09, 106MB/s]Downloading:  39%|███▉      | 611M/1.51G [00:06<00:09, 106MB/s]Downloading:  40%|████      | 622M/1.51G [00:06<00:09, 107MB/s]Downloading:  41%|████      | 632M/1.51G [00:06<00:09, 107MB/s]Downloading:  41%|████▏     | 642M/1.51G [00:06<00:08, 107MB/s]Downloading:  42%|████▏     | 652M/1.51G [00:06<00:08, 106MB/s]Downloading:  43%|████▎     | 663M/1.51G [00:06<00:08, 107MB/s]Downloading:  43%|████▎     | 673M/1.51G [00:06<00:08, 106MB/s]Downloading:  44%|████▍     | 683M/1.51G [00:06<00:08, 106MB/s]Downloading:  45%|████▍     | 693M/1.51G [00:06<00:08, 107MB/s]Downloading:  45%|████▌     | 703M/1.51G [00:06<00:08, 104MB/s]Downloading:  46%|████▌     | 713M/1.51G [00:07<00:08, 103MB/s]Downloading:  47%|████▋     | 724M/1.51G [00:07<00:08, 105MB/s]Downloading:  47%|████▋     | 734M/1.51G [00:07<00:08, 106MB/s]Downloading:  48%|████▊     | 744M/1.51G [00:07<00:08, 105MB/s]Downloading:  49%|████▊     | 754M/1.51G [00:07<00:07, 106MB/s]Downloading:  49%|████▉     | 764M/1.51G [00:07<00:07, 106MB/s]Downloading:  50%|████▉     | 775M/1.51G [00:07<00:07, 107MB/s]Downloading:  51%|█████     | 785M/1.51G [00:07<00:07, 107MB/s]Downloading:  51%|█████▏    | 795M/1.51G [00:07<00:07, 107MB/s]Downloading:  52%|█████▏    | 806M/1.51G [00:07<00:07, 107MB/s]Downloading:  53%|█████▎    | 816M/1.51G [00:08<00:07, 107MB/s]Downloading:  53%|█████▎    | 826M/1.51G [00:08<00:07, 106MB/s]Downloading:  54%|█████▍    | 836M/1.51G [00:08<00:07, 106MB/s]Downloading:  55%|█████▍    | 846M/1.51G [00:08<00:06, 107MB/s]Downloading:  55%|█████▌    | 857M/1.51G [00:08<00:06, 106MB/s]Downloading:  56%|█████▌    | 867M/1.51G [00:08<00:06, 106MB/s]Downloading:  57%|█████▋    | 877M/1.51G [00:08<00:06, 107MB/s]Downloading:  57%|█████▋    | 887M/1.51G [00:08<00:06, 106MB/s]Downloading:  58%|█████▊    | 897M/1.51G [00:08<00:06, 107MB/s]Downloading:  59%|█████▊    | 908M/1.51G [00:08<00:06, 107MB/s]Downloading:  59%|█████▉    | 918M/1.51G [00:09<00:06, 106MB/s]Downloading:  60%|█████▉    | 928M/1.51G [00:09<00:06, 105MB/s]Downloading:  61%|██████    | 938M/1.51G [00:09<00:06, 105MB/s]Downloading:  61%|██████    | 949M/1.51G [00:09<00:05, 106MB/s]Downloading:  62%|██████▏   | 959M/1.51G [00:09<00:05, 106MB/s]Downloading:  63%|██████▎   | 969M/1.51G [00:09<00:05, 106MB/s]Downloading:  63%|██████▎   | 979M/1.51G [00:09<00:05, 107MB/s]Downloading:  64%|██████▍   | 990M/1.51G [00:09<00:05, 107MB/s]Downloading:  65%|██████▍   | 0.98G/1.51G [00:09<00:05, 108MB/s]Downloading:  65%|██████▌   | 0.99G/1.51G [00:09<00:05, 107MB/s]Downloading:  66%|██████▌   | 1.00G/1.51G [00:10<00:05, 107MB/s]Downloading:  66%|██████▋   | 1.01G/1.51G [00:10<00:05, 106MB/s]Downloading:  67%|██████▋   | 1.02G/1.51G [00:10<00:05, 106MB/s]Downloading:  68%|██████▊   | 1.03G/1.51G [00:10<00:04, 105MB/s]Downloading:  68%|██████▊   | 1.04G/1.51G [00:10<00:04, 106MB/s]Downloading:  69%|██████▉   | 1.05G/1.51G [00:10<00:04, 107MB/s]Downloading:  70%|██████▉   | 1.06G/1.51G [00:10<00:04, 107MB/s]Downloading:  70%|███████   | 1.07G/1.51G [00:10<00:04, 107MB/s]Downloading:  71%|███████   | 1.08G/1.51G [00:10<00:04, 105MB/s]Downloading:  72%|███████▏  | 1.09G/1.51G [00:11<00:04, 106MB/s]Downloading:  72%|███████▏  | 1.10G/1.51G [00:11<00:04, 107MB/s]Downloading:  73%|███████▎  | 1.11G/1.51G [00:11<00:04, 104MB/s]Downloading:  74%|███████▎  | 1.12G/1.51G [00:11<00:04, 104MB/s]Downloading:  74%|███████▍  | 1.13G/1.51G [00:11<00:04, 103MB/s]Downloading:  75%|███████▌  | 1.14G/1.51G [00:11<00:04, 99.6MB/s]Downloading:  76%|███████▌  | 1.15G/1.51G [00:11<00:03, 101MB/s] Downloading:  76%|███████▋  | 1.16G/1.51G [00:11<00:03, 103MB/s]Downloading:  77%|███████▋  | 1.17G/1.51G [00:11<00:03, 103MB/s]Downloading:  78%|███████▊  | 1.17G/1.51G [00:11<00:03, 104MB/s]Downloading:  78%|███████▊  | 1.18G/1.51G [00:12<00:03, 105MB/s]Downloading:  79%|███████▉  | 1.19G/1.51G [00:12<00:03, 105MB/s]Downloading:  80%|███████▉  | 1.20G/1.51G [00:12<00:03, 99.7MB/s]Downloading:  80%|████████  | 1.21G/1.51G [00:12<00:03, 102MB/s] Downloading:  81%|████████  | 1.22G/1.51G [00:12<00:03, 101MB/s]Downloading:  82%|████████▏ | 1.23G/1.51G [00:12<00:02, 103MB/s]Downloading:  82%|████████▏ | 1.24G/1.51G [00:12<00:02, 103MB/s]Downloading:  83%|████████▎ | 1.25G/1.51G [00:12<00:02, 104MB/s]Downloading:  83%|████████▎ | 1.26G/1.51G [00:12<00:02, 102MB/s]Downloading:  84%|████████▍ | 1.27G/1.51G [00:12<00:02, 102MB/s]Downloading:  85%|████████▍ | 1.28G/1.51G [00:13<00:03, 66.8MB/s]Downloading:  86%|████████▌ | 1.29G/1.51G [00:13<00:02, 80.7MB/s]Downloading:  86%|████████▋ | 1.31G/1.51G [00:13<00:02, 91.8MB/s]Downloading:  87%|████████▋ | 1.32G/1.51G [00:13<00:02, 102MB/s] Downloading:  88%|████████▊ | 1.33G/1.51G [00:13<00:01, 110MB/s]Downloading:  89%|████████▊ | 1.34G/1.51G [00:13<00:01, 106MB/s]Downloading:  89%|████████▉ | 1.35G/1.51G [00:13<00:01, 109MB/s]Downloading:  90%|█████████ | 1.36G/1.51G [00:13<00:01, 107MB/s]Downloading:  91%|█████████ | 1.37G/1.51G [00:14<00:01, 107MB/s]Downloading:  91%|█████████▏| 1.38G/1.51G [00:14<00:01, 105MB/s]Downloading:  92%|█████████▏| 1.39G/1.51G [00:14<00:01, 105MB/s]Downloading:  93%|█████████▎| 1.40G/1.51G [00:14<00:01, 105MB/s]Downloading:  93%|█████████▎| 1.41G/1.51G [00:14<00:01, 105MB/s]Downloading:  94%|█████████▍| 1.42G/1.51G [00:14<00:00, 106MB/s]Downloading:  95%|█████████▍| 1.43G/1.51G [00:14<00:00, 106MB/s]Downloading:  95%|█████████▌| 1.44G/1.51G [00:14<00:00, 106MB/s]Downloading:  96%|█████████▌| 1.45G/1.51G [00:14<00:00, 106MB/s]Downloading:  97%|█████████▋| 1.46G/1.51G [00:15<00:00, 104MB/s]Downloading:  97%|█████████▋| 1.47G/1.51G [00:15<00:00, 104MB/s]Downloading:  98%|█████████▊| 1.48G/1.51G [00:15<00:00, 104MB/s]Downloading:  99%|█████████▊| 1.49G/1.51G [00:15<00:00, 104MB/s]Downloading:  99%|█████████▉| 1.50G/1.51G [00:15<00:00, 103MB/s]Downloading: 100%|█████████▉| 1.51G/1.51G [00:15<00:00, 104MB/s]Downloading: 100%|██████████| 1.51G/1.51G [00:15<00:00, 105MB/s]
storing https://huggingface.co/facebook/bart-large-xsum/resolve/main/pytorch_model.bin in cache at /home/sshukla7/.cache/huggingface/transformers/53da17fb043e9535dc70a075eb146181bc5d1432133fb5dd3ca38eae1b98d04b.15b23f00c3d95ca1df22d78da2297a719e7e04ab76d9636e492063c861d35d1f
creating metadata file for /home/sshukla7/.cache/huggingface/transformers/53da17fb043e9535dc70a075eb146181bc5d1432133fb5dd3ca38eae1b98d04b.15b23f00c3d95ca1df22d78da2297a719e7e04ab76d9636e492063c861d35d1f
loading weights file https://huggingface.co/facebook/bart-large-xsum/resolve/main/pytorch_model.bin from cache at /home/sshukla7/.cache/huggingface/transformers/53da17fb043e9535dc70a075eb146181bc5d1432133fb5dd3ca38eae1b98d04b.15b23f00c3d95ca1df22d78da2297a719e7e04ab76d9636e492063c861d35d1f
All model checkpoint weights were used when initializing BartForConditionalGeneration.

All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-xsum.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
/home/sshukla7/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:637: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`
  warnings.warn(
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.51k/1.51k [00:00<00:00, 639kB/s]
Running tokenizer on dataset #0:   0%|          | 0/21 [00:00<?, ?ba/s]
Running tokenizer on dataset #1:   0%|          | 0/21 [00:00<?, ?ba/s][A

Running tokenizer on dataset #2:   0%|          | 0/21 [00:00<?, ?ba/s][A[A


Running tokenizer on dataset #3:   0%|          | 0/21 [00:00<?, ?ba/s][A[A[A



Running tokenizer on dataset #4:   0%|          | 0/21 [00:00<?, ?ba/s][A[A[A[A




Running tokenizer on dataset #5:   0%|          | 0/21 [00:00<?, ?ba/s][A[A[A[A[A





Running tokenizer on dataset #6:   0%|          | 0/21 [00:00<?, ?ba/s][A[A[A[A[A[A






Running tokenizer on dataset #7:   0%|          | 0/21 [00:00<?, ?ba/s][A[A[A[A[A[A[A







Running tokenizer on dataset #8:   0%|          | 0/21 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:   0%|          | 0/21 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:   5%|▍         | 1/21 [00:01<00:20,  1.02s/ba]
Running tokenizer on dataset #1:   5%|▍         | 1/21 [00:01<00:20,  1.03s/ba][A


Running tokenizer on dataset #3:   5%|▍         | 1/21 [00:00<00:19,  1.01ba/s][A[A[A

Running tokenizer on dataset #2:   5%|▍         | 1/21 [00:01<00:21,  1.09s/ba][A[A



Running tokenizer on dataset #4:   5%|▍         | 1/21 [00:01<00:21,  1.06s/ba][A[A[A[A




Running tokenizer on dataset #5:   5%|▍         | 1/21 [00:01<00:20,  1.02s/ba][A[A[A[A[A





Running tokenizer on dataset #6:   5%|▍         | 1/21 [00:01<00:21,  1.07s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:   5%|▍         | 1/21 [00:01<00:20,  1.02s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:   5%|▍         | 1/21 [00:01<00:20,  1.02s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:   5%|▍         | 1/21 [00:01<00:20,  1.04s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  10%|▉         | 2/21 [00:02<00:19,  1.00s/ba]


Running tokenizer on dataset #3:  10%|▉         | 2/21 [00:01<00:18,  1.04ba/s][A[A[A
Running tokenizer on dataset #1:  10%|▉         | 2/21 [00:02<00:19,  1.03s/ba][A

Running tokenizer on dataset #2:  10%|▉         | 2/21 [00:02<00:19,  1.02s/ba][A[A




Running tokenizer on dataset #5:  10%|▉         | 2/21 [00:01<00:18,  1.02ba/s][A[A[A[A[A



Running tokenizer on dataset #4:  10%|▉         | 2/21 [00:02<00:19,  1.01s/ba][A[A[A[A





Running tokenizer on dataset #6:  10%|▉         | 2/21 [00:02<00:19,  1.04s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  10%|▉         | 2/21 [00:02<00:19,  1.00s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  10%|▉         | 2/21 [00:01<00:18,  1.03ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  10%|▉         | 2/21 [00:01<00:18,  1.01ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  14%|█▍        | 3/21 [00:02<00:17,  1.02ba/s]
Running tokenizer on dataset #1:  14%|█▍        | 3/21 [00:03<00:18,  1.02s/ba][A




Running tokenizer on dataset #5:  14%|█▍        | 3/21 [00:02<00:17,  1.01ba/s][A[A[A[A[A



Running tokenizer on dataset #4:  14%|█▍        | 3/21 [00:03<00:18,  1.04s/ba][A[A[A[A

Running tokenizer on dataset #2:  14%|█▍        | 3/21 [00:03<00:19,  1.08s/ba][A[A


Running tokenizer on dataset #3:  14%|█▍        | 3/21 [00:03<00:19,  1.10s/ba][A[A[A







Running tokenizer on dataset #8:  14%|█▍        | 3/21 [00:02<00:17,  1.01ba/s][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  14%|█▍        | 3/21 [00:03<00:19,  1.08s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  14%|█▍        | 3/21 [00:03<00:20,  1.13s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  19%|█▉        | 4/21 [00:03<00:16,  1.03ba/s]
Running tokenizer on dataset #1:  19%|█▉        | 4/21 [00:04<00:17,  1.02s/ba][A




Running tokenizer on dataset #5:  19%|█▉        | 4/21 [00:03<00:16,  1.01ba/s][A[A[A[A[A






Running tokenizer on dataset #7:  14%|█▍        | 3/21 [00:03<00:25,  1.39s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  19%|█▉        | 4/21 [00:04<00:17,  1.01s/ba][A[A[A[A

Running tokenizer on dataset #2:  19%|█▉        | 4/21 [00:04<00:17,  1.04s/ba][A[A







Running tokenizer on dataset #8:  19%|█▉        | 4/21 [00:03<00:16,  1.02ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  19%|█▉        | 4/21 [00:04<00:17,  1.06s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  24%|██▍       | 5/21 [00:04<00:15,  1.05ba/s]



Running tokenizer on dataset #4:  24%|██▍       | 5/21 [00:04<00:15,  1.03ba/s][A[A[A[A




Running tokenizer on dataset #5:  24%|██▍       | 5/21 [00:04<00:15,  1.02ba/s][A[A[A[A[A
Running tokenizer on dataset #1:  24%|██▍       | 5/21 [00:05<00:16,  1.01s/ba][A





Running tokenizer on dataset #6:  19%|█▉        | 4/21 [00:04<00:22,  1.33s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  24%|██▍       | 5/21 [00:05<00:16,  1.00s/ba][A[A







Running tokenizer on dataset #8:  24%|██▍       | 5/21 [00:04<00:15,  1.03ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  24%|██▍       | 5/21 [00:05<00:16,  1.01s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  29%|██▊       | 6/21 [00:05<00:14,  1.06ba/s]


Running tokenizer on dataset #3:  19%|█▉        | 4/21 [00:05<00:28,  1.70s/ba][A[A[A



Running tokenizer on dataset #4:  29%|██▊       | 6/21 [00:05<00:14,  1.04ba/s][A[A[A[A




Running tokenizer on dataset #5:  29%|██▊       | 6/21 [00:05<00:14,  1.03ba/s][A[A[A[A[A
Running tokenizer on dataset #1:  29%|██▊       | 6/21 [00:06<00:14,  1.01ba/s][A

Running tokenizer on dataset #2:  29%|██▊       | 6/21 [00:06<00:14,  1.01ba/s][A[A







Running tokenizer on dataset #8:  29%|██▊       | 6/21 [00:05<00:14,  1.04ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  29%|██▊       | 6/21 [00:06<00:14,  1.01ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  33%|███▎      | 7/21 [00:06<00:13,  1.06ba/s]






Running tokenizer on dataset #7:  19%|█▉        | 4/21 [00:06<00:32,  1.92s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  33%|███▎      | 7/21 [00:06<00:13,  1.05ba/s][A[A[A[A[A



Running tokenizer on dataset #4:  33%|███▎      | 7/21 [00:06<00:13,  1.04ba/s][A[A[A[A
Running tokenizer on dataset #1:  33%|███▎      | 7/21 [00:07<00:13,  1.01ba/s][A

Running tokenizer on dataset #2:  33%|███▎      | 7/21 [00:07<00:13,  1.02ba/s][A[A







Running tokenizer on dataset #8:  33%|███▎      | 7/21 [00:06<00:13,  1.04ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  33%|███▎      | 7/21 [00:07<00:13,  1.01ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  38%|███▊      | 8/21 [00:07<00:12,  1.06ba/s]




Running tokenizer on dataset #5:  38%|███▊      | 8/21 [00:07<00:12,  1.05ba/s][A[A[A[A[A



Running tokenizer on dataset #4:  38%|███▊      | 8/21 [00:07<00:12,  1.04ba/s][A[A[A[A
Running tokenizer on dataset #1:  38%|███▊      | 8/21 [00:07<00:12,  1.02ba/s][A

Running tokenizer on dataset #2:  38%|███▊      | 8/21 [00:08<00:12,  1.03ba/s][A[A







Running tokenizer on dataset #8:  38%|███▊      | 8/21 [00:07<00:12,  1.05ba/s][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  24%|██▍       | 5/21 [00:08<00:31,  1.98s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  38%|███▊      | 8/21 [00:08<00:12,  1.01ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  43%|████▎     | 9/21 [00:08<00:11,  1.07ba/s]


Running tokenizer on dataset #3:  24%|██▍       | 5/21 [00:08<00:33,  2.10s/ba][A[A[A




Running tokenizer on dataset #5:  43%|████▎     | 9/21 [00:08<00:11,  1.05ba/s][A[A[A[A[A



Running tokenizer on dataset #4:  43%|████▎     | 9/21 [00:08<00:11,  1.05ba/s][A[A[A[A
Running tokenizer on dataset #1:  43%|████▎     | 9/21 [00:08<00:11,  1.03ba/s][A

Running tokenizer on dataset #2:  43%|████▎     | 9/21 [00:08<00:11,  1.04ba/s][A[A







Running tokenizer on dataset #8:  43%|████▎     | 9/21 [00:08<00:11,  1.04ba/s][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  48%|████▊     | 10/21 [00:09<00:10,  1.09ba/s]








Running tokenizer on dataset #9:  43%|████▎     | 9/21 [00:09<00:11,  1.02ba/s][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  24%|██▍       | 5/21 [00:09<00:35,  2.23s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  48%|████▊     | 10/21 [00:09<00:10,  1.07ba/s][A[A[A[A




Running tokenizer on dataset #5:  48%|████▊     | 10/21 [00:09<00:10,  1.04ba/s][A[A[A[A[A
Running tokenizer on dataset #1:  48%|████▊     | 10/21 [00:09<00:10,  1.04ba/s][A

Running tokenizer on dataset #2:  48%|████▊     | 10/21 [00:09<00:10,  1.05ba/s][A[A







Running tokenizer on dataset #8:  48%|████▊     | 10/21 [00:09<00:10,  1.04ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  48%|████▊     | 10/21 [00:09<00:10,  1.05ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  52%|█████▏    | 11/21 [00:10<00:10,  1.02s/ba]


Running tokenizer on dataset #3:  29%|██▊       | 6/21 [00:10<00:32,  2.14s/ba][A[A[A



Running tokenizer on dataset #4:  52%|█████▏    | 11/21 [00:10<00:10,  1.04s/ba][A[A[A[A




Running tokenizer on dataset #5:  52%|█████▏    | 11/21 [00:10<00:10,  1.06s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  52%|█████▏    | 11/21 [00:10<00:10,  1.01s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  52%|█████▏    | 11/21 [00:11<00:10,  1.06s/ba][A





Running tokenizer on dataset #6:  29%|██▊       | 6/21 [00:10<00:34,  2.30s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  52%|█████▏    | 11/21 [00:11<00:10,  1.04s/ba][A[A








Running tokenizer on dataset #9:  52%|█████▏    | 11/21 [00:11<00:09,  1.00ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  57%|█████▋    | 12/21 [00:11<00:08,  1.00ba/s]



Running tokenizer on dataset #4:  57%|█████▋    | 12/21 [00:11<00:09,  1.02s/ba][A[A[A[A




Running tokenizer on dataset #5:  57%|█████▋    | 12/21 [00:11<00:09,  1.02s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  57%|█████▋    | 12/21 [00:11<00:08,  1.01ba/s][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  57%|█████▋    | 12/21 [00:12<00:09,  1.03s/ba][A

Running tokenizer on dataset #2:  57%|█████▋    | 12/21 [00:12<00:09,  1.01s/ba][A[A








Running tokenizer on dataset #9:  57%|█████▋    | 12/21 [00:11<00:08,  1.02ba/s][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  29%|██▊       | 6/21 [00:12<00:35,  2.39s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  62%|██████▏   | 13/21 [00:12<00:07,  1.01ba/s]







Running tokenizer on dataset #8:  62%|██████▏   | 13/21 [00:12<00:07,  1.04ba/s][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  62%|██████▏   | 13/21 [00:12<00:07,  1.01ba/s][A[A[A[A[A



Running tokenizer on dataset #4:  62%|██████▏   | 13/21 [00:12<00:08,  1.00s/ba][A[A[A[A
Running tokenizer on dataset #1:  62%|██████▏   | 13/21 [00:13<00:08,  1.00s/ba][A

Running tokenizer on dataset #2:  62%|██████▏   | 13/21 [00:13<00:07,  1.01ba/s][A[A








Running tokenizer on dataset #9:  62%|██████▏   | 13/21 [00:12<00:07,  1.03ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  67%|██████▋   | 14/21 [00:13<00:06,  1.03ba/s]


Running tokenizer on dataset #3:  33%|███▎      | 7/21 [00:13<00:32,  2.35s/ba][A[A[A







Running tokenizer on dataset #8:  67%|██████▋   | 14/21 [00:13<00:06,  1.05ba/s][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  67%|██████▋   | 14/21 [00:13<00:06,  1.02ba/s][A[A[A[A[A



Running tokenizer on dataset #4:  67%|██████▋   | 14/21 [00:13<00:06,  1.01ba/s][A[A[A[A
Running tokenizer on dataset #1:  67%|██████▋   | 14/21 [00:14<00:06,  1.01ba/s][A

Running tokenizer on dataset #2:  67%|██████▋   | 14/21 [00:13<00:06,  1.02ba/s][A[A





Running tokenizer on dataset #6:  33%|███▎      | 7/21 [00:13<00:35,  2.53s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  67%|██████▋   | 14/21 [00:13<00:06,  1.03ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  71%|███████▏  | 15/21 [00:14<00:05,  1.05ba/s]




Running tokenizer on dataset #5:  71%|███████▏  | 15/21 [00:14<00:05,  1.03ba/s][A[A[A[A[A







Running tokenizer on dataset #8:  71%|███████▏  | 15/21 [00:14<00:05,  1.04ba/s][A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  71%|███████▏  | 15/21 [00:14<00:05,  1.02ba/s][A[A[A[A
Running tokenizer on dataset #1:  71%|███████▏  | 15/21 [00:14<00:05,  1.03ba/s][A

Running tokenizer on dataset #2:  71%|███████▏  | 15/21 [00:14<00:05,  1.03ba/s][A[A






Running tokenizer on dataset #7:  33%|███▎      | 7/21 [00:14<00:35,  2.53s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  71%|███████▏  | 15/21 [00:14<00:05,  1.04ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  76%|███████▌  | 16/21 [00:15<00:04,  1.05ba/s]




Running tokenizer on dataset #5:  76%|███████▌  | 16/21 [00:15<00:04,  1.04ba/s][A[A[A[A[A







Running tokenizer on dataset #8:  76%|███████▌  | 16/21 [00:15<00:04,  1.04ba/s][A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  76%|███████▌  | 16/21 [00:15<00:04,  1.02ba/s][A[A[A[A
Running tokenizer on dataset #1:  76%|███████▌  | 16/21 [00:15<00:04,  1.04ba/s][A

Running tokenizer on dataset #2:  76%|███████▌  | 16/21 [00:15<00:04,  1.03ba/s][A[A








Running tokenizer on dataset #9:  76%|███████▌  | 16/21 [00:15<00:04,  1.05ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  81%|████████  | 17/21 [00:16<00:03,  1.07ba/s]


Running tokenizer on dataset #3:  38%|███▊      | 8/21 [00:16<00:32,  2.49s/ba][A[A[A




Running tokenizer on dataset #5:  81%|████████  | 17/21 [00:16<00:03,  1.05ba/s][A[A[A[A[A







Running tokenizer on dataset #8:  81%|████████  | 17/21 [00:16<00:03,  1.05ba/s][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  81%|████████  | 17/21 [00:16<00:03,  1.05ba/s][A



Running tokenizer on dataset #4:  81%|████████  | 17/21 [00:16<00:03,  1.02ba/s][A[A[A[A

Running tokenizer on dataset #2:  81%|████████  | 17/21 [00:16<00:03,  1.03ba/s][A[A





Running tokenizer on dataset #6:  38%|███▊      | 8/21 [00:16<00:33,  2.60s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  81%|████████  | 17/21 [00:16<00:03,  1.05ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  86%|████████▌ | 18/21 [00:17<00:02,  1.06ba/s]







Running tokenizer on dataset #8:  86%|████████▌ | 18/21 [00:17<00:02,  1.06ba/s][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  86%|████████▌ | 18/21 [00:17<00:02,  1.05ba/s][A[A[A[A[A
Running tokenizer on dataset #1:  86%|████████▌ | 18/21 [00:17<00:02,  1.04ba/s][A






Running tokenizer on dataset #7:  38%|███▊      | 8/21 [00:17<00:33,  2.58s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  86%|████████▌ | 18/21 [00:17<00:02,  1.02ba/s][A[A[A[A

Running tokenizer on dataset #2:  86%|████████▌ | 18/21 [00:17<00:02,  1.03ba/s][A[A








Running tokenizer on dataset #9:  86%|████████▌ | 18/21 [00:17<00:02,  1.06ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  90%|█████████ | 19/21 [00:18<00:01,  1.07ba/s]




Running tokenizer on dataset #5:  90%|█████████ | 19/21 [00:18<00:01,  1.05ba/s][A[A[A[A[A







Running tokenizer on dataset #8:  90%|█████████ | 19/21 [00:18<00:01,  1.05ba/s][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  90%|█████████ | 19/21 [00:18<00:01,  1.04ba/s][A



Running tokenizer on dataset #4:  90%|█████████ | 19/21 [00:18<00:01,  1.03ba/s][A[A[A[A

Running tokenizer on dataset #2:  90%|█████████ | 19/21 [00:18<00:01,  1.03ba/s][A[A








Running tokenizer on dataset #9:  90%|█████████ | 19/21 [00:18<00:01,  1.08ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  95%|█████████▌| 20/21 [00:19<00:00,  1.07ba/s]Running tokenizer on dataset #0: 100%|██████████| 21/21 [00:19<00:00,  1.31ba/s]Running tokenizer on dataset #0: 100%|██████████| 21/21 [00:19<00:00,  1.08ba/s]



Running tokenizer on dataset #3:  43%|████▎     | 9/21 [00:19<00:31,  2.65s/ba][A[A[A




Running tokenizer on dataset #5:  95%|█████████▌| 20/21 [00:19<00:00,  1.06ba/s][A[A[A[A[A







Running tokenizer on dataset #8:  95%|█████████▌| 20/21 [00:19<00:00,  1.04ba/s][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  95%|█████████▌| 20/21 [00:19<00:00,  1.05ba/s][A



Running tokenizer on dataset #4:  95%|█████████▌| 20/21 [00:19<00:00,  1.04ba/s][A[A[A[A





Running tokenizer on dataset #6:  43%|████▎     | 9/21 [00:19<00:32,  2.68s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  95%|█████████▌| 20/21 [00:19<00:00,  1.08ba/s][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  95%|█████████▌| 20/21 [00:19<00:00,  1.02ba/s][A[A




Running tokenizer on dataset #5: 100%|██████████| 21/21 [00:19<00:00,  1.28ba/s][A[A[A[A[ARunning tokenizer on dataset #5: 100%|██████████| 21/21 [00:19<00:00,  1.06ba/s]








Running tokenizer on dataset #8: 100%|██████████| 21/21 [00:19<00:00,  1.27ba/s][A[A[A[A[A[A[A[ARunning tokenizer on dataset #8: 100%|██████████| 21/21 [00:19<00:00,  1.07ba/s]

Running tokenizer on dataset #1: 100%|██████████| 21/21 [00:20<00:00,  1.27ba/s][ARunning tokenizer on dataset #1: 100%|██████████| 21/21 [00:20<00:00,  1.05ba/s]




Running tokenizer on dataset #4: 100%|██████████| 21/21 [00:20<00:00,  1.26ba/s][A[A[A[ARunning tokenizer on dataset #4: 100%|██████████| 21/21 [00:20<00:00,  1.05ba/s]









Running tokenizer on dataset #9: 100%|██████████| 21/21 [00:19<00:00,  1.31ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #9: 100%|██████████| 21/21 [00:19<00:00,  1.06ba/s]


Running tokenizer on dataset #2: 100%|██████████| 21/21 [00:20<00:00,  1.25ba/s][A[ARunning tokenizer on dataset #2: 100%|██████████| 21/21 [00:20<00:00,  1.04ba/s]







Running tokenizer on dataset #7:  43%|████▎     | 9/21 [00:20<00:31,  2.59s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  48%|████▊     | 10/21 [00:20<00:24,  2.26s/ba][A[A[A[A[A[A


Running tokenizer on dataset #3:  48%|████▊     | 10/21 [00:21<00:26,  2.38s/ba][A[A[A






Running tokenizer on dataset #7:  48%|████▊     | 10/21 [00:21<00:23,  2.18s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  52%|█████▏    | 11/21 [00:22<00:19,  1.96s/ba][A[A[A[A[A[A


Running tokenizer on dataset #3:  52%|█████▏    | 11/21 [00:22<00:20,  2.02s/ba][A[A[A






Running tokenizer on dataset #7:  52%|█████▏    | 11/21 [00:22<00:19,  1.91s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  57%|█████▋    | 12/21 [00:23<00:15,  1.68s/ba][A[A[A





Running tokenizer on dataset #6:  57%|█████▋    | 12/21 [00:23<00:15,  1.68s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  57%|█████▋    | 12/21 [00:23<00:14,  1.62s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  62%|██████▏   | 13/21 [00:24<00:11,  1.45s/ba][A[A[A





Running tokenizer on dataset #6:  62%|██████▏   | 13/21 [00:24<00:11,  1.48s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  62%|██████▏   | 13/21 [00:24<00:11,  1.41s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  67%|██████▋   | 14/21 [00:25<00:08,  1.28s/ba][A[A[A





Running tokenizer on dataset #6:  67%|██████▋   | 14/21 [00:25<00:09,  1.31s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  67%|██████▋   | 14/21 [00:25<00:08,  1.27s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  71%|███████▏  | 15/21 [00:26<00:07,  1.19s/ba][A[A[A





Running tokenizer on dataset #6:  71%|███████▏  | 15/21 [00:26<00:07,  1.20s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  71%|███████▏  | 15/21 [00:26<00:06,  1.16s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  76%|███████▌  | 16/21 [00:27<00:05,  1.11s/ba][A[A[A





Running tokenizer on dataset #6:  76%|███████▌  | 16/21 [00:27<00:05,  1.12s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  76%|███████▌  | 16/21 [00:27<00:05,  1.08s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  81%|████████  | 17/21 [00:27<00:04,  1.06s/ba][A[A[A





Running tokenizer on dataset #6:  81%|████████  | 17/21 [00:27<00:04,  1.06s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  81%|████████  | 17/21 [00:28<00:04,  1.03s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  86%|████████▌ | 18/21 [00:28<00:03,  1.02s/ba][A[A[A





Running tokenizer on dataset #6:  86%|████████▌ | 18/21 [00:28<00:03,  1.02s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  86%|████████▌ | 18/21 [00:29<00:02,  1.00ba/s][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  90%|█████████ | 19/21 [00:29<00:02,  1.00s/ba][A[A[A





Running tokenizer on dataset #6:  90%|█████████ | 19/21 [00:29<00:01,  1.00ba/s][A[A[A[A[A[A






Running tokenizer on dataset #7:  90%|█████████ | 19/21 [00:30<00:01,  1.02ba/s][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  95%|█████████▌| 20/21 [00:30<00:00,  1.02ba/s][A[A[A





Running tokenizer on dataset #6:  95%|█████████▌| 20/21 [00:30<00:00,  1.03ba/s][A[A[A[A[A[A


Running tokenizer on dataset #3: 100%|██████████| 21/21 [00:31<00:00,  1.25ba/s][A[A[ARunning tokenizer on dataset #3: 100%|██████████| 21/21 [00:31<00:00,  1.48s/ba]






Running tokenizer on dataset #6: 100%|██████████| 21/21 [00:31<00:00,  1.25ba/s][A[A[A[A[A[ARunning tokenizer on dataset #6: 100%|██████████| 21/21 [00:31<00:00,  1.48s/ba]







Running tokenizer on dataset #7:  95%|█████████▌| 20/21 [00:31<00:00,  1.03ba/s][A[A[A[A[A[A[A






Running tokenizer on dataset #7: 100%|██████████| 21/21 [00:31<00:00,  1.28ba/s][A[A[A[A[A[A[ARunning tokenizer on dataset #7: 100%|██████████| 21/21 [00:31<00:00,  1.50s/ba]
Running tokenizer on dataset #0:   0%|          | 0/2 [00:00<?, ?ba/s]
Running tokenizer on dataset #1:   0%|          | 0/2 [00:00<?, ?ba/s][A

Running tokenizer on dataset #2:   0%|          | 0/2 [00:00<?, ?ba/s][A[A


Running tokenizer on dataset #3:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A



Running tokenizer on dataset #4:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A




Running tokenizer on dataset #5:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A





Running tokenizer on dataset #6:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A[A






Running tokenizer on dataset #7:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A[A[A







Running tokenizer on dataset #8:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  50%|█████     | 1/2 [00:01<00:01,  1.01s/ba]
Running tokenizer on dataset #1:  50%|█████     | 1/2 [00:00<00:00,  1.01ba/s][A

Running tokenizer on dataset #2:  50%|█████     | 1/2 [00:00<00:00,  1.01ba/s][A[A



Running tokenizer on dataset #4:  50%|█████     | 1/2 [00:00<00:00,  1.04ba/s][A[A[A[A


Running tokenizer on dataset #3:  50%|█████     | 1/2 [00:01<00:01,  1.01s/ba][A[A[A




Running tokenizer on dataset #5:  50%|█████     | 1/2 [00:00<00:00,  1.04ba/s][A[A[A[A[ARunning tokenizer on dataset #0: 100%|██████████| 2/2 [00:01<00:00,  1.99ba/s]





Running tokenizer on dataset #6:  50%|█████     | 1/2 [00:00<00:00,  1.06ba/s][A[A[A[A[A[ARunning tokenizer on dataset #0: 100%|██████████| 2/2 [00:01<00:00,  1.73ba/s]

Running tokenizer on dataset #1: 100%|██████████| 2/2 [00:01<00:00,  2.01ba/s][ARunning tokenizer on dataset #1: 100%|██████████| 2/2 [00:01<00:00,  1.75ba/s]


Running tokenizer on dataset #2: 100%|██████████| 2/2 [00:01<00:00,  2.05ba/s][A[ARunning tokenizer on dataset #2: 100%|██████████| 2/2 [00:01<00:00,  1.78ba/s]







Running tokenizer on dataset #7:  50%|█████     | 1/2 [00:00<00:00,  1.08ba/s][A[A[A[A[A[A[A


Running tokenizer on dataset #3: 100%|██████████| 2/2 [00:01<00:00,  2.01ba/s][A[A[ARunning tokenizer on dataset #3: 100%|██████████| 2/2 [00:01<00:00,  1.74ba/s]




Running tokenizer on dataset #4: 100%|██████████| 2/2 [00:01<00:00,  2.05ba/s][A[A[A[ARunning tokenizer on dataset #4: 100%|██████████| 2/2 [00:01<00:00,  1.78ba/s]





Running tokenizer on dataset #5: 100%|██████████| 2/2 [00:01<00:00,  2.09ba/s][A[A[A[A[ARunning tokenizer on dataset #5: 100%|██████████| 2/2 [00:01<00:00,  1.82ba/s]






Running tokenizer on dataset #6: 100%|██████████| 2/2 [00:01<00:00,  2.15ba/s][A[A[A[A[A[ARunning tokenizer on dataset #6: 100%|██████████| 2/2 [00:01<00:00,  1.86ba/s]







Running tokenizer on dataset #7: 100%|██████████| 2/2 [00:01<00:00,  2.16ba/s][A[A[A[A[A[A[ARunning tokenizer on dataset #7: 100%|██████████| 2/2 [00:01<00:00,  1.88ba/s]








Running tokenizer on dataset #8:  50%|█████     | 1/2 [00:01<00:01,  1.04s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  50%|█████     | 1/2 [00:01<00:01,  1.01s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8: 100%|██████████| 2/2 [00:01<00:00,  1.96ba/s][A[A[A[A[A[A[A[ARunning tokenizer on dataset #8: 100%|██████████| 2/2 [00:01<00:00,  1.70ba/s]









Running tokenizer on dataset #9: 100%|██████████| 2/2 [00:01<00:00,  2.04ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #9: 100%|██████████| 2/2 [00:01<00:00,  1.76ba/s]
Running tokenizer on dataset #0:   0%|          | 0/2 [00:00<?, ?ba/s]
Running tokenizer on dataset #1:   0%|          | 0/2 [00:00<?, ?ba/s][A

Running tokenizer on dataset #2:   0%|          | 0/2 [00:00<?, ?ba/s][A[A


Running tokenizer on dataset #3:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A



Running tokenizer on dataset #4:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A




Running tokenizer on dataset #5:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A





Running tokenizer on dataset #6:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A[A






Running tokenizer on dataset #7:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A[A[A







Running tokenizer on dataset #8:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:   0%|          | 0/2 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  50%|█████     | 1/2 [00:01<00:01,  1.00s/ba][A

Running tokenizer on dataset #2:  50%|█████     | 1/2 [00:01<00:01,  1.02s/ba][A[ARunning tokenizer on dataset #0:  50%|█████     | 1/2 [00:01<00:01,  1.06s/ba]


Running tokenizer on dataset #3:  50%|█████     | 1/2 [00:01<00:01,  1.01s/ba][A[A[A



Running tokenizer on dataset #4:  50%|█████     | 1/2 [00:01<00:01,  1.05s/ba][A[A[A[A
Running tokenizer on dataset #1: 100%|██████████| 2/2 [00:01<00:00,  2.00ba/s][ARunning tokenizer on dataset #1: 100%|██████████| 2/2 [00:01<00:00,  1.74ba/s]






Running tokenizer on dataset #6:  50%|█████     | 1/2 [00:00<00:00,  1.00ba/s][A[A[A[A[A[A

Running tokenizer on dataset #2: 100%|██████████| 2/2 [00:01<00:00,  2.00ba/s][A[ARunning tokenizer on dataset #2: 100%|██████████| 2/2 [00:01<00:00,  1.73ba/s]





Running tokenizer on dataset #5:  50%|█████     | 1/2 [00:01<00:01,  1.05s/ba][A[A[A[A[ARunning tokenizer on dataset #0: 100%|██████████| 2/2 [00:01<00:00,  1.89ba/s]Running tokenizer on dataset #0: 100%|██████████| 2/2 [00:01<00:00,  1.64ba/s]



Running tokenizer on dataset #3: 100%|██████████| 2/2 [00:01<00:00,  2.01ba/s][A[A[ARunning tokenizer on dataset #3: 100%|██████████| 2/2 [00:01<00:00,  1.74ba/s]








Running tokenizer on dataset #8:  50%|█████     | 1/2 [00:00<00:00,  1.05ba/s][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  50%|█████     | 1/2 [00:00<00:00,  1.02ba/s][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  50%|█████     | 1/2 [00:01<00:01,  1.05s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6: 100%|██████████| 2/2 [00:01<00:00,  2.04ba/s][A[A[A[A[A[ARunning tokenizer on dataset #6: 100%|██████████| 2/2 [00:01<00:00,  1.76ba/s]





Running tokenizer on dataset #5: 100%|██████████| 2/2 [00:01<00:00,  1.91ba/s][A[A[A[A[ARunning tokenizer on dataset #5: 100%|██████████| 2/2 [00:01<00:00,  1.66ba/s]




Running tokenizer on dataset #4: 100%|██████████| 2/2 [00:01<00:00,  1.76ba/s][A[A[A[ARunning tokenizer on dataset #4: 100%|██████████| 2/2 [00:01<00:00,  1.56ba/s]








Running tokenizer on dataset #8: 100%|██████████| 2/2 [00:01<00:00,  2.00ba/s][A[A[A[A[A[A[A[ARunning tokenizer on dataset #8: 100%|██████████| 2/2 [00:01<00:00,  1.76ba/s]









Running tokenizer on dataset #9: 100%|██████████| 2/2 [00:01<00:00,  1.98ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #9: 100%|██████████| 2/2 [00:01<00:00,  1.73ba/s]







Running tokenizer on dataset #7: 100%|██████████| 2/2 [00:01<00:00,  1.82ba/s][A[A[A[A[A[A[ARunning tokenizer on dataset #7: 100%|██████████| 2/2 [00:01<00:00,  1.60ba/s]
/home/sshukla7/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/3140 [00:00<?, ?it/s]  0%|          | 1/3140 [00:04<4:21:01,  4.99s/it]  0%|          | 2/3140 [00:07<3:17:24,  3.77s/it]  0%|          | 3/3140 [00:10<2:57:00,  3.39s/it]  0%|          | 4/3140 [00:13<2:46:52,  3.19s/it]  0%|          | 5/3140 [00:16<2:39:38,  3.06s/it]  0%|          | 6/3140 [00:19<2:36:20,  2.99s/it]  0%|          | 7/3140 [00:22<2:35:38,  2.98s/it]  0%|          | 8/3140 [00:24<2:26:58,  2.82s/it]  0%|          | 9/3140 [00:27<2:28:32,  2.85s/it]  0%|          | 10/3140 [00:30<2:24:39,  2.77s/it]  0%|          | 11/3140 [00:33<2:27:26,  2.83s/it]  0%|          | 12/3140 [00:36<2:28:33,  2.85s/it]  0%|          | 13/3140 [00:39<2:29:57,  2.88s/it]  0%|          | 14/3140 [00:42<2:29:33,  2.87s/it]  0%|          | 15/3140 [00:44<2:30:37,  2.89s/it]  1%|          | 16/3140 [00:47<2:31:32,  2.91s/it]  1%|          | 17/3140 [00:50<2:30:05,  2.88s/it]  1%|          | 18/3140 [00:53<2:30:39,  2.90s/it]  1%|          | 19/3140 [00:56<2:30:49,  2.90s/it]  1%|          | 20/3140 [00:59<2:31:07,  2.91s/it]  1%|          | 21/3140 [01:02<2:29:46,  2.88s/it]  1%|          | 22/3140 [01:05<2:30:50,  2.90s/it]  1%|          | 23/3140 [01:08<2:31:05,  2.91s/it]  1%|          | 24/3140 [01:11<2:31:04,  2.91s/it]  1%|          | 25/3140 [01:14<2:31:19,  2.91s/it]  1%|          | 26/3140 [01:16<2:30:31,  2.90s/it]  1%|          | 27/3140 [01:19<2:28:15,  2.86s/it]  1%|          | 28/3140 [01:22<2:28:02,  2.85s/it]  1%|          | 29/3140 [01:25<2:25:44,  2.81s/it]  1%|          | 30/3140 [01:28<2:27:19,  2.84s/it]  1%|          | 31/3140 [01:30<2:25:07,  2.80s/it]  1%|          | 32/3140 [01:33<2:26:28,  2.83s/it]  1%|          | 33/3140 [01:36<2:27:33,  2.85s/it]  1%|          | 34/3140 [01:39<2:28:25,  2.87s/it]  1%|          | 35/3140 [01:42<2:28:52,  2.88s/it]  1%|          | 36/3140 [01:45<2:29:32,  2.89s/it]  1%|          | 37/3140 [01:48<2:29:48,  2.90s/it]  1%|          | 38/3140 [01:51<2:29:29,  2.89s/it]  1%|          | 39/3140 [01:54<2:29:36,  2.89s/it]  1%|▏         | 40/3140 [01:56<2:29:48,  2.90s/it]  1%|▏         | 41/3140 [01:59<2:20:05,  2.71s/it]  1%|▏         | 42/3140 [02:02<2:23:18,  2.78s/it]  1%|▏         | 43/3140 [02:05<2:25:36,  2.82s/it]  1%|▏         | 44/3140 [02:07<2:25:49,  2.83s/it]  1%|▏         | 45/3140 [02:10<2:27:33,  2.86s/it]  1%|▏         | 46/3140 [02:13<2:23:32,  2.78s/it]  1%|▏         | 47/3140 [02:15<2:17:36,  2.67s/it]  2%|▏         | 48/3140 [02:18<2:21:07,  2.74s/it]  2%|▏         | 49/3140 [02:21<2:24:04,  2.80s/it]  2%|▏         | 50/3140 [02:24<2:26:18,  2.84s/it]  2%|▏         | 51/3140 [02:27<2:23:05,  2.78s/it]  2%|▏         | 52/3140 [02:29<2:20:34,  2.73s/it]  2%|▏         | 53/3140 [02:32<2:23:11,  2.78s/it]  2%|▏         | 54/3140 [02:35<2:25:09,  2.82s/it]  2%|▏         | 55/3140 [02:38<2:26:52,  2.86s/it]  2%|▏         | 56/3140 [02:41<2:27:58,  2.88s/it]  2%|▏         | 57/3140 [02:44<2:23:23,  2.79s/it]  2%|▏         | 58/3140 [02:47<2:25:21,  2.83s/it]  2%|▏         | 59/3140 [02:49<2:26:44,  2.86s/it]  2%|▏         | 60/3140 [02:52<2:25:15,  2.83s/it]  2%|▏         | 61/3140 [02:55<2:26:25,  2.85s/it]  2%|▏         | 62/3140 [02:58<2:27:34,  2.88s/it]  2%|▏         | 63/3140 [03:01<2:22:32,  2.78s/it]  2%|▏         | 64/3140 [03:04<2:24:47,  2.82s/it]  2%|▏         | 65/3140 [03:06<2:24:40,  2.82s/it]  2%|▏         | 66/3140 [03:09<2:26:27,  2.86s/it]  2%|▏         | 67/3140 [03:12<2:27:17,  2.88s/it]  2%|▏         | 68/3140 [03:15<2:21:07,  2.76s/it]  2%|▏         | 69/3140 [03:18<2:23:22,  2.80s/it]Traceback (most recent call last):
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_1.py", line 905, in <module>
    main()
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_1.py", line 737, in main
    accelerator.backward(total_loss / args.gradient_accumulation_steps)
  File "/home/sshukla7/.local/lib/python3.9/site-packages/accelerate/accelerator.py", line 408, in backward
    loss.backward(**kwargs)
  File "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 79.35 GiB total capacity; 50.73 GiB already allocated; 916.69 MiB free; 77.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1).
wandb: 
wandb: Run history:
wandb:      train/crs_att_loss ▁▇█
wandb:      train/dec_att_loss █▆▁
wandb:      train/dec_hid_loss █▅▁
wandb:      train/enc_att_loss ▂▁█
wandb: train/enc_hid_last_loss █▄▁
wandb:      train/enc_hid_loss █▄▁
wandb:       train/logits_loss █▄▁
wandb:              train/loss █▄▁
wandb:                train/lr ▁▅█
wandb:              train/step ▁▅█
wandb:         train/task_loss █▄▁
wandb: 
wandb: Run summary:
wandb:      train/crs_att_loss 0.01825
wandb:      train/dec_att_loss 0.02649
wandb:      train/dec_hid_loss 1.07951
wandb:      train/enc_att_loss 0.0007
wandb: train/enc_hid_last_loss 1.46796
wandb:      train/enc_hid_loss 0.31673
wandb:       train/logits_loss 2.8261
wandb:              train/loss 15.30305
wandb:                train/lr 1e-05
wandb:              train/step 59
wandb:         train/task_loss 9.58464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/sshukla7/sshukla7/DQ_BART/wandb/offline-run-20230404_152932-3s6jh3dx
wandb: Find logs at: ./wandb/offline-run-20230404_152932-3s6jh3dx/logs
