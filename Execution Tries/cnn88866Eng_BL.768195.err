
Lmod is automatically replacing "gnu9/9.3.0" with "gnu10/10.3.0-ya".


Inactive Modules:
  1) hwloc/2.1.0     2) libfabric     3) openmpi4     4) ucx

ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'
2023-05-07 19:50:58.555164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-07 19:51:00.993472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: Tracking run with wandb version 0.15.0
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Downloading builder script:   0%|          | 0.00/8.33k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 8.33k/8.33k [00:00<00:00, 13.2MB/s]
Downloading metadata:   0%|          | 0.00/9.88k [00:00<?, ?B/s]Downloading metadata: 100%|██████████| 9.88k/9.88k [00:00<00:00, 16.4MB/s]
Downloading readme:   0%|          | 0.00/15.1k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 15.1k/15.1k [00:00<00:00, 23.9MB/s]
Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]
Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s][A
Downloading data:   7%|▋         | 11.2M/159M [00:00<00:01, 112MB/s][A
Downloading data:  16%|█▌        | 24.8M/159M [00:00<00:01, 126MB/s][A
Downloading data:  24%|██▍       | 38.3M/159M [00:00<00:00, 130MB/s][A
Downloading data:  33%|███▎      | 52.1M/159M [00:00<00:00, 133MB/s][A
Downloading data:  42%|████▏     | 66.0M/159M [00:00<00:00, 135MB/s][A
Downloading data:  50%|█████     | 80.0M/159M [00:00<00:00, 137MB/s][A
Downloading data:  59%|█████▉    | 93.9M/159M [00:00<00:00, 138MB/s][A
Downloading data:  68%|██████▊   | 108M/159M [00:00<00:00, 138MB/s] [A
Downloading data:  77%|███████▋  | 122M/159M [00:00<00:00, 139MB/s][A
Downloading data:  86%|████████▌ | 136M/159M [00:01<00:00, 139MB/s][A
Downloading data:  94%|█████████▍| 150M/159M [00:01<00:00, 139MB/s][ADownloading data: 100%|██████████| 159M/159M [00:01<00:00, 136MB/s]
Downloading data files:  20%|██        | 1/5 [00:01<00:05,  1.48s/it]
Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s][A
Downloading data:   3%|▎         | 11.7M/376M [00:00<00:03, 117MB/s][A
Downloading data:   7%|▋         | 25.9M/376M [00:00<00:02, 131MB/s][A
Downloading data:  11%|█         | 40.1M/376M [00:00<00:02, 136MB/s][A
Downloading data:  14%|█▍        | 54.2M/376M [00:00<00:02, 138MB/s][A
Downloading data:  18%|█▊        | 68.5M/376M [00:00<00:02, 140MB/s][A
Downloading data:  22%|██▏       | 82.8M/376M [00:00<00:02, 141MB/s][A
Downloading data:  26%|██▌       | 96.9M/376M [00:00<00:01, 141MB/s][A
Downloading data:  30%|██▉       | 111M/376M [00:00<00:01, 141MB/s] [A
Downloading data:  33%|███▎      | 125M/376M [00:00<00:01, 142MB/s][A
Downloading data:  37%|███▋      | 140M/376M [00:01<00:01, 142MB/s][A
Downloading data:  41%|████      | 154M/376M [00:01<00:01, 142MB/s][A
Downloading data:  45%|████▍     | 168M/376M [00:01<00:01, 142MB/s][A
Downloading data:  48%|████▊     | 182M/376M [00:01<00:01, 142MB/s][A
Downloading data:  52%|█████▏    | 196M/376M [00:01<00:01, 142MB/s][A
Downloading data:  56%|█████▌    | 211M/376M [00:01<00:01, 142MB/s][A
Downloading data:  60%|█████▉    | 225M/376M [00:01<00:01, 142MB/s][A
Downloading data:  64%|██████▎   | 239M/376M [00:01<00:00, 142MB/s][A
Downloading data:  67%|██████▋   | 253M/376M [00:01<00:00, 141MB/s][A
Downloading data:  71%|███████   | 267M/376M [00:01<00:00, 141MB/s][A
Downloading data:  75%|███████▍  | 281M/376M [00:02<00:00, 141MB/s][A
Downloading data:  79%|███████▊  | 296M/376M [00:02<00:00, 141MB/s][A
Downloading data:  82%|████████▏ | 310M/376M [00:02<00:00, 141MB/s][A
Downloading data:  86%|████████▌ | 324M/376M [00:02<00:00, 141MB/s][A
Downloading data:  90%|████████▉ | 338M/376M [00:02<00:00, 140MB/s][A
Downloading data:  94%|█████████▎| 352M/376M [00:02<00:00, 141MB/s][A
Downloading data:  97%|█████████▋| 366M/376M [00:02<00:00, 141MB/s][ADownloading data: 100%|██████████| 376M/376M [00:02<00:00, 141MB/s]
Downloading data files:  40%|████      | 2/5 [00:04<00:07,  2.42s/it]
Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s][A
Downloading data: 17.9MB [00:00, 179MB/s]                    [A
Downloading data: 35.8MB [00:00, 170MB/s][ADownloading data: 46.4MB [00:00, 168MB/s]
Downloading data files:  60%|██████    | 3/5 [00:07<00:05,  2.58s/it]
Downloading data:   0%|          | 0.00/661k [00:00<?, ?B/s][ADownloading data: 2.43MB [00:00, 116MB/s]                   
Downloading data files:  80%|████████  | 4/5 [00:07<00:01,  1.67s/it]
Downloading data:   0%|          | 0.00/572k [00:00<?, ?B/s][ADownloading data: 2.11MB [00:00, 142MB/s]                   
Downloading data files: 100%|██████████| 5/5 [00:08<00:00,  1.24s/it]Downloading data files: 100%|██████████| 5/5 [00:08<00:00,  1.62s/it]
Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]Generating train split:   0%|          | 1/287113 [00:00<29:58:34,  2.66 examples/s]Generating train split:   0%|          | 489/287113 [00:00<03:32, 1345.79 examples/s]Generating train split:   0%|          | 1000/287113 [00:00<02:16, 2098.06 examples/s]Generating train split:   1%|          | 1564/287113 [00:00<01:34, 3017.48 examples/s]Generating train split:   1%|          | 2088/287113 [00:00<01:18, 3618.67 examples/s]Generating train split:   1%|          | 2659/287113 [00:00<01:07, 4200.48 examples/s]Generating train split:   1%|          | 3199/287113 [00:01<01:02, 4539.57 examples/s]Generating train split:   1%|▏         | 3980/287113 [00:01<00:59, 4787.90 examples/s]Generating train split:   2%|▏         | 4520/287113 [00:01<00:57, 4948.30 examples/s]Generating train split:   2%|▏         | 5043/287113 [00:01<00:56, 5024.43 examples/s]Generating train split:   2%|▏         | 5599/287113 [00:01<00:54, 5172.92 examples/s]Generating train split:   2%|▏         | 6414/287113 [00:01<00:53, 5237.09 examples/s]Generating train split:   2%|▏         | 6977/287113 [00:01<00:52, 5339.05 examples/s]Generating train split:   3%|▎         | 7779/287113 [00:01<00:52, 5338.62 examples/s]Generating train split:   3%|▎         | 8325/287113 [00:01<00:51, 5367.42 examples/s]Generating train split:   3%|▎         | 8873/287113 [00:02<00:51, 5396.86 examples/s]Generating train split:   3%|▎         | 9702/287113 [00:02<00:50, 5441.00 examples/s]Generating train split:   4%|▎         | 10513/287113 [00:02<00:50, 5424.89 examples/s]Generating train split:   4%|▍         | 11342/287113 [00:02<00:50, 5443.47 examples/s]Generating train split:   4%|▍         | 11906/287113 [00:02<00:50, 5486.62 examples/s]Generating train split:   4%|▍         | 12734/287113 [00:02<00:49, 5495.29 examples/s]Generating train split:   5%|▍         | 13289/287113 [00:02<00:49, 5501.15 examples/s]Generating train split:   5%|▍         | 14102/287113 [00:03<00:50, 5432.64 examples/s]Generating train split:   5%|▌         | 14682/287113 [00:03<00:49, 5518.39 examples/s]Generating train split:   5%|▌         | 15239/287113 [00:03<00:49, 5530.65 examples/s]Generating train split:   6%|▌         | 16030/287113 [00:03<00:49, 5433.51 examples/s]Generating train split:   6%|▌         | 16602/287113 [00:03<00:49, 5503.86 examples/s]Generating train split:   6%|▌         | 17438/287113 [00:03<00:48, 5526.92 examples/s]Generating train split:   6%|▋         | 18001/287113 [00:03<00:48, 5549.26 examples/s]Generating train split:   6%|▋         | 18558/287113 [00:03<00:48, 5552.13 examples/s]Generating train split:   7%|▋         | 19387/287113 [00:03<00:48, 5540.62 examples/s]Generating train split:   7%|▋         | 19945/287113 [00:04<00:48, 5549.89 examples/s]Generating train split:   7%|▋         | 20759/287113 [00:04<00:48, 5502.03 examples/s]Generating train split:   8%|▊         | 21594/287113 [00:04<00:48, 5520.19 examples/s]Generating train split:   8%|▊         | 22425/287113 [00:04<00:47, 5524.38 examples/s]Generating train split:   8%|▊         | 22991/287113 [00:04<00:47, 5553.60 examples/s]Generating train split:   8%|▊         | 23798/287113 [00:04<00:47, 5490.95 examples/s]Generating train split:   9%|▊         | 24633/287113 [00:04<00:47, 5512.93 examples/s]Generating train split:   9%|▉         | 25436/287113 [00:05<00:47, 5458.24 examples/s]Generating train split:   9%|▉         | 26000/287113 [00:05<00:47, 5481.48 examples/s]Generating train split:   9%|▉         | 26821/287113 [00:05<00:47, 5474.53 examples/s]Generating train split:  10%|▉         | 27639/287113 [00:05<00:47, 5457.59 examples/s]Generating train split:  10%|▉         | 28187/287113 [00:05<00:47, 5460.66 examples/s]Generating train split:  10%|█         | 28737/287113 [00:05<00:47, 5468.32 examples/s]Generating train split:  10%|█         | 29558/287113 [00:05<00:47, 5437.45 examples/s]Generating train split:  11%|█         | 30353/287113 [00:05<00:47, 5387.48 examples/s]Generating train split:  11%|█         | 30907/287113 [00:06<00:47, 5422.49 examples/s]Generating train split:  11%|█         | 31719/287113 [00:06<00:47, 5416.81 examples/s]Generating train split:  11%|█▏        | 32505/287113 [00:06<00:47, 5356.42 examples/s]Generating train split:  12%|█▏        | 33042/287113 [00:06<00:47, 5358.12 examples/s]Generating train split:  12%|█▏        | 33602/287113 [00:06<00:46, 5416.49 examples/s]Generating train split:  12%|█▏        | 34410/287113 [00:06<00:46, 5404.09 examples/s]Generating train split:  12%|█▏        | 35212/287113 [00:06<00:46, 5380.87 examples/s]Generating train split:  12%|█▏        | 35770/287113 [00:07<00:46, 5426.85 examples/s]Generating train split:  13%|█▎        | 36580/287113 [00:07<00:46, 5413.36 examples/s]Generating train split:  13%|█▎        | 37377/287113 [00:07<00:46, 5376.05 examples/s]Generating train split:  13%|█▎        | 37937/287113 [00:07<00:45, 5425.47 examples/s]Generating train split:  13%|█▎        | 38717/287113 [00:07<00:46, 5348.76 examples/s]Generating train split:  14%|█▍        | 39509/287113 [00:07<00:46, 5325.55 examples/s]Generating train split:  14%|█▍        | 40311/287113 [00:07<00:46, 5329.07 examples/s]Generating train split:  14%|█▍        | 40858/287113 [00:07<00:45, 5360.28 examples/s]Generating train split:  15%|█▍        | 41649/287113 [00:08<00:46, 5327.69 examples/s]Generating train split:  15%|█▍        | 42397/287113 [00:08<00:47, 5196.85 examples/s]Generating train split:  15%|█▍        | 42952/287113 [00:08<00:46, 5278.91 examples/s]Generating train split:  15%|█▌        | 43748/287113 [00:08<00:46, 5285.25 examples/s]Generating train split:  15%|█▌        | 44280/287113 [00:08<00:45, 5290.04 examples/s]Generating train split:  16%|█▌        | 44829/287113 [00:08<00:45, 5338.05 examples/s]Generating train split:  16%|█▌        | 45629/287113 [00:08<00:45, 5331.56 examples/s]Generating train split:  16%|█▌        | 46396/287113 [00:09<00:45, 5257.08 examples/s]Generating train split:  16%|█▋        | 46952/287113 [00:09<00:45, 5329.28 examples/s]Generating train split:  17%|█▋        | 47746/287113 [00:09<00:45, 5313.79 examples/s]Generating train split:  17%|█▋        | 48549/287113 [00:09<00:44, 5321.58 examples/s]Generating train split:  17%|█▋        | 49085/287113 [00:09<00:44, 5329.64 examples/s]Generating train split:  17%|█▋        | 49637/287113 [00:09<00:44, 5376.20 examples/s]Generating train split:  18%|█▊        | 50452/287113 [00:09<00:43, 5392.67 examples/s]Generating train split:  18%|█▊        | 51000/287113 [00:09<00:43, 5370.39 examples/s]Generating train split:  18%|█▊        | 51554/287113 [00:09<00:43, 5411.76 examples/s]Generating train split:  18%|█▊        | 52357/287113 [00:10<00:43, 5388.67 examples/s]Generating train split:  18%|█▊        | 52908/287113 [00:10<00:43, 5417.06 examples/s]Generating train split:  19%|█▊        | 53698/287113 [00:10<00:43, 5361.50 examples/s]Generating train split:  19%|█▉        | 54493/287113 [00:10<00:43, 5335.81 examples/s]Generating train split:  19%|█▉        | 55301/287113 [00:10<00:43, 5349.54 examples/s]Generating train split:  20%|█▉        | 56092/287113 [00:10<00:43, 5321.74 examples/s]Generating train split:  20%|█▉        | 56635/287113 [00:10<00:43, 5344.39 examples/s]Generating train split:  20%|█▉        | 57175/287113 [00:11<00:42, 5355.99 examples/s]Generating train split:  20%|██        | 57726/287113 [00:11<00:42, 5393.82 examples/s]Generating train split:  20%|██        | 58516/287113 [00:11<00:42, 5344.66 examples/s]Generating train split:  21%|██        | 59285/287113 [00:11<00:43, 5266.97 examples/s]Generating train split:  21%|██        | 59831/287113 [00:11<00:42, 5312.33 examples/s]Generating train split:  21%|██        | 60620/287113 [00:11<00:42, 5288.91 examples/s]Generating train split:  21%|██▏       | 61155/287113 [00:11<00:42, 5302.74 examples/s]Generating train split:  21%|██▏       | 61690/287113 [00:11<00:42, 5313.46 examples/s]Generating train split:  22%|██▏       | 62482/287113 [00:12<00:42, 5297.75 examples/s]Generating train split:  22%|██▏       | 63281/287113 [00:12<00:42, 5302.34 examples/s]Generating train split:  22%|██▏       | 64060/287113 [00:12<00:42, 5264.85 examples/s]Generating train split:  23%|██▎       | 64607/287113 [00:12<00:41, 5312.16 examples/s]Generating train split:  23%|██▎       | 65390/287113 [00:12<00:42, 5273.63 examples/s]Generating train split:  23%|██▎       | 65925/287113 [00:12<00:41, 5290.23 examples/s]Generating train split:  23%|██▎       | 66720/287113 [00:12<00:41, 5290.17 examples/s]Generating train split:  24%|██▎       | 67511/287113 [00:12<00:41, 5281.88 examples/s]Generating train split:  24%|██▎       | 68042/287113 [00:13<00:41, 5287.53 examples/s]Generating train split:  24%|██▍       | 68573/287113 [00:13<00:41, 5290.01 examples/s]Generating train split:  24%|██▍       | 69355/287113 [00:13<00:41, 5258.57 examples/s]Generating train split:  24%|██▍       | 69905/287113 [00:13<00:40, 5314.31 examples/s]Generating train split:  25%|██▍       | 70692/287113 [00:13<00:40, 5287.89 examples/s]Generating train split:  25%|██▍       | 71477/287113 [00:13<00:40, 5266.95 examples/s]Generating train split:  25%|██▌       | 72272/287113 [00:13<00:40, 5261.95 examples/s]Generating train split:  25%|██▌       | 72811/287113 [00:13<00:40, 5289.67 examples/s]Generating train split:  26%|██▌       | 73617/287113 [00:14<00:40, 5315.49 examples/s]Generating train split:  26%|██▌       | 74406/287113 [00:14<00:40, 5293.79 examples/s]Generating train split:  26%|██▌       | 74953/287113 [00:14<00:39, 5331.93 examples/s]Generating train split:  26%|██▋       | 75738/287113 [00:14<00:39, 5295.71 examples/s]Generating train split:  27%|██▋       | 76270/287113 [00:14<00:40, 5248.70 examples/s]Generating train split:  27%|██▋       | 76805/287113 [00:14<00:39, 5272.70 examples/s]Generating train split:  27%|██▋       | 77585/287113 [00:14<00:39, 5242.49 examples/s]Generating train split:  27%|██▋       | 78385/287113 [00:15<00:39, 5271.82 examples/s]Generating train split:  27%|██▋       | 78919/287113 [00:15<00:39, 5286.51 examples/s]Generating train split:  28%|██▊       | 79709/287113 [00:15<00:39, 5272.36 examples/s]Generating train split:  28%|██▊       | 80483/287113 [00:15<00:39, 5229.75 examples/s]Generating train split:  28%|██▊       | 81269/287113 [00:15<00:39, 5196.24 examples/s]Generating train split:  28%|██▊       | 81811/287113 [00:15<00:39, 5245.59 examples/s]Generating train split:  29%|██▉       | 82584/287113 [00:15<00:39, 5212.34 examples/s]Generating train split:  29%|██▉       | 83354/287113 [00:15<00:39, 5181.09 examples/s]Generating train split:  29%|██▉       | 83896/287113 [00:16<00:38, 5233.76 examples/s]Generating train split:  29%|██▉       | 84662/287113 [00:16<00:39, 5186.79 examples/s]Generating train split:  30%|██▉       | 85438/287113 [00:16<00:38, 5180.01 examples/s]Generating train split:  30%|██▉       | 85988/287113 [00:16<00:38, 5253.86 examples/s]Generating train split:  30%|███       | 86765/287113 [00:16<00:38, 5226.90 examples/s]Generating train split:  30%|███       | 87554/287113 [00:16<00:38, 5236.14 examples/s]Generating train split:  31%|███       | 88337/287113 [00:16<00:38, 5229.10 examples/s]Generating train split:  31%|███       | 88872/287113 [00:17<00:37, 5253.79 examples/s]Generating train split:  31%|███       | 89675/287113 [00:17<00:37, 5282.30 examples/s]Generating train split:  31%|███▏      | 90392/287113 [00:17<00:57, 3427.73 examples/s]Generating train split:  32%|███▏      | 90863/287113 [00:17<00:53, 3644.56 examples/s]Generating train split:  32%|███▏      | 91307/287113 [00:17<00:51, 3798.23 examples/s]Generating train split:  32%|███▏      | 91793/287113 [00:17<00:48, 4028.08 examples/s]Generating train split:  32%|███▏      | 92248/287113 [00:17<00:47, 4118.68 examples/s]Generating train split:  32%|███▏      | 92736/287113 [00:18<00:45, 4308.48 examples/s]Generating train split:  33%|███▎      | 93436/287113 [00:18<00:43, 4404.40 examples/s]Generating train split:  33%|███▎      | 93922/287113 [00:18<00:42, 4514.79 examples/s]Generating train split:  33%|███▎      | 94611/287113 [00:18<00:42, 4512.55 examples/s]Generating train split:  33%|███▎      | 95287/287113 [00:18<00:42, 4507.73 examples/s]Generating train split:  33%|███▎      | 95779/287113 [00:18<00:41, 4605.48 examples/s]Generating train split:  34%|███▎      | 96473/287113 [00:18<00:41, 4587.21 examples/s]Generating train split:  34%|███▍      | 96966/287113 [00:18<00:40, 4666.58 examples/s]Generating train split:  34%|███▍      | 97650/287113 [00:19<00:40, 4628.87 examples/s]Generating train split:  34%|███▍      | 98340/287113 [00:19<00:40, 4615.93 examples/s]Generating train split:  34%|███▍      | 99014/287113 [00:19<00:41, 4574.67 examples/s]Generating train split:  35%|███▍      | 99495/287113 [00:19<00:40, 4627.20 examples/s]Generating train split:  35%|███▍      | 99965/287113 [00:19<00:40, 4642.20 examples/s]Generating train split:  35%|███▌      | 100653/287113 [00:19<00:40, 4618.83 examples/s]Generating train split:  35%|███▌      | 101336/287113 [00:19<00:40, 4592.44 examples/s]Generating train split:  35%|███▌      | 101821/287113 [00:20<00:39, 4651.79 examples/s]Generating train split:  36%|███▌      | 102515/287113 [00:20<00:39, 4619.16 examples/s]Generating train split:  36%|███▌      | 103000/287113 [00:20<00:39, 4635.59 examples/s]Generating train split:  36%|███▌      | 103472/287113 [00:20<00:39, 4652.28 examples/s]Generating train split:  36%|███▌      | 103965/287113 [00:20<00:38, 4725.09 examples/s]Generating train split:  36%|███▋      | 104644/287113 [00:20<00:39, 4623.46 examples/s]Generating train split:  37%|███▋      | 105319/287113 [00:20<00:39, 4578.51 examples/s]Generating train split:  37%|███▋      | 105795/287113 [00:20<00:40, 4523.85 examples/s]Generating train split:  37%|███▋      | 106489/287113 [00:21<00:39, 4555.00 examples/s]Generating train split:  37%|███▋      | 106981/287113 [00:21<00:38, 4643.19 examples/s]Generating train split:  37%|███▋      | 107665/287113 [00:21<00:38, 4610.96 examples/s]Generating train split:  38%|███▊      | 108341/287113 [00:21<00:39, 4573.12 examples/s]Generating train split:  38%|███▊      | 108828/287113 [00:21<00:38, 4640.07 examples/s]Generating train split:  38%|███▊      | 109519/287113 [00:21<00:38, 4626.69 examples/s]Generating train split:  38%|███▊      | 110000/287113 [00:21<00:38, 4606.17 examples/s]Generating train split:  38%|███▊      | 110496/287113 [00:21<00:37, 4695.47 examples/s]Generating train split:  39%|███▊      | 111188/287113 [00:22<00:37, 4660.29 examples/s]Generating train split:  39%|███▉      | 111683/287113 [00:22<00:37, 4729.59 examples/s]Generating train split:  39%|███▉      | 112384/287113 [00:22<00:37, 4704.67 examples/s]Generating train split:  39%|███▉      | 112876/287113 [00:22<00:36, 4757.03 examples/s]Generating train split:  40%|███▉      | 113568/287113 [00:22<00:36, 4702.81 examples/s]Generating train split:  40%|███▉      | 114259/287113 [00:22<00:37, 4667.47 examples/s]Generating train split:  40%|███▉      | 114758/287113 [00:22<00:36, 4739.10 examples/s]Generating train split:  40%|████      | 115447/287113 [00:22<00:36, 4687.37 examples/s]Generating train split:  40%|████      | 115943/287113 [00:23<00:36, 4748.69 examples/s]Generating train split:  41%|████      | 116493/287113 [00:23<01:15, 2266.92 examples/s]Generating train split:  41%|████      | 116982/287113 [00:23<01:04, 2645.81 examples/s]Generating train split:  41%|████      | 117430/287113 [00:23<00:57, 2959.74 examples/s]Generating train split:  41%|████      | 117916/287113 [00:23<00:50, 3332.82 examples/s]Generating train split:  41%|████      | 118372/287113 [00:24<00:46, 3599.51 examples/s]Generating train split:  41%|████▏     | 118864/287113 [00:24<00:43, 3912.49 examples/s]Generating train split:  42%|████▏     | 119540/287113 [00:24<00:40, 4110.60 examples/s]Generating train split:  42%|████▏     | 120000/287113 [00:24<00:40, 4172.44 examples/s]Generating train split:  42%|████▏     | 120478/287113 [00:24<00:38, 4323.73 examples/s]Generating train split:  42%|████▏     | 120959/287113 [00:24<00:37, 4450.51 examples/s]Generating train split:  42%|████▏     | 121644/287113 [00:24<00:36, 4491.80 examples/s]Generating train split:  43%|████▎     | 122337/287113 [00:24<00:36, 4533.15 examples/s]Generating train split:  43%|████▎     | 122820/287113 [00:24<00:35, 4604.62 examples/s]Generating train split:  43%|████▎     | 123513/287113 [00:25<00:35, 4606.45 examples/s]Generating train split:  43%|████▎     | 123986/287113 [00:25<00:35, 4635.64 examples/s]Generating train split:  43%|████▎     | 124684/287113 [00:25<00:35, 4640.28 examples/s]Generating train split:  44%|████▎     | 125364/287113 [00:25<00:35, 4603.25 examples/s]Generating train split:  44%|████▍     | 125853/287113 [00:25<00:34, 4669.18 examples/s]Generating train split:  44%|████▍     | 126547/287113 [00:25<00:34, 4651.80 examples/s]Generating train split:  44%|████▍     | 127246/287113 [00:25<00:34, 4649.75 examples/s]Generating train split:  44%|████▍     | 127724/287113 [00:26<00:34, 4680.06 examples/s]Generating train split:  45%|████▍     | 128427/287113 [00:26<00:33, 4679.00 examples/s]Generating train split:  45%|████▍     | 128905/287113 [00:26<00:33, 4702.26 examples/s]Generating train split:  45%|████▌     | 129626/287113 [00:26<00:33, 4734.32 examples/s]Generating train split:  45%|████▌     | 130335/287113 [00:26<00:33, 4729.26 examples/s]Generating train split:  46%|████▌     | 130821/287113 [00:26<00:32, 4758.99 examples/s]Generating train split:  46%|████▌     | 131527/287113 [00:26<00:32, 4737.55 examples/s]Generating train split:  46%|████▌     | 132230/287113 [00:26<00:33, 4691.90 examples/s]Generating train split:  46%|████▌     | 132721/287113 [00:27<00:32, 4742.06 examples/s]Generating train split:  46%|████▋     | 133381/287113 [00:27<00:33, 4621.83 examples/s]Generating train split:  47%|████▋     | 133877/287113 [00:27<00:32, 4701.47 examples/s]Generating train split:  47%|████▋     | 134568/287113 [00:27<00:32, 4665.66 examples/s]Generating train split:  47%|████▋     | 135281/287113 [00:27<00:32, 4689.76 examples/s]Generating train split:  47%|████▋     | 135784/287113 [00:27<00:31, 4767.80 examples/s]Generating train split:  48%|████▊     | 136484/287113 [00:27<00:32, 4689.25 examples/s]Generating train split:  48%|████▊     | 136984/287113 [00:27<00:31, 4762.35 examples/s]Generating train split:  48%|████▊     | 137670/287113 [00:28<00:31, 4694.53 examples/s]Generating train split:  48%|████▊     | 138312/287113 [00:28<00:32, 4555.80 examples/s]Generating train split:  48%|████▊     | 138807/287113 [00:28<00:31, 4648.49 examples/s]Generating train split:  49%|████▊     | 139507/287113 [00:28<00:31, 4652.84 examples/s]Generating train split:  49%|████▉     | 140000/287113 [00:28<00:31, 4649.94 examples/s]Generating train split:  49%|████▉     | 140493/287113 [00:28<00:31, 4717.01 examples/s]Generating train split:  49%|████▉     | 140983/287113 [00:28<00:30, 4762.69 examples/s]Generating train split:  49%|████▉     | 141697/287113 [00:28<00:30, 4757.19 examples/s]Generating train split:  50%|████▉     | 142389/287113 [00:29<00:30, 4706.07 examples/s]Generating train split:  50%|████▉     | 142876/287113 [00:29<00:30, 4744.28 examples/s]Generating train split:  50%|█████     | 143563/287113 [00:29<00:30, 4677.29 examples/s]Generating train split:  50%|█████     | 144261/287113 [00:29<00:30, 4665.82 examples/s]Generating train split:  50%|█████     | 144750/287113 [00:29<00:30, 4717.19 examples/s]Generating train split:  51%|█████     | 145465/287113 [00:29<00:29, 4730.96 examples/s]Generating train split:  51%|█████     | 145959/287113 [00:29<00:29, 4779.00 examples/s]Generating train split:  51%|█████     | 146666/287113 [00:30<00:29, 4753.55 examples/s]Generating train split:  51%|█████▏    | 147363/287113 [00:30<00:29, 4716.22 examples/s]Generating train split:  51%|█████▏    | 147851/287113 [00:30<00:29, 4754.61 examples/s]Generating train split:  52%|█████▏    | 148547/287113 [00:30<00:29, 4712.58 examples/s]Generating train split:  52%|█████▏    | 149231/287113 [00:30<00:29, 4660.76 examples/s]Generating train split:  52%|█████▏    | 149717/287113 [00:30<00:29, 4705.20 examples/s]Generating train split:  52%|█████▏    | 150392/287113 [00:30<00:29, 4631.58 examples/s]Generating train split:  53%|█████▎    | 150886/287113 [00:30<00:28, 4703.25 examples/s]Generating train split:  53%|█████▎    | 151559/287113 [00:31<00:29, 4607.67 examples/s]Generating train split:  53%|█████▎    | 152262/287113 [00:31<00:29, 4629.48 examples/s]Generating train split:  53%|█████▎    | 152746/287113 [00:31<00:28, 4675.71 examples/s]Generating train split:  53%|█████▎    | 153440/287113 [00:31<00:28, 4653.85 examples/s]Generating train split:  54%|█████▎    | 153932/287113 [00:31<00:28, 4717.48 examples/s]Generating train split:  54%|█████▍    | 154634/287113 [00:31<00:28, 4701.58 examples/s]Generating train split:  54%|█████▍    | 155337/287113 [00:31<00:28, 4693.99 examples/s]Generating train split:  54%|█████▍    | 155837/287113 [00:32<00:27, 4764.27 examples/s]Generating train split:  55%|█████▍    | 156548/287113 [00:32<00:27, 4753.52 examples/s]Generating train split:  55%|█████▍    | 157264/287113 [00:32<00:27, 4753.86 examples/s]Generating train split:  55%|█████▍    | 157752/287113 [00:32<00:27, 4781.80 examples/s]Generating train split:  55%|█████▌    | 158248/287113 [00:32<00:27, 4765.17 examples/s]Generating train split:  55%|█████▌    | 158742/287113 [00:32<00:26, 4808.96 examples/s]Generating train split:  56%|█████▌    | 159470/287113 [00:32<00:26, 4822.93 examples/s]Generating train split:  56%|█████▌    | 159965/287113 [00:32<00:26, 4851.37 examples/s]Generating train split:  56%|█████▌    | 160452/287113 [00:32<00:26, 4851.63 examples/s]Generating train split:  56%|█████▌    | 160956/287113 [00:33<00:25, 4902.83 examples/s]Generating train split:  56%|█████▋    | 161676/287113 [00:33<00:25, 4860.02 examples/s]Generating train split:  57%|█████▋    | 162391/287113 [00:33<00:25, 4826.60 examples/s]Generating train split:  57%|█████▋    | 162904/287113 [00:33<00:25, 4900.64 examples/s]Generating train split:  57%|█████▋    | 163639/287113 [00:33<00:25, 4897.62 examples/s]Generating train split:  57%|█████▋    | 164363/287113 [00:33<00:25, 4871.13 examples/s]Generating train split:  57%|█████▋    | 164883/287113 [00:33<00:24, 4947.65 examples/s]Generating train split:  58%|█████▊    | 165599/287113 [00:34<00:24, 4885.61 examples/s]Generating train split:  58%|█████▊    | 166325/287113 [00:34<00:24, 4868.77 examples/s]Generating train split:  58%|█████▊    | 166853/287113 [00:34<00:24, 4964.49 examples/s]Generating train split:  58%|█████▊    | 167586/287113 [00:34<00:24, 4935.71 examples/s]Generating train split:  59%|█████▊    | 168311/287113 [00:34<00:24, 4897.02 examples/s]Generating train split:  59%|█████▉    | 168809/287113 [00:34<00:24, 4914.03 examples/s]Generating train split:  59%|█████▉    | 169510/287113 [00:34<00:24, 4830.76 examples/s]Generating train split:  59%|█████▉    | 170000/287113 [00:34<00:24, 4812.29 examples/s]Generating train split:  59%|█████▉    | 170498/287113 [00:35<00:24, 4852.57 examples/s]Generating train split:  60%|█████▉    | 171000/287113 [00:35<00:24, 4837.42 examples/s]Generating train split:  60%|█████▉    | 171491/287113 [00:35<00:23, 4856.25 examples/s]Generating train split:  60%|█████▉    | 172000/287113 [00:35<00:23, 4845.77 examples/s]Generating train split:  60%|██████    | 172511/287113 [00:35<00:23, 4917.74 examples/s]Generating train split:  60%|██████    | 173256/287113 [00:35<00:23, 4881.84 examples/s]Generating train split:  61%|██████    | 173767/287113 [00:35<00:22, 4939.50 examples/s]Generating train split:  61%|██████    | 174526/287113 [00:35<00:22, 4922.22 examples/s]Generating train split:  61%|██████    | 175267/287113 [00:35<00:22, 4924.57 examples/s]Generating train split:  61%|██████    | 175768/287113 [00:36<00:22, 4942.81 examples/s]Generating train split:  61%|██████▏   | 176509/287113 [00:36<00:22, 4907.87 examples/s]Generating train split:  62%|██████▏   | 177246/287113 [00:36<00:22, 4853.71 examples/s]Generating train split:  62%|██████▏   | 177757/287113 [00:36<00:22, 4911.71 examples/s]Generating train split:  62%|██████▏   | 178500/287113 [00:36<00:22, 4851.36 examples/s]Generating train split:  62%|██████▏   | 179000/287113 [00:36<00:22, 4862.94 examples/s]Generating train split:  63%|██████▎   | 179493/287113 [00:36<00:22, 4875.37 examples/s]Generating train split:  63%|██████▎   | 180000/287113 [00:36<00:22, 4835.38 examples/s]Generating train split:  63%|██████▎   | 180495/287113 [00:37<00:21, 4864.57 examples/s]Generating train split:  63%|██████▎   | 181000/287113 [00:37<00:21, 4869.72 examples/s]Generating train split:  63%|██████▎   | 181496/287113 [00:37<00:21, 4893.00 examples/s]Generating train split:  63%|██████▎   | 182000/287113 [00:37<00:21, 4848.07 examples/s]Generating train split:  64%|██████▎   | 182510/287113 [00:37<00:21, 4916.21 examples/s]Generating train split:  64%|██████▎   | 183009/287113 [00:37<00:21, 4934.41 examples/s]Generating train split:  64%|██████▍   | 183519/287113 [00:37<00:20, 4978.53 examples/s]Generating train split:  64%|██████▍   | 184276/287113 [00:37<00:20, 5001.54 examples/s]Generating train split:  64%|██████▍   | 184799/287113 [00:37<00:20, 5061.10 examples/s]Generating train split:  65%|██████▍   | 185538/287113 [00:38<00:20, 5008.47 examples/s]Generating train split:  65%|██████▍   | 186287/287113 [00:38<00:20, 5000.02 examples/s]Generating train split:  65%|██████▌   | 186828/287113 [00:38<00:19, 5097.75 examples/s]Generating train split:  65%|██████▌   | 187572/287113 [00:38<00:19, 5047.71 examples/s]Generating train split:  66%|██████▌   | 188331/287113 [00:38<00:19, 5051.09 examples/s]Generating train split:  66%|██████▌   | 188843/287113 [00:38<00:19, 5066.53 examples/s]Generating train split:  66%|██████▌   | 189579/287113 [00:38<00:19, 5010.55 examples/s]Generating train split:  66%|██████▋   | 190316/287113 [00:39<00:19, 4974.23 examples/s]Generating train split:  66%|██████▋   | 190838/287113 [00:39<00:19, 5031.57 examples/s]Generating train split:  67%|██████▋   | 191580/287113 [00:39<00:19, 4996.64 examples/s]Generating train split:  67%|██████▋   | 192084/287113 [00:39<00:18, 5005.62 examples/s]Generating train split:  67%|██████▋   | 192600/287113 [00:39<00:18, 5044.96 examples/s]Generating train split:  67%|██████▋   | 193358/287113 [00:39<00:18, 5043.67 examples/s]Generating train split:  68%|██████▊   | 194095/287113 [00:39<00:18, 4993.86 examples/s]Generating train split:  68%|██████▊   | 194617/287113 [00:39<00:18, 5045.46 examples/s]Generating train split:  68%|██████▊   | 195364/287113 [00:40<00:18, 5019.87 examples/s]Generating train split:  68%|██████▊   | 195884/287113 [00:40<00:18, 5063.25 examples/s]Generating train split:  68%|██████▊   | 196617/287113 [00:40<00:18, 4998.40 examples/s]Generating train split:  69%|██████▊   | 197384/287113 [00:40<00:17, 5032.75 examples/s]Generating train split:  69%|██████▉   | 197899/287113 [00:40<00:17, 5058.00 examples/s]Generating train split:  69%|██████▉   | 198649/287113 [00:40<00:17, 5034.26 examples/s]Generating train split:  69%|██████▉   | 199402/287113 [00:40<00:17, 5027.41 examples/s]Generating train split:  70%|██████▉   | 199916/287113 [00:40<00:17, 5049.85 examples/s]Generating train split:  70%|██████▉   | 200662/287113 [00:41<00:17, 4992.42 examples/s]Generating train split:  70%|███████   | 201405/287113 [00:41<00:17, 4975.04 examples/s]Generating train split:  70%|███████   | 201927/287113 [00:41<00:16, 5012.45 examples/s]Generating train split:  71%|███████   | 202432/287113 [00:41<00:16, 5017.12 examples/s]Generating train split:  71%|███████   | 202979/287113 [00:41<00:16, 5132.77 examples/s]Generating train split:  71%|███████   | 203710/287113 [00:41<00:16, 5036.54 examples/s]Generating train split:  71%|███████   | 204440/287113 [00:41<00:16, 4976.42 examples/s]Generating train split:  71%|███████▏  | 204957/287113 [00:41<00:16, 5021.30 examples/s]Generating train split:  72%|███████▏  | 205698/287113 [00:42<00:16, 4989.29 examples/s]Generating train split:  72%|███████▏  | 206454/287113 [00:42<00:16, 5002.46 examples/s]Generating train split:  72%|███████▏  | 207192/287113 [00:42<00:16, 4972.82 examples/s]Generating train split:  72%|███████▏  | 207707/287113 [00:42<00:15, 5013.38 examples/s]Generating train split:  73%|███████▎  | 208467/287113 [00:42<00:15, 5028.87 examples/s]Generating train split:  73%|███████▎  | 208994/287113 [00:42<00:15, 5083.60 examples/s]Generating train split:  73%|███████▎  | 209745/287113 [00:42<00:15, 5054.64 examples/s]Generating train split:  73%|███████▎  | 210514/287113 [00:43<00:15, 5029.57 examples/s]Generating train split:  74%|███████▎  | 211261/287113 [00:43<00:15, 4986.70 examples/s]Generating train split:  74%|███████▍  | 211767/287113 [00:43<00:15, 5001.80 examples/s]Generating train split:  74%|███████▍  | 212502/287113 [00:43<00:15, 4950.70 examples/s]Generating train split:  74%|███████▍  | 213000/287113 [00:43<00:15, 4925.12 examples/s]Generating train split:  74%|███████▍  | 213505/287113 [00:43<00:14, 4955.33 examples/s]Generating train split:  75%|███████▍  | 214254/287113 [00:43<00:14, 4942.25 examples/s]Generating train split:  75%|███████▍  | 214750/287113 [00:43<00:14, 4945.63 examples/s]Generating train split:  75%|███████▍  | 215259/287113 [00:44<00:14, 4917.88 examples/s]Generating train split:  75%|███████▌  | 215766/287113 [00:44<00:14, 4955.13 examples/s]Generating train split:  75%|███████▌  | 216519/287113 [00:44<00:14, 4906.43 examples/s]Generating train split:  76%|███████▌  | 217251/287113 [00:44<00:14, 4895.06 examples/s]Generating train split:  76%|███████▌  | 217784/287113 [00:44<00:13, 4999.17 examples/s]Generating train split:  76%|███████▌  | 218528/287113 [00:44<00:13, 4982.66 examples/s]Generating train split:  76%|███████▋  | 219276/287113 [00:44<00:13, 4983.32 examples/s]Generating train split:  77%|███████▋  | 219795/287113 [00:44<00:13, 5027.23 examples/s]Generating train split:  77%|███████▋  | 220564/287113 [00:45<00:13, 5057.31 examples/s]Generating train split:  77%|███████▋  | 221316/287113 [00:45<00:13, 5041.19 examples/s]Generating train split:  77%|███████▋  | 221864/287113 [00:45<00:12, 5143.17 examples/s]Generating train split:  78%|███████▊  | 222601/287113 [00:45<00:12, 5063.16 examples/s]Generating train split:  78%|███████▊  | 223323/287113 [00:45<00:12, 4980.10 examples/s]Generating train split:  78%|███████▊  | 224047/287113 [00:45<00:12, 4927.02 examples/s]Generating train split:  78%|███████▊  | 224556/287113 [00:45<00:12, 4962.87 examples/s]Generating train split:  78%|███████▊  | 225279/287113 [00:46<00:12, 4892.31 examples/s]Generating train split:  79%|███████▊  | 225790/287113 [00:46<00:12, 4942.17 examples/s]Generating train split:  79%|███████▉  | 226504/287113 [00:46<00:12, 4877.03 examples/s]Generating train split:  79%|███████▉  | 227000/287113 [00:46<00:12, 4819.83 examples/s]Generating train split:  79%|███████▉  | 227485/287113 [00:46<00:12, 4826.77 examples/s]Generating train split:  79%|███████▉  | 227986/287113 [00:46<00:12, 4872.64 examples/s]Generating train split:  80%|███████▉  | 228714/287113 [00:46<00:12, 4863.66 examples/s]Generating train split:  80%|███████▉  | 229446/287113 [00:46<00:11, 4864.48 examples/s]Generating train split:  80%|████████  | 229946/287113 [00:46<00:11, 4893.00 examples/s]Generating train split:  80%|████████  | 230679/287113 [00:47<00:11, 4886.68 examples/s]Generating train split:  81%|████████  | 231409/287113 [00:47<00:11, 4876.44 examples/s]Generating train split:  81%|████████  | 231917/287113 [00:47<00:11, 4922.44 examples/s]Generating train split:  81%|████████  | 232492/287113 [00:47<00:23, 2333.67 examples/s]Generating train split:  81%|████████  | 232998/287113 [00:48<00:19, 2719.62 examples/s]Generating train split:  81%|████████▏ | 233460/287113 [00:48<00:17, 3038.84 examples/s]Generating train split:  81%|████████▏ | 233958/287113 [00:48<00:15, 3413.89 examples/s]Generating train split:  82%|████████▏ | 234430/287113 [00:48<00:14, 3695.75 examples/s]Generating train split:  82%|████████▏ | 234923/287113 [00:48<00:13, 3986.18 examples/s]Generating train split:  82%|████████▏ | 235619/287113 [00:48<00:12, 4202.57 examples/s]Generating train split:  82%|████████▏ | 236091/287113 [00:48<00:11, 4326.35 examples/s]Generating train split:  82%|████████▏ | 236593/287113 [00:48<00:11, 4502.07 examples/s]Generating train split:  83%|████████▎ | 237255/287113 [00:48<00:11, 4455.92 examples/s]Generating train split:  83%|████████▎ | 237759/287113 [00:49<00:10, 4600.42 examples/s]Generating train split:  83%|████████▎ | 238472/287113 [00:49<00:10, 4650.95 examples/s]Generating train split:  83%|████████▎ | 238976/287113 [00:49<00:10, 4743.01 examples/s]Generating train split:  83%|████████▎ | 239683/287113 [00:49<00:10, 4729.47 examples/s]Generating train split:  84%|████████▎ | 240393/287113 [00:49<00:09, 4727.24 examples/s]Generating train split:  84%|████████▍ | 240894/287113 [00:49<00:09, 4792.90 examples/s]Generating train split:  84%|████████▍ | 241611/287113 [00:49<00:09, 4786.02 examples/s]Generating train split:  84%|████████▍ | 242095/287113 [00:49<00:09, 4796.63 examples/s]Generating train split:  84%|████████▍ | 242600/287113 [00:50<00:09, 4859.58 examples/s]Generating train split:  85%|████████▍ | 243317/287113 [00:50<00:09, 4827.00 examples/s]Generating train split:  85%|████████▍ | 243835/287113 [00:50<00:08, 4913.72 examples/s]Generating train split:  85%|████████▌ | 244528/287113 [00:50<00:08, 4808.78 examples/s]Generating train split:  85%|████████▌ | 245236/287113 [00:50<00:08, 4756.42 examples/s]Generating train split:  86%|████████▌ | 245742/287113 [00:50<00:08, 4827.19 examples/s]Generating train split:  86%|████████▌ | 246238/287113 [00:50<00:08, 4777.99 examples/s]Generating train split:  86%|████████▌ | 246745/287113 [00:50<00:08, 4852.73 examples/s]Generating train split:  86%|████████▌ | 247440/287113 [00:51<00:08, 4770.33 examples/s]Generating train split:  86%|████████▋ | 247946/287113 [00:51<00:08, 4840.22 examples/s]Generating train split:  87%|████████▋ | 248649/287113 [00:51<00:08, 4780.82 examples/s]Generating train split:  87%|████████▋ | 249365/287113 [00:51<00:07, 4775.21 examples/s]Generating train split:  87%|████████▋ | 249849/287113 [00:51<00:07, 4790.35 examples/s]Generating train split:  87%|████████▋ | 250575/287113 [00:51<00:07, 4805.78 examples/s]Generating train split:  88%|████████▊ | 251280/287113 [00:51<00:07, 4768.39 examples/s]Generating train split:  88%|████████▊ | 251770/287113 [00:51<00:07, 4799.10 examples/s]Generating train split:  88%|████████▊ | 252258/287113 [00:52<00:07, 4751.19 examples/s]Generating train split:  88%|████████▊ | 252751/287113 [00:52<00:07, 4795.56 examples/s]Generating train split:  88%|████████▊ | 253239/287113 [00:52<00:07, 4750.24 examples/s]Generating train split:  88%|████████▊ | 253748/287113 [00:52<00:06, 4839.46 examples/s]Generating train split:  89%|████████▊ | 254456/287113 [00:52<00:06, 4790.61 examples/s]Generating train split:  89%|████████▉ | 254958/287113 [00:52<00:06, 4845.18 examples/s]Generating train split:  89%|████████▉ | 255660/287113 [00:52<00:06, 4782.60 examples/s]Generating train split:  89%|████████▉ | 256379/287113 [00:52<00:06, 4780.91 examples/s]Generating train split:  89%|████████▉ | 256878/287113 [00:53<00:06, 4831.08 examples/s]Generating train split:  90%|████████▉ | 257581/287113 [00:53<00:06, 4778.72 examples/s]Generating train split:  90%|████████▉ | 258262/287113 [00:53<00:06, 4697.59 examples/s]Generating train split:  90%|█████████ | 258765/287113 [00:53<00:05, 4775.02 examples/s]Generating train split:  90%|█████████ | 259255/287113 [00:53<00:05, 4739.91 examples/s]Generating train split:  90%|█████████ | 259764/287113 [00:53<00:05, 4829.27 examples/s]Generating train split:  91%|█████████ | 260256/287113 [00:53<00:05, 4769.69 examples/s]Generating train split:  91%|█████████ | 260758/287113 [00:53<00:05, 4838.96 examples/s]Generating train split:  91%|█████████ | 261499/287113 [00:53<00:05, 4822.91 examples/s]Generating train split:  91%|█████████▏| 262000/287113 [00:54<00:05, 4805.65 examples/s]Generating train split:  91%|█████████▏| 262504/287113 [00:54<00:05, 4864.86 examples/s]Generating train split:  92%|█████████▏| 263000/287113 [00:54<00:04, 4832.61 examples/s]Generating train split:  92%|█████████▏| 263516/287113 [00:54<00:04, 4922.34 examples/s]Generating train split:  92%|█████████▏| 264252/287113 [00:54<00:04, 4849.56 examples/s]Generating train split:  92%|█████████▏| 264753/287113 [00:54<00:04, 4887.74 examples/s]Generating train split:  92%|█████████▏| 265262/287113 [00:54<00:04, 4876.04 examples/s]Generating train split:  93%|█████████▎| 265784/287113 [00:54<00:04, 4969.20 examples/s]Generating train split:  93%|█████████▎| 266524/287113 [00:55<00:04, 4936.76 examples/s]Generating train split:  93%|█████████▎| 267259/287113 [00:55<00:04, 4919.78 examples/s]Generating train split:  93%|█████████▎| 267787/287113 [00:55<00:03, 5007.46 examples/s]Generating train split:  94%|█████████▎| 268501/287113 [00:55<00:03, 4917.01 examples/s]Generating train split:  94%|█████████▎| 269000/287113 [00:55<00:03, 4920.77 examples/s]Generating train split:  94%|█████████▍| 269521/287113 [00:55<00:03, 4993.90 examples/s]Generating train split:  94%|█████████▍| 270268/287113 [00:55<00:03, 4984.66 examples/s]Generating train split:  94%|█████████▍| 270790/287113 [00:55<00:03, 5042.75 examples/s]Generating train split:  95%|█████████▍| 271522/287113 [00:56<00:03, 4953.86 examples/s]Generating train split:  95%|█████████▍| 272253/287113 [00:56<00:03, 4924.84 examples/s]Generating train split:  95%|█████████▌| 272765/287113 [00:56<00:02, 4968.84 examples/s]Generating train split:  95%|█████████▌| 273507/287113 [00:56<00:02, 4923.82 examples/s]Generating train split:  96%|█████████▌| 274260/287113 [00:56<00:02, 4944.72 examples/s]Generating train split:  96%|█████████▌| 274777/287113 [00:56<00:02, 4972.95 examples/s]Generating train split:  96%|█████████▌| 275516/287113 [00:56<00:02, 4955.29 examples/s]Generating train split:  96%|█████████▌| 276258/287113 [00:56<00:02, 4901.01 examples/s]Generating train split:  96%|█████████▋| 276767/287113 [00:57<00:02, 4941.11 examples/s]Generating train split:  97%|█████████▋| 277520/287113 [00:57<00:01, 4923.33 examples/s]Generating train split:  97%|█████████▋| 278243/287113 [00:57<00:01, 4885.64 examples/s]Generating train split:  97%|█████████▋| 278759/287113 [00:57<00:01, 4946.37 examples/s]Generating train split:  97%|█████████▋| 279488/287113 [00:57<00:01, 4863.90 examples/s]Generating train split:  98%|█████████▊| 280000/287113 [00:57<00:01, 4841.28 examples/s]Generating train split:  98%|█████████▊| 280498/287113 [00:57<00:01, 4874.53 examples/s]Generating train split:  98%|█████████▊| 281000/287113 [00:57<00:01, 4860.21 examples/s]Generating train split:  98%|█████████▊| 281503/287113 [00:58<00:01, 4905.13 examples/s]Generating train split:  98%|█████████▊| 282000/287113 [00:58<00:01, 4864.53 examples/s]Generating train split:  98%|█████████▊| 282502/287113 [00:58<00:00, 4905.54 examples/s]Generating train split:  99%|█████████▊| 282996/287113 [00:58<00:00, 4909.13 examples/s]Generating train split:  99%|█████████▉| 283715/287113 [00:58<00:00, 4861.44 examples/s]Generating train split:  99%|█████████▉| 284405/287113 [00:58<00:00, 4765.27 examples/s]Generating train split:  99%|█████████▉| 284892/287113 [00:58<00:00, 4790.12 examples/s]Generating train split:  99%|█████████▉| 285559/287113 [00:58<00:00, 4668.65 examples/s]Generating train split: 100%|█████████▉| 286273/287113 [00:59<00:00, 4692.73 examples/s]Generating train split: 100%|█████████▉| 286787/287113 [00:59<00:00, 4799.60 examples/s]                                                                                        Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]Generating validation split:   0%|          | 1/13368 [00:07<29:37:52,  7.98s/ examples]Generating validation split:   4%|▍         | 575/13368 [00:08<02:06, 101.06 examples/s]Generating validation split:   8%|▊         | 1102/13368 [00:08<00:53, 227.72 examples/s]Generating validation split:   8%|▊         | 1102/13368 [00:19<00:53, 227.72 examples/s]Generating validation split:   9%|▉         | 1221/13368 [00:25<05:16, 38.36 examples/s] Generating validation split:  13%|█▎        | 1775/13368 [00:25<02:30, 77.27 examples/s]Generating validation split:  17%|█▋        | 2294/13368 [00:25<01:26, 128.39 examples/s]Generating validation split:  21%|██▏       | 2868/13368 [00:25<00:50, 207.43 examples/s]Generating validation split:  25%|██▌       | 3399/13368 [00:25<00:32, 308.07 examples/s]Generating validation split:  30%|██▉       | 3960/13368 [00:25<00:20, 453.36 examples/s]Generating validation split:  36%|███▌      | 4756/13368 [00:26<00:11, 722.45 examples/s]Generating validation split:  41%|████▏     | 5532/13368 [00:26<00:07, 1048.83 examples/s]Generating validation split:  47%|████▋     | 6267/13368 [00:26<00:05, 1413.87 examples/s]Generating validation split:  51%|█████     | 6784/13368 [00:26<00:03, 1724.84 examples/s]Generating validation split:  56%|█████▋    | 7524/13368 [00:26<00:02, 2207.92 examples/s]Generating validation split:  62%|██████▏   | 8258/13368 [00:26<00:01, 2682.26 examples/s]Generating validation split:  66%|██████▌   | 8776/13368 [00:26<00:01, 3042.27 examples/s]Generating validation split:  71%|███████   | 9518/13368 [00:26<00:01, 3490.26 examples/s]Generating validation split:  77%|███████▋  | 10261/13368 [00:27<00:00, 3857.18 examples/s]Generating validation split:  81%|████████  | 10786/13368 [00:27<00:00, 4124.24 examples/s]Generating validation split:  86%|████████▋ | 11532/13368 [00:27<00:00, 4370.16 examples/s]Generating validation split:  92%|█████████▏| 12281/13368 [00:27<00:00, 4552.09 examples/s]Generating validation split:  96%|█████████▌| 12796/13368 [00:27<00:00, 4681.50 examples/s]Generating validation split: 100%|██████████| 13368/13368 [00:28<00:00, 1545.36 examples/s]                                                                                           Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]Generating test split:   0%|          | 1/11490 [00:08<25:38:56,  8.04s/ examples]Generating test split:   5%|▍         | 573/11490 [00:08<01:49, 100.02 examples/s]Generating test split:   7%|▋         | 857/11490 [00:21<01:46, 100.02 examples/s]Generating test split:  10%|▉         | 1094/11490 [00:26<04:13, 41.07 examples/s]Generating test split:  14%|█▍        | 1659/11490 [00:26<02:08, 76.54 examples/s]Generating test split:  19%|█▉        | 2176/11490 [00:26<01:16, 122.14 examples/s]Generating test split:  24%|██▍       | 2733/11490 [00:26<00:45, 190.94 examples/s]Generating test split:  29%|██▊       | 3283/11490 [00:26<00:28, 284.89 examples/s]Generating test split:  33%|███▎      | 3837/11490 [00:27<00:18, 414.57 examples/s]Generating test split:  40%|████      | 4626/11490 [00:27<00:10, 657.89 examples/s]Generating test split:  47%|████▋     | 5363/11490 [00:27<00:06, 942.62 examples/s]Generating test split:  51%|█████     | 5886/11490 [00:27<00:04, 1199.33 examples/s]Generating test split:  58%|█████▊    | 6616/11490 [00:27<00:03, 1616.12 examples/s]Generating test split:  64%|██████▍   | 7352/11490 [00:27<00:01, 2074.89 examples/s]Generating test split:  69%|██████▊   | 7871/11490 [00:27<00:01, 2439.42 examples/s]Generating test split:  75%|███████▍  | 8616/11490 [00:28<00:00, 2946.29 examples/s]Generating test split:  81%|████████▏ | 9350/11490 [00:28<00:00, 3378.93 examples/s]Generating test split:  86%|████████▌ | 9859/11490 [00:28<00:00, 3677.84 examples/s]Generating test split:  92%|█████████▏| 10571/11490 [00:28<00:00, 3965.45 examples/s]Generating test split:  98%|█████████▊| 11306/11490 [00:28<00:00, 4225.69 examples/s]                                                                                       0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  2.51it/s]100%|██████████| 3/3 [00:00<00:00,  6.94it/s]
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.58k/1.58k [00:00<00:00, 5.73MB/s]
loading configuration file config.json from cache at /home/sshukla7/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json
Model config BartConfig {
  "_name_or_path": "facebook/bart-large-cnn",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "force_bos_token_to_be_generated": true,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "length_penalty": 2.0,
  "max_length": 142,
  "max_position_embeddings": 1024,
  "min_length": 56,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "num_beams": 4,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "prefix": " ",
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "no_repeat_ngram_size": 3,
      "num_beams": 4
    }
  },
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50264
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/sshukla7/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json
Model config BartConfig {
  "_name_or_path": "facebook/bart-large-cnn",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "force_bos_token_to_be_generated": true,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "length_penalty": 2.0,
  "max_length": 142,
  "max_position_embeddings": 1024,
  "min_length": 56,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "num_beams": 4,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "prefix": " ",
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "no_repeat_ngram_size": 3,
      "num_beams": 4
    }
  },
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50264
}

Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 67.5MB/s]
Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 92.4MB/s]
Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 147MB/s]
loading file vocab.json from cache at /home/sshukla7/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/vocab.json
loading file merges.txt from cache at /home/sshukla7/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/merges.txt
loading file tokenizer.json from cache at /home/sshukla7/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/sshukla7/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json
Model config BartConfig {
  "_name_or_path": "facebook/bart-large-cnn",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "force_bos_token_to_be_generated": true,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "length_penalty": 2.0,
  "max_length": 142,
  "max_position_embeddings": 1024,
  "min_length": 56,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "num_beams": 4,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "prefix": " ",
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "no_repeat_ngram_size": 3,
      "num_beams": 4
    }
  },
  "transformers_version": "4.28.1",
  "use_cache": true,
  "vocab_size": 50264
}

Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]Downloading pytorch_model.bin:   3%|▎         | 41.9M/1.63G [00:00<00:03, 400MB/s]Downloading pytorch_model.bin:   6%|▋         | 105M/1.63G [00:00<00:03, 491MB/s] Downloading pytorch_model.bin:  10%|▉         | 157M/1.63G [00:00<00:03, 422MB/s]Downloading pytorch_model.bin:  13%|█▎        | 210M/1.63G [00:00<00:03, 445MB/s]Downloading pytorch_model.bin:  16%|█▌        | 262M/1.63G [00:00<00:02, 463MB/s]Downloading pytorch_model.bin:  19%|█▉        | 315M/1.63G [00:00<00:02, 460MB/s]Downloading pytorch_model.bin:  23%|██▎       | 367M/1.63G [00:00<00:02, 464MB/s]Downloading pytorch_model.bin:  26%|██▌       | 419M/1.63G [00:00<00:02, 472MB/s]Downloading pytorch_model.bin:  30%|██▉       | 482M/1.63G [00:01<00:02, 485MB/s]Downloading pytorch_model.bin:  33%|███▎      | 535M/1.63G [00:01<00:02, 454MB/s]Downloading pytorch_model.bin:  36%|███▌      | 587M/1.63G [00:01<00:02, 426MB/s]Downloading pytorch_model.bin:  39%|███▉      | 640M/1.63G [00:01<00:02, 433MB/s]Downloading pytorch_model.bin:  43%|████▎     | 692M/1.63G [00:01<00:02, 448MB/s]Downloading pytorch_model.bin:  46%|████▋     | 755M/1.63G [00:01<00:01, 473MB/s]Downloading pytorch_model.bin:  50%|████▉     | 807M/1.63G [00:01<00:01, 452MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 860M/1.63G [00:01<00:01, 412MB/s]Downloading pytorch_model.bin:  56%|█████▌    | 912M/1.63G [00:02<00:01, 405MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 965M/1.63G [00:02<00:01, 413MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 1.01G/1.63G [00:02<00:01, 406MB/s]Downloading pytorch_model.bin:  65%|██████▍   | 1.05G/1.63G [00:02<00:01, 323MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 1.09G/1.63G [00:02<00:01, 302MB/s]Downloading pytorch_model.bin:  70%|██████▉   | 1.13G/1.63G [00:02<00:01, 272MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 1.17G/1.63G [00:02<00:01, 301MB/s]Downloading pytorch_model.bin:  75%|███████▍  | 1.22G/1.63G [00:03<00:01, 299MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 1.26G/1.63G [00:03<00:01, 301MB/s]Downloading pytorch_model.bin:  79%|███████▉  | 1.29G/1.63G [00:03<00:01, 300MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 1.34G/1.63G [00:03<00:00, 338MB/s]Downloading pytorch_model.bin:  85%|████████▌ | 1.38G/1.63G [00:03<00:00, 336MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 1.43G/1.63G [00:03<00:00, 339MB/s]Downloading pytorch_model.bin:  90%|█████████ | 1.47G/1.63G [00:03<00:00, 348MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 1.51G/1.63G [00:03<00:00, 365MB/s]Downloading pytorch_model.bin:  96%|█████████▌| 1.56G/1.63G [00:04<00:00, 381MB/s]Downloading pytorch_model.bin:  99%|█████████▊| 1.60G/1.63G [00:04<00:00, 361MB/s]Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [00:04<00:00, 382MB/s]
loading weights file pytorch_model.bin from cache at /home/sshukla7/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "length_penalty": 2.0,
  "max_length": 142,
  "min_length": 56,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.28.1"
}

All model checkpoint weights were used when initializing BartForConditionalGeneration.

All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
Downloading (…)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 363/363 [00:00<00:00, 1.24MB/s]
loading configuration file generation_config.json from cache at /home/sshukla7/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/generation_config.json
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "length_penalty": 2.0,
  "max_length": 142,
  "min_length": 56,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.28.1"
}

/home/sshukla7/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:649: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'
  warnings.warn(
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.58k/1.58k [00:00<00:00, 9.36MB/s]
Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "length_penalty": 2.0,
  "max_length": 142,
  "min_length": 56,
  "no_repeat_ngram_size": 3,
  "num_beams": 4,
  "pad_token_id": 1,
  "transformers_version": "4.28.1"
}

Running tokenizer on dataset (num_proc=10):   0%|          | 0/287113 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   0%|          | 1000/287113 [00:01<09:05, 524.46 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   1%|          | 2000/287113 [00:02<04:03, 1170.03 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   2%|▏         | 6000/287113 [00:02<01:01, 4589.89 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   3%|▎         | 9000/287113 [00:02<00:41, 6756.35 examples/s]Running tokenizer on dataset (num_proc=10):   4%|▍         | 11000/287113 [00:03<01:21, 3396.63 examples/s]Running tokenizer on dataset (num_proc=10):   5%|▍         | 13000/287113 [00:03<01:05, 4191.85 examples/s]Running tokenizer on dataset (num_proc=10):   5%|▌         | 15000/287113 [00:03<00:51, 5323.58 examples/s]Running tokenizer on dataset (num_proc=10):   7%|▋         | 19000/287113 [00:04<00:32, 8195.78 examples/s]Running tokenizer on dataset (num_proc=10):   7%|▋         | 21000/287113 [00:05<01:02, 4263.09 examples/s]Running tokenizer on dataset (num_proc=10):   8%|▊         | 23000/287113 [00:05<00:51, 5104.86 examples/s]Running tokenizer on dataset (num_proc=10):   9%|▊         | 25000/287113 [00:05<00:44, 5904.09 examples/s]Running tokenizer on dataset (num_proc=10):   9%|▉         | 27000/287113 [00:05<00:36, 7117.97 examples/s]Running tokenizer on dataset (num_proc=10):  10%|█         | 29000/287113 [00:05<00:33, 7681.85 examples/s]Running tokenizer on dataset (num_proc=10):  11%|█         | 31000/287113 [00:06<00:59, 4336.80 examples/s]Running tokenizer on dataset (num_proc=10):  11%|█         | 32000/287113 [00:07<00:55, 4598.81 examples/s]Running tokenizer on dataset (num_proc=10):  12%|█▏        | 34000/287113 [00:07<00:50, 5024.22 examples/s]Running tokenizer on dataset (num_proc=10):  13%|█▎        | 36000/287113 [00:07<00:39, 6397.23 examples/s]Running tokenizer on dataset (num_proc=10):  14%|█▎        | 39000/287113 [00:07<00:31, 7853.16 examples/s]Running tokenizer on dataset (num_proc=10):  14%|█▍        | 41000/287113 [00:09<01:03, 3860.28 examples/s]Running tokenizer on dataset (num_proc=10):  15%|█▍        | 43000/287113 [00:09<00:49, 4918.28 examples/s]Running tokenizer on dataset (num_proc=10):  16%|█▌        | 45000/287113 [00:09<00:54, 4419.18 examples/s]Running tokenizer on dataset (num_proc=10):  17%|█▋        | 48000/287113 [00:09<00:38, 6163.05 examples/s]Running tokenizer on dataset (num_proc=10):  17%|█▋        | 50000/287113 [00:10<00:48, 4913.94 examples/s]Running tokenizer on dataset (num_proc=10):  18%|█▊        | 51000/287113 [00:10<00:47, 4944.66 examples/s]Running tokenizer on dataset (num_proc=10):  19%|█▉        | 54000/287113 [00:11<00:50, 4656.92 examples/s]Running tokenizer on dataset (num_proc=10):  19%|█▉        | 55000/287113 [00:11<00:47, 4875.66 examples/s]Running tokenizer on dataset (num_proc=10):  20%|█▉        | 57000/287113 [00:11<00:36, 6305.93 examples/s]Running tokenizer on dataset (num_proc=10):  20%|██        | 58000/287113 [00:11<00:34, 6606.71 examples/s]Running tokenizer on dataset (num_proc=10):  21%|██        | 60000/287113 [00:12<00:44, 5154.88 examples/s]Running tokenizer on dataset (num_proc=10):  22%|██▏       | 63000/287113 [00:12<00:29, 7630.74 examples/s]Running tokenizer on dataset (num_proc=10):  23%|██▎       | 65000/287113 [00:13<00:52, 4215.39 examples/s]Running tokenizer on dataset (num_proc=10):  24%|██▎       | 68000/287113 [00:13<00:37, 5796.22 examples/s]Running tokenizer on dataset (num_proc=10):  24%|██▍       | 70000/287113 [00:13<00:34, 6251.50 examples/s]Running tokenizer on dataset (num_proc=10):  25%|██▌       | 72000/287113 [00:14<00:31, 6734.39 examples/s]Running tokenizer on dataset (num_proc=10):  26%|██▌       | 74000/287113 [00:14<00:44, 4829.07 examples/s]Running tokenizer on dataset (num_proc=10):  26%|██▌       | 75000/287113 [00:15<00:51, 4152.82 examples/s]Running tokenizer on dataset (num_proc=10):  27%|██▋       | 78000/287113 [00:15<00:38, 5444.70 examples/s]Running tokenizer on dataset (num_proc=10):  29%|██▊       | 82000/287113 [00:15<00:28, 7276.70 examples/s]Running tokenizer on dataset (num_proc=10):  29%|██▉       | 83000/287113 [00:16<00:28, 7197.80 examples/s]Running tokenizer on dataset (num_proc=10):  29%|██▉       | 84000/287113 [00:16<00:42, 4769.47 examples/s]Running tokenizer on dataset (num_proc=10):  30%|██▉       | 85000/287113 [00:17<00:55, 3651.86 examples/s]Running tokenizer on dataset (num_proc=10):  31%|███       | 89000/287113 [00:17<00:33, 5952.27 examples/s]Running tokenizer on dataset (num_proc=10):  32%|███▏      | 92000/287113 [00:17<00:29, 6623.77 examples/s]Running tokenizer on dataset (num_proc=10):  33%|███▎      | 94000/287113 [00:18<00:35, 5470.49 examples/s]Running tokenizer on dataset (num_proc=10):  33%|███▎      | 95000/287113 [00:18<00:42, 4517.65 examples/s]Running tokenizer on dataset (num_proc=10):  33%|███▎      | 96000/287113 [00:19<00:41, 4621.61 examples/s]Running tokenizer on dataset (num_proc=10):  34%|███▍      | 98000/287113 [00:19<00:31, 6057.68 examples/s]Running tokenizer on dataset (num_proc=10):  35%|███▍      | 100000/287113 [00:19<00:28, 6551.15 examples/s]Running tokenizer on dataset (num_proc=10):  36%|███▌      | 102000/287113 [00:19<00:23, 7763.22 examples/s]Running tokenizer on dataset (num_proc=10):  36%|███▌      | 104000/287113 [00:20<00:29, 6123.05 examples/s]Running tokenizer on dataset (num_proc=10):  37%|███▋      | 105000/287113 [00:20<00:36, 4978.96 examples/s]Running tokenizer on dataset (num_proc=10):  37%|███▋      | 106000/287113 [00:20<00:44, 4066.73 examples/s]Running tokenizer on dataset (num_proc=10):  38%|███▊      | 108000/287113 [00:20<00:32, 5508.52 examples/s]Running tokenizer on dataset (num_proc=10):  38%|███▊      | 110000/287113 [00:21<00:30, 5849.92 examples/s]Running tokenizer on dataset (num_proc=10):  39%|███▉      | 113000/287113 [00:21<00:21, 8281.80 examples/s]Running tokenizer on dataset (num_proc=10):  40%|████      | 115000/287113 [00:21<00:26, 6434.83 examples/s]Running tokenizer on dataset (num_proc=10):  40%|████      | 116000/287113 [00:22<00:42, 3986.30 examples/s]Running tokenizer on dataset (num_proc=10):  41%|████      | 118000/287113 [00:22<00:31, 5333.20 examples/s]Running tokenizer on dataset (num_proc=10):  41%|████▏     | 119000/287113 [00:22<00:30, 5533.57 examples/s]Running tokenizer on dataset (num_proc=10):  42%|████▏     | 120000/287113 [00:23<00:29, 5659.06 examples/s]Running tokenizer on dataset (num_proc=10):  42%|████▏     | 122000/287113 [00:23<00:22, 7308.12 examples/s]Running tokenizer on dataset (num_proc=10):  43%|████▎     | 124000/287113 [00:23<00:18, 8991.79 examples/s]Running tokenizer on dataset (num_proc=10):  44%|████▍     | 126000/287113 [00:24<00:40, 3952.49 examples/s]Running tokenizer on dataset (num_proc=10):  44%|████▍     | 127000/287113 [00:24<00:36, 4342.16 examples/s]Running tokenizer on dataset (num_proc=10):  45%|████▍     | 129000/287113 [00:24<00:28, 5505.03 examples/s]Running tokenizer on dataset (num_proc=10):  46%|████▌     | 132000/287113 [00:24<00:21, 7186.70 examples/s]Running tokenizer on dataset (num_proc=10):  46%|████▋     | 133000/287113 [00:25<00:20, 7360.16 examples/s]Running tokenizer on dataset (num_proc=10):  47%|████▋     | 135000/287113 [00:25<00:16, 9227.76 examples/s]Running tokenizer on dataset (num_proc=10):  48%|████▊     | 137000/287113 [00:26<00:37, 4038.57 examples/s]Running tokenizer on dataset (num_proc=10):  48%|████▊     | 139000/287113 [00:26<00:27, 5336.25 examples/s]Running tokenizer on dataset (num_proc=10):  49%|████▉     | 141000/287113 [00:26<00:24, 6035.11 examples/s]Running tokenizer on dataset (num_proc=10):  50%|████▉     | 143000/287113 [00:26<00:20, 7130.87 examples/s]Running tokenizer on dataset (num_proc=10):  51%|█████     | 145000/287113 [00:27<00:21, 6688.78 examples/s]Running tokenizer on dataset (num_proc=10):  51%|█████     | 146000/287113 [00:27<00:32, 4324.96 examples/s]Running tokenizer on dataset (num_proc=10):  51%|█████     | 147000/287113 [00:27<00:32, 4269.45 examples/s]Running tokenizer on dataset (num_proc=10):  52%|█████▏    | 149000/287113 [00:28<00:23, 5825.17 examples/s]Running tokenizer on dataset (num_proc=10):  53%|█████▎    | 151000/287113 [00:28<00:19, 6813.78 examples/s]Running tokenizer on dataset (num_proc=10):  53%|█████▎    | 152000/287113 [00:28<00:19, 6814.61 examples/s]Running tokenizer on dataset (num_proc=10):  53%|█████▎    | 153000/287113 [00:28<00:23, 5603.95 examples/s]Running tokenizer on dataset (num_proc=10):  54%|█████▎    | 154000/287113 [00:28<00:22, 5793.71 examples/s]Running tokenizer on dataset (num_proc=10):  54%|█████▍    | 155000/287113 [00:29<00:21, 6218.30 examples/s]Running tokenizer on dataset (num_proc=10):  54%|█████▍    | 156000/287113 [00:29<00:22, 5877.08 examples/s]Running tokenizer on dataset (num_proc=10):  55%|█████▍    | 157000/287113 [00:29<00:28, 4486.70 examples/s]Running tokenizer on dataset (num_proc=10):  55%|█████▌    | 158000/287113 [00:29<00:27, 4636.14 examples/s]Running tokenizer on dataset (num_proc=10):  55%|█████▌    | 159000/287113 [00:29<00:23, 5354.87 examples/s]Running tokenizer on dataset (num_proc=10):  56%|█████▌    | 161000/287113 [00:30<00:16, 7718.89 examples/s]Running tokenizer on dataset (num_proc=10):  56%|█████▋    | 162000/287113 [00:30<00:18, 6645.76 examples/s]Running tokenizer on dataset (num_proc=10):  57%|█████▋    | 163000/287113 [00:30<00:26, 4698.62 examples/s]Running tokenizer on dataset (num_proc=10):  57%|█████▋    | 164000/287113 [00:30<00:23, 5261.49 examples/s]Running tokenizer on dataset (num_proc=10):  58%|█████▊    | 166000/287113 [00:30<00:17, 7082.60 examples/s]Running tokenizer on dataset (num_proc=10):  58%|█████▊    | 167000/287113 [00:31<00:22, 5368.34 examples/s]Running tokenizer on dataset (num_proc=10):  59%|█████▊    | 168000/287113 [00:31<00:26, 4563.44 examples/s]Running tokenizer on dataset (num_proc=10):  59%|█████▉    | 169000/287113 [00:31<00:22, 5240.23 examples/s]Running tokenizer on dataset (num_proc=10):  60%|█████▉    | 172000/287113 [00:32<00:18, 6262.29 examples/s]Running tokenizer on dataset (num_proc=10):  60%|██████    | 173000/287113 [00:32<00:23, 4793.73 examples/s]Running tokenizer on dataset (num_proc=10):  61%|██████    | 175000/287113 [00:32<00:18, 5942.81 examples/s]Running tokenizer on dataset (num_proc=10):  61%|██████▏   | 176000/287113 [00:32<00:17, 6428.42 examples/s]Running tokenizer on dataset (num_proc=10):  62%|██████▏   | 178000/287113 [00:33<00:22, 4882.63 examples/s]Running tokenizer on dataset (num_proc=10):  63%|██████▎   | 181000/287113 [00:33<00:15, 6720.40 examples/s]Running tokenizer on dataset (num_proc=10):  63%|██████▎   | 182000/287113 [00:33<00:18, 5557.69 examples/s]Running tokenizer on dataset (num_proc=10):  64%|██████▎   | 183000/287113 [00:34<00:20, 5178.30 examples/s]Running tokenizer on dataset (num_proc=10):  64%|██████▍   | 184000/287113 [00:34<00:17, 5749.19 examples/s]Running tokenizer on dataset (num_proc=10):  64%|██████▍   | 185000/287113 [00:34<00:16, 6073.59 examples/s]Running tokenizer on dataset (num_proc=10):  65%|██████▍   | 186000/287113 [00:34<00:17, 5646.15 examples/s]Running tokenizer on dataset (num_proc=10):  65%|██████▌   | 188000/287113 [00:35<00:19, 5173.56 examples/s]Running tokenizer on dataset (num_proc=10):  66%|██████▌   | 189000/287113 [00:35<00:17, 5676.60 examples/s]Running tokenizer on dataset (num_proc=10):  67%|██████▋   | 191000/287113 [00:35<00:15, 6153.03 examples/s]Running tokenizer on dataset (num_proc=10):  67%|██████▋   | 192000/287113 [00:35<00:18, 5037.97 examples/s]Running tokenizer on dataset (num_proc=10):  68%|██████▊   | 195000/287113 [00:36<00:17, 5333.70 examples/s]Running tokenizer on dataset (num_proc=10):  68%|██████▊   | 196000/287113 [00:36<00:17, 5100.34 examples/s]Running tokenizer on dataset (num_proc=10):  69%|██████▉   | 198000/287113 [00:36<00:13, 6508.27 examples/s]Running tokenizer on dataset (num_proc=10):  69%|██████▉   | 199000/287113 [00:36<00:12, 6833.62 examples/s]Running tokenizer on dataset (num_proc=10):  70%|██████▉   | 200000/287113 [00:37<00:15, 5662.04 examples/s]Running tokenizer on dataset (num_proc=10):  70%|███████   | 201000/287113 [00:37<00:14, 5952.18 examples/s]Running tokenizer on dataset (num_proc=10):  70%|███████   | 202000/287113 [00:37<00:14, 5701.38 examples/s]Running tokenizer on dataset (num_proc=10):  71%|███████   | 204000/287113 [00:37<00:10, 8070.00 examples/s]Running tokenizer on dataset (num_proc=10):  71%|███████▏  | 205000/287113 [00:38<00:20, 3972.41 examples/s]Running tokenizer on dataset (num_proc=10):  72%|███████▏  | 206000/287113 [00:38<00:19, 4140.21 examples/s]Running tokenizer on dataset (num_proc=10):  72%|███████▏  | 207000/287113 [00:38<00:16, 4852.49 examples/s]Running tokenizer on dataset (num_proc=10):  73%|███████▎  | 210000/287113 [00:38<00:13, 5770.81 examples/s]Running tokenizer on dataset (num_proc=10):  74%|███████▍  | 212000/287113 [00:39<00:11, 6347.94 examples/s]Running tokenizer on dataset (num_proc=10):  75%|███████▍  | 215000/287113 [00:40<00:14, 4859.95 examples/s]Running tokenizer on dataset (num_proc=10):  75%|███████▌  | 216000/287113 [00:40<00:13, 5306.20 examples/s]Running tokenizer on dataset (num_proc=10):  76%|███████▌  | 217000/287113 [00:40<00:12, 5739.85 examples/s]Running tokenizer on dataset (num_proc=10):  76%|███████▌  | 218000/287113 [00:40<00:11, 5803.04 examples/s]Running tokenizer on dataset (num_proc=10):  77%|███████▋  | 220000/287113 [00:40<00:12, 5459.91 examples/s]Running tokenizer on dataset (num_proc=10):  78%|███████▊  | 223000/287113 [00:40<00:08, 7947.91 examples/s]Running tokenizer on dataset (num_proc=10):  78%|███████▊  | 225000/287113 [00:41<00:14, 4308.52 examples/s]Running tokenizer on dataset (num_proc=10):  79%|███████▊  | 226000/287113 [00:42<00:14, 4364.24 examples/s]Running tokenizer on dataset (num_proc=10):  79%|███████▉  | 228000/287113 [00:42<00:10, 5841.99 examples/s]Running tokenizer on dataset (num_proc=10):  80%|████████  | 230000/287113 [00:42<00:08, 6622.87 examples/s]Running tokenizer on dataset (num_proc=10):  80%|████████  | 231000/287113 [00:42<00:08, 6551.72 examples/s]Running tokenizer on dataset (num_proc=10):  82%|████████▏ | 234000/287113 [00:43<00:07, 7037.40 examples/s]Running tokenizer on dataset (num_proc=10):  82%|████████▏ | 235000/287113 [00:43<00:12, 4210.67 examples/s]Running tokenizer on dataset (num_proc=10):  82%|████████▏ | 236000/287113 [00:43<00:11, 4626.50 examples/s]Running tokenizer on dataset (num_proc=10):  83%|████████▎ | 238000/287113 [00:44<00:09, 5385.18 examples/s]Running tokenizer on dataset (num_proc=10):  84%|████████▎ | 240000/287113 [00:44<00:08, 5824.47 examples/s]Running tokenizer on dataset (num_proc=10):  84%|████████▍ | 242000/287113 [00:44<00:06, 6687.64 examples/s]Running tokenizer on dataset (num_proc=10):  85%|████████▍ | 243000/287113 [00:44<00:06, 6522.85 examples/s]Running tokenizer on dataset (num_proc=10):  85%|████████▍ | 244000/287113 [00:44<00:06, 6324.06 examples/s]Running tokenizer on dataset (num_proc=10):  85%|████████▌ | 245000/287113 [00:45<00:08, 4709.97 examples/s]Running tokenizer on dataset (num_proc=10):  86%|████████▌ | 246000/287113 [00:45<00:09, 4422.11 examples/s]Running tokenizer on dataset (num_proc=10):  86%|████████▋ | 248000/287113 [00:45<00:07, 5220.88 examples/s]Running tokenizer on dataset (num_proc=10):  87%|████████▋ | 249000/287113 [00:46<00:08, 4607.26 examples/s]Running tokenizer on dataset (num_proc=10):  87%|████████▋ | 251000/287113 [00:46<00:06, 6007.65 examples/s]Running tokenizer on dataset (num_proc=10):  88%|████████▊ | 252000/287113 [00:46<00:06, 5585.75 examples/s]Running tokenizer on dataset (num_proc=10):  88%|████████▊ | 253712/287113 [00:46<00:05, 5674.95 examples/s]Running tokenizer on dataset (num_proc=10):  89%|████████▉ | 255712/287113 [00:47<00:04, 6985.92 examples/s]Running tokenizer on dataset (num_proc=10):  89%|████████▉ | 256712/287113 [00:47<00:05, 5452.24 examples/s]Running tokenizer on dataset (num_proc=10):  90%|████████▉ | 257712/287113 [00:47<00:07, 4155.47 examples/s]Running tokenizer on dataset (num_proc=10):  90%|█████████ | 258712/287113 [00:48<00:06, 4398.87 examples/s]Running tokenizer on dataset (num_proc=10):  91%|█████████ | 260712/287113 [00:48<00:04, 6106.88 examples/s]Running tokenizer on dataset (num_proc=10):  91%|█████████ | 261712/287113 [00:48<00:05, 4850.04 examples/s]Running tokenizer on dataset (num_proc=10):  92%|█████████▏| 262712/287113 [00:48<00:04, 5550.22 examples/s]Running tokenizer on dataset (num_proc=10):  92%|█████████▏| 263712/287113 [00:48<00:04, 5512.68 examples/s]Running tokenizer on dataset (num_proc=10):  93%|█████████▎| 265712/287113 [00:49<00:03, 5836.68 examples/s]Running tokenizer on dataset (num_proc=10):  93%|█████████▎| 266712/287113 [00:49<00:05, 3505.70 examples/s]Running tokenizer on dataset (num_proc=10):  94%|█████████▍| 269712/287113 [00:49<00:03, 5702.87 examples/s]Running tokenizer on dataset (num_proc=10):  94%|█████████▍| 271134/287113 [00:50<00:02, 6633.22 examples/s]Running tokenizer on dataset (num_proc=10):  95%|█████████▌| 272846/287113 [00:50<00:02, 6491.17 examples/s]Running tokenizer on dataset (num_proc=10):  95%|█████████▌| 273846/287113 [00:50<00:02, 6268.68 examples/s]Running tokenizer on dataset (num_proc=10):  96%|█████████▌| 274846/287113 [00:51<00:04, 2925.41 examples/s]Running tokenizer on dataset (num_proc=10):  96%|█████████▌| 275846/287113 [00:51<00:03, 3369.09 examples/s]Running tokenizer on dataset (num_proc=10):  97%|█████████▋| 277846/287113 [00:51<00:01, 4833.45 examples/s]Running tokenizer on dataset (num_proc=10):  97%|█████████▋| 278846/287113 [00:52<00:01, 4182.26 examples/s]Running tokenizer on dataset (num_proc=10):  97%|█████████▋| 279846/287113 [00:52<00:01, 3854.52 examples/s]Running tokenizer on dataset (num_proc=10):  98%|█████████▊| 280557/287113 [00:52<00:01, 3596.32 examples/s]Running tokenizer on dataset (num_proc=10):  98%|█████████▊| 281268/287113 [00:53<00:01, 2941.49 examples/s]Running tokenizer on dataset (num_proc=10):  98%|█████████▊| 281980/287113 [00:53<00:01, 2915.48 examples/s]Running tokenizer on dataset (num_proc=10):  99%|█████████▊| 282980/287113 [00:53<00:01, 3280.43 examples/s]Running tokenizer on dataset (num_proc=10):  99%|█████████▉| 284980/287113 [00:54<00:00, 3024.70 examples/s]Running tokenizer on dataset (num_proc=10): 100%|█████████▉| 285691/287113 [00:54<00:00, 2333.04 examples/s]Running tokenizer on dataset (num_proc=10): 100%|█████████▉| 286402/287113 [00:55<00:00, 2688.21 examples/s]Running tokenizer on dataset (num_proc=10): 100%|██████████| 287113/287113 [00:55<00:00, 2093.99 examples/s]                                                                                                            Running tokenizer on dataset (num_proc=10):   0%|          | 0/13368 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   7%|▋         | 1000/13368 [00:01<00:19, 648.22 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  22%|██▏       | 3000/13368 [00:01<00:04, 2183.55 examples/s]Running tokenizer on dataset (num_proc=10):  30%|██▉       | 4000/13368 [00:01<00:03, 2740.38 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  35%|███▍      | 4674/13368 [00:02<00:03, 2872.40 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  42%|████▏     | 5674/13368 [00:02<00:02, 3654.94 examples/s]Running tokenizer on dataset (num_proc=10):  75%|███████▍  | 10011/13368 [00:02<00:00, 9514.85 examples/s]Running tokenizer on dataset (num_proc=10):  90%|████████▉ | 12022/13368 [00:02<00:00, 6197.80 examples/s]Running tokenizer on dataset (num_proc=10): 100%|██████████| 13368/13368 [00:03<00:00, 6657.20 examples/s]                                                                                                          Running tokenizer on dataset (num_proc=10):   0%|          | 0/11490 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   9%|▊         | 1000/11490 [00:01<00:16, 623.90 examples/s]Running tokenizer on dataset (num_proc=10):  26%|██▌       | 3000/11490 [00:01<00:04, 2074.83 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  39%|███▊      | 4447/11490 [00:02<00:02, 2791.13 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  49%|████▊     | 5596/11490 [00:02<00:01, 3449.98 examples/s]Running tokenizer on dataset (num_proc=10):  57%|█████▋    | 6596/11490 [00:02<00:01, 3931.57 examples/s]Running tokenizer on dataset (num_proc=10):  94%|█████████▎| 10745/11490 [00:02<00:00, 8819.31 examples/s]                                                                                                          /home/sshukla7/.local/lib/python3.10/site-packages/accelerate/accelerator.py:498: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnEng_BL.py:674: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("rouge")
  0%|          | 0/179460 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/179460 [00:05<290:15:28,  5.82s/it]  0%|          | 2/179460 [00:09<214:49:21,  4.31s/it]  0%|          | 3/179460 [00:12<190:23:40,  3.82s/it]  0%|          | 4/179460 [00:15<179:53:35,  3.61s/it]  0%|          | 5/179460 [00:18<172:07:30,  3.45s/it]  0%|          | 6/179460 [00:22<168:28:13,  3.38s/it]  0%|          | 7/179460 [00:25<166:11:28,  3.33s/it]  0%|          | 8/179460 [00:28<164:36:57,  3.30s/it]  0%|          | 9/179460 [00:31<163:47:38,  3.29s/it]  0%|          | 10/179460 [00:34<163:18:37,  3.28s/it]  0%|          | 11/179460 [00:38<163:34:59,  3.28s/it]  0%|          | 12/179460 [00:41<162:33:00,  3.26s/it]  0%|          | 13/179460 [00:44<161:40:18,  3.24s/it]  0%|          | 14/179460 [00:47<161:07:54,  3.23s/it]  0%|          | 15/179460 [00:51<159:46:07,  3.21s/it]  0%|          | 16/179460 [00:54<159:22:00,  3.20s/it]  0%|          | 17/179460 [00:57<159:43:57,  3.20s/it]  0%|          | 18/179460 [01:00<159:47:21,  3.21s/it]  0%|          | 19/179460 [01:03<159:26:51,  3.20s/it]  0%|          | 20/179460 [01:07<159:37:37,  3.20s/it]  0%|          | 21/179460 [01:10<159:56:34,  3.21s/it]  0%|          | 22/179460 [01:13<158:57:06,  3.19s/it]  0%|          | 23/179460 [01:16<158:13:29,  3.17s/it]  0%|          | 24/179460 [01:19<158:31:02,  3.18s/it]  0%|          | 25/179460 [01:22<158:08:45,  3.17s/it]  0%|          | 26/179460 [01:26<158:49:36,  3.19s/it]  0%|          | 27/179460 [01:29<159:18:52,  3.20s/it]  0%|          | 28/179460 [01:32<158:38:32,  3.18s/it]  0%|          | 29/179460 [01:35<159:12:46,  3.19s/it]  0%|          | 30/179460 [01:38<158:19:12,  3.18s/it]  0%|          | 31/179460 [01:42<158:49:38,  3.19s/it]  0%|          | 32/179460 [01:45<158:30:00,  3.18s/it]  0%|          | 33/179460 [01:48<158:16:54,  3.18s/it]  0%|          | 34/179460 [01:51<158:36:50,  3.18s/it]  0%|          | 35/179460 [01:54<158:21:20,  3.18s/it]  0%|          | 36/179460 [01:57<156:46:51,  3.15s/it]  0%|          | 37/179460 [02:01<157:18:40,  3.16s/it]  0%|          | 38/179460 [02:04<156:56:41,  3.15s/it]  0%|          | 39/179460 [02:07<157:47:59,  3.17s/it]  0%|          | 40/179460 [02:10<157:55:38,  3.17s/it]  0%|          | 41/179460 [02:13<157:14:42,  3.16s/it]  0%|          | 42/179460 [02:16<157:38:42,  3.16s/it]  0%|          | 43/179460 [02:20<158:08:20,  3.17s/it]  0%|          | 44/179460 [02:23<158:00:47,  3.17s/it]  0%|          | 45/179460 [02:26<157:40:28,  3.16s/it]  0%|          | 46/179460 [02:29<158:46:28,  3.19s/it]  0%|          | 47/179460 [02:32<158:31:00,  3.18s/it]  0%|          | 48/179460 [02:35<158:59:05,  3.19s/it]  0%|          | 49/179460 [02:39<159:08:25,  3.19s/it]  0%|          | 50/179460 [02:42<158:40:57,  3.18s/it]  0%|          | 51/179460 [02:45<158:47:38,  3.19s/it]  0%|          | 52/179460 [02:48<158:06:26,  3.17s/it]  0%|          | 53/179460 [02:51<157:45:18,  3.17s/it]  0%|          | 54/179460 [02:55<158:23:41,  3.18s/it]  0%|          | 55/179460 [02:58<158:12:30,  3.17s/it]  0%|          | 56/179460 [03:01<158:41:28,  3.18s/it]  0%|          | 57/179460 [03:04<159:00:06,  3.19s/it]  0%|          | 58/179460 [03:07<159:21:57,  3.20s/it]  0%|          | 59/179460 [03:10<158:54:57,  3.19s/it]  0%|          | 60/179460 [03:14<158:46:11,  3.19s/it]  0%|          | 61/179460 [03:17<158:02:34,  3.17s/it]  0%|          | 62/179460 [03:20<156:55:12,  3.15s/it]  0%|          | 63/179460 [03:23<155:57:33,  3.13s/it]  0%|          | 64/179460 [03:26<157:11:09,  3.15s/it]  0%|          | 65/179460 [03:29<157:59:00,  3.17s/it]  0%|          | 66/179460 [03:33<158:18:39,  3.18s/it]  0%|          | 67/179460 [03:36<158:46:18,  3.19s/it]  0%|          | 68/179460 [03:39<158:28:51,  3.18s/it]  0%|          | 69/179460 [03:42<158:59:22,  3.19s/it]  0%|          | 70/179460 [03:45<159:20:22,  3.20s/it]  0%|          | 71/179460 [03:49<158:30:56,  3.18s/it]  0%|          | 72/179460 [03:52<158:21:11,  3.18s/it]  0%|          | 73/179460 [03:55<158:12:12,  3.17s/it]  0%|          | 74/179460 [03:58<157:58:23,  3.17s/it]  0%|          | 75/179460 [04:01<157:46:13,  3.17s/it]  0%|          | 76/179460 [04:04<158:42:59,  3.19s/it]  0%|          | 77/179460 [04:08<158:35:44,  3.18s/it]  0%|          | 78/179460 [04:11<158:04:56,  3.17s/it]  0%|          | 79/179460 [04:14<158:31:02,  3.18s/it]  0%|          | 80/179460 [04:17<157:37:48,  3.16s/it]  0%|          | 81/179460 [04:20<157:12:50,  3.16s/it]  0%|          | 82/179460 [04:23<157:36:00,  3.16s/it]  0%|          | 83/179460 [04:26<156:45:55,  3.15s/it]  0%|          | 84/179460 [04:30<156:40:20,  3.14s/it]  0%|          | 85/179460 [04:33<155:47:04,  3.13s/it]  0%|          | 86/179460 [04:36<156:31:21,  3.14s/it]  0%|          | 87/179460 [04:39<156:10:12,  3.13s/it]  0%|          | 88/179460 [04:42<156:06:16,  3.13s/it]  0%|          | 89/179460 [04:45<156:57:41,  3.15s/it]  0%|          | 90/179460 [04:49<158:00:51,  3.17s/it]  0%|          | 91/179460 [04:52<158:11:39,  3.18s/it]  0%|          | 92/179460 [04:55<158:15:21,  3.18s/it]  0%|          | 93/179460 [04:58<158:34:25,  3.18s/it]  0%|          | 94/179460 [05:01<158:31:52,  3.18s/it]  0%|          | 95/179460 [05:04<158:29:15,  3.18s/it]  0%|          | 96/179460 [05:08<158:06:46,  3.17s/it]  0%|          | 97/179460 [05:11<158:02:55,  3.17s/it]  0%|          | 98/179460 [05:14<158:12:41,  3.18s/it]  0%|          | 99/179460 [05:17<157:08:05,  3.15s/it]  0%|          | 100/179460 [05:20<157:06:03,  3.15s/it]  0%|          | 101/179460 [05:23<157:47:31,  3.17s/it]  0%|          | 102/179460 [05:27<157:14:47,  3.16s/it]  0%|          | 103/179460 [05:30<157:09:01,  3.15s/it]  0%|          | 104/179460 [05:33<157:39:49,  3.16s/it]  0%|          | 105/179460 [05:36<157:50:12,  3.17s/it]  0%|          | 106/179460 [05:39<158:38:22,  3.18s/it]  0%|          | 107/179460 [05:42<158:10:02,  3.17s/it]  0%|          | 108/179460 [05:46<158:29:49,  3.18s/it]  0%|          | 109/179460 [05:49<159:00:31,  3.19s/it]  0%|          | 110/179460 [05:52<158:37:10,  3.18s/it]  0%|          | 111/179460 [05:55<158:56:51,  3.19s/it]  0%|          | 112/179460 [05:58<159:09:15,  3.19s/it]  0%|          | 113/179460 [06:02<158:26:56,  3.18s/it]  0%|          | 114/179460 [06:05<157:49:49,  3.17s/it]  0%|          | 115/179460 [06:08<158:16:27,  3.18s/it]  0%|          | 116/179460 [06:11<158:26:17,  3.18s/it]  0%|          | 117/179460 [06:14<158:13:47,  3.18s/it]  0%|          | 118/179460 [06:17<158:45:11,  3.19s/it]  0%|          | 119/179460 [06:21<158:24:44,  3.18s/it]  0%|          | 120/179460 [06:24<158:16:18,  3.18s/it]  0%|          | 121/179460 [06:27<158:35:42,  3.18s/it]  0%|          | 122/179460 [06:30<158:55:06,  3.19s/it]  0%|          | 123/179460 [06:33<158:14:32,  3.18s/it]  0%|          | 124/179460 [06:36<157:05:19,  3.15s/it]  0%|          | 125/179460 [06:40<157:02:52,  3.15s/it]  0%|          | 126/179460 [06:43<157:32:18,  3.16s/it]  0%|          | 127/179460 [06:46<158:07:14,  3.17s/it]  0%|          | 128/179460 [06:49<158:08:27,  3.17s/it]  0%|          | 129/179460 [06:52<157:09:36,  3.15s/it]  0%|          | 130/179460 [06:55<157:02:57,  3.15s/it]  0%|          | 131/179460 [06:59<157:09:00,  3.15s/it]  0%|          | 132/179460 [07:02<157:35:21,  3.16s/it]  0%|          | 133/179460 [07:05<158:08:55,  3.17s/it]  0%|          | 134/179460 [07:08<158:22:18,  3.18s/it]  0%|          | 135/179460 [07:11<158:01:46,  3.17s/it]  0%|          | 136/179460 [07:15<158:21:29,  3.18s/it]  0%|          | 137/179460 [07:18<157:41:23,  3.17s/it]  0%|          | 138/179460 [07:21<158:03:12,  3.17s/it]  0%|          | 139/179460 [07:24<158:03:07,  3.17s/it]  0%|          | 140/179460 [07:27<157:32:08,  3.16s/it]  0%|          | 141/179460 [07:30<157:33:29,  3.16s/it]  0%|          | 142/179460 [07:33<156:54:24,  3.15s/it]  0%|          | 143/179460 [07:37<156:42:36,  3.15s/it]  0%|          | 144/179460 [07:40<156:02:14,  3.13s/it]  0%|          | 145/179460 [07:43<156:03:18,  3.13s/it]  0%|          | 146/179460 [07:46<156:18:04,  3.14s/it]  0%|          | 147/179460 [07:49<156:19:57,  3.14s/it]  0%|          | 148/179460 [07:52<155:55:17,  3.13s/it]  0%|          | 149/179460 [07:55<156:29:10,  3.14s/it]  0%|          | 150/179460 [07:59<156:15:46,  3.14s/it]  0%|          | 151/179460 [08:02<156:15:26,  3.14s/it]  0%|          | 152/179460 [08:05<157:10:53,  3.16s/it]  0%|          | 153/179460 [08:08<156:45:31,  3.15s/it]  0%|          | 154/179460 [08:11<156:43:50,  3.15s/it]  0%|          | 155/179460 [08:14<156:40:14,  3.15s/it]  0%|          | 156/179460 [08:17<157:07:14,  3.15s/it]  0%|          | 157/179460 [08:21<156:54:29,  3.15s/it]  0%|          | 158/179460 [08:24<157:53:39,  3.17s/it]  0%|          | 159/179460 [08:27<157:49:42,  3.17s/it]  0%|          | 160/179460 [08:30<156:47:54,  3.15s/it]  0%|          | 161/179460 [08:33<157:18:20,  3.16s/it]  0%|          | 162/179460 [08:36<157:53:43,  3.17s/it]  0%|          | 163/179460 [08:40<156:36:42,  3.14s/it]  0%|          | 164/179460 [08:43<157:21:17,  3.16s/it]  0%|          | 165/179460 [08:46<157:51:26,  3.17s/it]  0%|          | 166/179460 [08:49<158:10:48,  3.18s/it]  0%|          | 167/179460 [08:52<158:07:37,  3.18s/it]  0%|          | 168/179460 [08:55<157:16:46,  3.16s/it]  0%|          | 169/179460 [08:59<157:02:22,  3.15s/it]  0%|          | 170/179460 [09:02<157:26:55,  3.16s/it]  0%|          | 171/179460 [09:05<157:56:57,  3.17s/it]  0%|          | 172/179460 [09:08<157:11:26,  3.16s/it]  0%|          | 173/179460 [09:11<157:48:31,  3.17s/it]  0%|          | 174/179460 [09:14<157:42:44,  3.17s/it]  0%|          | 175/179460 [09:18<157:40:16,  3.17s/it]  0%|          | 176/179460 [09:21<158:04:46,  3.17s/it]  0%|          | 177/179460 [09:24<158:20:54,  3.18s/it]  0%|          | 178/179460 [09:27<158:33:18,  3.18s/it]  0%|          | 179/179460 [09:30<158:36:37,  3.18s/it]  0%|          | 180/179460 [09:33<157:13:45,  3.16s/it]  0%|          | 181/179460 [09:37<157:20:47,  3.16s/it]  0%|          | 182/179460 [09:40<157:21:56,  3.16s/it]  0%|          | 183/179460 [09:43<157:50:24,  3.17s/it]  0%|          | 184/179460 [09:46<158:14:58,  3.18s/it]  0%|          | 185/179460 [09:49<158:15:49,  3.18s/it]  0%|          | 186/179460 [09:52<157:36:27,  3.16s/it]  0%|          | 187/179460 [09:56<156:22:15,  3.14s/it]  0%|          | 188/179460 [09:59<156:21:57,  3.14s/it]  0%|          | 189/179460 [10:02<156:29:27,  3.14s/it]  0%|          | 190/179460 [10:05<156:37:50,  3.15s/it]  0%|          | 191/179460 [10:08<156:49:38,  3.15s/it]  0%|          | 192/179460 [10:11<157:04:33,  3.15s/it]  0%|          | 193/179460 [10:15<161:12:38,  3.24s/it]  0%|          | 194/179460 [10:18<159:31:01,  3.20s/it]  0%|          | 195/179460 [10:21<158:44:17,  3.19s/it]  0%|          | 196/179460 [10:24<158:46:39,  3.19s/it]  0%|          | 197/179460 [10:27<158:53:20,  3.19s/it]  0%|          | 198/179460 [10:31<158:39:32,  3.19s/it]  0%|          | 199/179460 [10:34<158:03:38,  3.17s/it]  0%|          | 200/179460 [10:37<157:39:12,  3.17s/it]  0%|          | 201/179460 [10:40<158:07:18,  3.18s/it]  0%|          | 202/179460 [10:43<157:19:59,  3.16s/it]  0%|          | 203/179460 [10:46<156:50:14,  3.15s/it]  0%|          | 204/179460 [10:49<156:50:20,  3.15s/it]  0%|          | 205/179460 [10:53<157:30:48,  3.16s/it]  0%|          | 206/179460 [10:56<157:12:53,  3.16s/it]  0%|          | 207/179460 [10:59<157:06:13,  3.16s/it]  0%|          | 208/179460 [11:02<156:55:54,  3.15s/it]  0%|          | 209/179460 [11:05<157:38:01,  3.17s/it]  0%|          | 210/179460 [11:08<157:43:29,  3.17s/it]  0%|          | 211/179460 [11:12<158:08:32,  3.18s/it]  0%|          | 212/179460 [11:15<157:38:29,  3.17s/it]  0%|          | 213/179460 [11:18<157:09:11,  3.16s/it]  0%|          | 214/179460 [11:21<157:47:10,  3.17s/it]  0%|          | 215/179460 [11:24<157:28:35,  3.16s/it]  0%|          | 216/179460 [11:27<157:26:40,  3.16s/it]  0%|          | 217/179460 [11:31<156:50:58,  3.15s/it]  0%|          | 218/179460 [11:34<157:28:51,  3.16s/it]  0%|          | 219/179460 [11:37<157:21:59,  3.16s/it]  0%|          | 220/179460 [11:40<156:53:53,  3.15s/it]  0%|          | 221/179460 [11:43<157:15:26,  3.16s/it]  0%|          | 222/179460 [11:46<157:48:26,  3.17s/it]  0%|          | 223/179460 [11:50<157:27:16,  3.16s/it]  0%|          | 224/179460 [11:53<157:25:04,  3.16s/it]  0%|          | 225/179460 [11:56<157:39:31,  3.17s/it]  0%|          | 226/179460 [11:59<157:26:48,  3.16s/it]  0%|          | 227/179460 [12:02<157:38:28,  3.17s/it]  0%|          | 228/179460 [12:05<156:51:03,  3.15s/it]  0%|          | 229/179460 [12:08<156:37:10,  3.15s/it]  0%|          | 230/179460 [12:12<156:29:40,  3.14s/it]  0%|          | 231/179460 [12:15<156:34:51,  3.15s/it]  0%|          | 232/179460 [12:18<156:15:54,  3.14s/it]  0%|          | 233/179460 [12:21<156:33:59,  3.14s/it]  0%|          | 234/179460 [12:24<155:55:49,  3.13s/it]  0%|          | 235/179460 [12:27<156:09:50,  3.14s/it]  0%|          | 236/179460 [12:30<157:00:04,  3.15s/it]  0%|          | 237/179460 [12:34<157:26:46,  3.16s/it]  0%|          | 238/179460 [12:37<157:30:38,  3.16s/it]  0%|          | 239/179460 [12:40<157:53:53,  3.17s/it]  0%|          | 240/179460 [12:43<157:00:53,  3.15s/it]  0%|          | 241/179460 [12:46<156:42:31,  3.15s/it]  0%|          | 242/179460 [12:49<157:03:14,  3.15s/it]  0%|          | 243/179460 [12:53<156:39:12,  3.15s/it]  0%|          | 244/179460 [12:56<157:17:30,  3.16s/it]  0%|          | 245/179460 [12:59<157:02:44,  3.15s/it]  0%|          | 246/179460 [13:02<157:14:56,  3.16s/it]  0%|          | 247/179460 [13:05<157:32:56,  3.16s/it]  0%|          | 248/179460 [13:08<156:30:28,  3.14s/it]  0%|          | 249/179460 [13:11<156:26:08,  3.14s/it]  0%|          | 250/179460 [13:15<156:42:14,  3.15s/it]  0%|          | 251/179460 [13:18<156:00:14,  3.13s/it]  0%|          | 252/179460 [13:21<156:31:50,  3.14s/it]  0%|          | 253/179460 [13:24<156:39:46,  3.15s/it]  0%|          | 254/179460 [13:27<157:15:31,  3.16s/it]  0%|          | 255/179460 [13:30<157:10:27,  3.16s/it]  0%|          | 256/179460 [13:34<157:44:42,  3.17s/it]  0%|          | 257/179460 [13:37<157:37:44,  3.17s/it]  0%|          | 258/179460 [13:40<157:03:52,  3.16s/it]  0%|          | 259/179460 [13:43<156:58:29,  3.15s/it]  0%|          | 260/179460 [13:46<157:24:59,  3.16s/it]  0%|          | 261/179460 [13:49<157:41:51,  3.17s/it]  0%|          | 262/179460 [13:53<157:47:14,  3.17s/it]  0%|          | 263/179460 [13:56<157:56:18,  3.17s/it]  0%|          | 264/179460 [13:59<158:17:15,  3.18s/it]  0%|          | 265/179460 [14:02<157:31:07,  3.16s/it]  0%|          | 266/179460 [14:05<157:17:31,  3.16s/it]  0%|          | 267/179460 [14:08<157:33:54,  3.17s/it]  0%|          | 268/179460 [14:12<156:48:08,  3.15s/it]  0%|          | 269/179460 [14:15<156:39:31,  3.15s/it]  0%|          | 270/179460 [14:18<157:17:36,  3.16s/it]  0%|          | 271/179460 [14:21<157:02:09,  3.15s/it]  0%|          | 272/179460 [14:24<156:37:05,  3.15s/it]  0%|          | 273/179460 [14:27<156:07:51,  3.14s/it]  0%|          | 274/179460 [14:30<155:22:22,  3.12s/it]  0%|          | 275/179460 [14:33<155:59:31,  3.13s/it]  0%|          | 276/179460 [14:37<156:35:56,  3.15s/it]  0%|          | 277/179460 [14:40<157:18:57,  3.16s/it]  0%|          | 278/179460 [14:43<157:45:20,  3.17s/it]  0%|          | 279/179460 [14:46<156:52:53,  3.15s/it]  0%|          | 280/179460 [14:49<157:10:48,  3.16s/it]  0%|          | 281/179460 [14:52<157:16:52,  3.16s/it]  0%|          | 282/179460 [14:56<157:44:51,  3.17s/it]  0%|          | 283/179460 [14:59<158:04:52,  3.18s/it]  0%|          | 284/179460 [15:02<158:18:29,  3.18s/it]  0%|          | 285/179460 [15:05<158:32:29,  3.19s/it]  0%|          | 286/179460 [15:08<158:41:12,  3.19s/it]  0%|          | 287/179460 [15:12<157:58:23,  3.17s/it]  0%|          | 288/179460 [15:15<157:00:20,  3.15s/it]  0%|          | 289/179460 [15:18<156:42:02,  3.15s/it]  0%|          | 290/179460 [15:21<156:51:45,  3.15s/it]  0%|          | 291/179460 [15:24<155:41:39,  3.13s/it]  0%|          | 292/179460 [15:27<155:55:04,  3.13s/it]  0%|          | 293/179460 [15:30<156:23:42,  3.14s/it]  0%|          | 294/179460 [15:34<157:06:29,  3.16s/it]  0%|          | 295/179460 [15:37<157:09:56,  3.16s/it]  0%|          | 296/179460 [15:40<156:12:20,  3.14s/it]  0%|          | 297/179460 [15:43<155:41:11,  3.13s/it]  0%|          | 298/179460 [15:46<155:24:39,  3.12s/it]  0%|          | 299/179460 [15:49<156:26:39,  3.14s/it]  0%|          | 300/179460 [15:52<156:30:49,  3.14s/it]  0%|          | 301/179460 [15:56<157:22:02,  3.16s/it]  0%|          | 302/179460 [15:59<157:35:00,  3.17s/it]  0%|          | 303/179460 [16:02<157:08:31,  3.16s/it]  0%|          | 304/179460 [16:05<156:58:22,  3.15s/it]  0%|          | 305/179460 [16:08<157:03:55,  3.16s/it]  0%|          | 306/179460 [16:11<157:16:54,  3.16s/it]  0%|          | 307/179460 [16:15<157:45:03,  3.17s/it]  0%|          | 308/179460 [16:18<157:25:45,  3.16s/it]  0%|          | 309/179460 [16:21<157:03:50,  3.16s/it]  0%|          | 310/179460 [16:24<157:36:46,  3.17s/it]  0%|          | 311/179460 [16:27<156:57:56,  3.15s/it]  0%|          | 312/179460 [16:30<156:52:18,  3.15s/it]  0%|          | 313/179460 [16:34<157:27:20,  3.16s/it]  0%|          | 314/179460 [16:37<156:49:38,  3.15s/it]  0%|          | 315/179460 [16:40<156:40:30,  3.15s/it]  0%|          | 316/179460 [16:43<155:53:54,  3.13s/it]  0%|          | 317/179460 [16:46<156:15:38,  3.14s/it]  0%|          | 318/179460 [16:49<157:13:54,  3.16s/it]  0%|          | 319/179460 [16:52<157:50:39,  3.17s/it]  0%|          | 320/179460 [16:56<157:24:23,  3.16s/it]  0%|          | 321/179460 [16:59<157:43:42,  3.17s/it]  0%|          | 322/179460 [17:02<157:22:08,  3.16s/it]  0%|          | 323/179460 [17:05<156:20:25,  3.14s/it]  0%|          | 324/179460 [17:08<156:27:58,  3.14s/it]  0%|          | 325/179460 [17:11<156:54:48,  3.15s/it]  0%|          | 326/179460 [17:14<156:31:11,  3.15s/it]  0%|          | 327/179460 [17:18<156:51:20,  3.15s/it]  0%|          | 328/179460 [17:21<156:37:25,  3.15s/it]  0%|          | 329/179460 [17:24<157:12:07,  3.16s/it]  0%|          | 330/179460 [17:27<157:18:23,  3.16s/it]  0%|          | 331/179460 [17:30<156:51:52,  3.15s/it]  0%|          | 332/179460 [17:33<157:00:01,  3.16s/it]  0%|          | 333/179460 [17:37<157:27:16,  3.16s/it]  0%|          | 334/179460 [17:40<157:52:25,  3.17s/it]  0%|          | 335/179460 [17:43<158:13:19,  3.18s/it]  0%|          | 336/179460 [17:46<157:47:36,  3.17s/it]  0%|          | 337/179460 [17:49<158:06:40,  3.18s/it]  0%|          | 338/179460 [17:52<158:01:58,  3.18s/it]  0%|          | 339/179460 [17:56<156:51:27,  3.15s/it]  0%|          | 340/179460 [17:59<156:53:40,  3.15s/it]  0%|          | 341/179460 [18:02<157:27:56,  3.16s/it]  0%|          | 342/179460 [18:05<157:27:52,  3.16s/it]  0%|          | 343/179460 [18:08<156:51:25,  3.15s/it]  0%|          | 344/179460 [18:11<156:13:54,  3.14s/it]  0%|          | 345/179460 [18:15<156:57:53,  3.15s/it]  0%|          | 346/179460 [18:18<156:35:08,  3.15s/it]  0%|          | 347/179460 [18:21<157:00:29,  3.16s/it]  0%|          | 348/179460 [18:24<156:31:28,  3.15s/it]  0%|          | 349/179460 [18:27<155:59:34,  3.14s/it]  0%|          | 350/179460 [18:30<156:10:11,  3.14s/it]  0%|          | 351/179460 [18:33<156:00:43,  3.14s/it]  0%|          | 352/179460 [18:36<155:37:53,  3.13s/it]  0%|          | 353/179460 [18:40<156:38:29,  3.15s/it]  0%|          | 354/179460 [18:43<156:32:02,  3.15s/it]  0%|          | 355/179460 [18:46<156:54:22,  3.15s/it]  0%|          | 356/179460 [18:49<156:53:04,  3.15s/it]  0%|          | 357/179460 [18:52<156:36:28,  3.15s/it]  0%|          | 358/179460 [18:55<156:58:01,  3.16s/it]  0%|          | 359/179460 [18:59<157:34:47,  3.17s/it]  0%|          | 360/179460 [19:02<157:02:53,  3.16s/it]  0%|          | 361/179460 [19:05<157:39:15,  3.17s/it]  0%|          | 362/179460 [19:08<157:39:03,  3.17s/it]  0%|          | 363/179460 [19:11<157:58:40,  3.18s/it]  0%|          | 364/179460 [19:14<157:36:07,  3.17s/it]  0%|          | 365/179460 [19:18<156:33:28,  3.15s/it]Traceback (most recent call last):
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnEng_BL.py", line 952, in <module>
    main()
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnEng_BL.py", line 784, in main
    accelerator.backward(total_loss / args.gradient_accumulation_steps)
  File "/home/sshukla7/.local/lib/python3.10/site-packages/accelerate/accelerator.py", line 1683, in backward
    loss.backward(**kwargs)
  File "/opt/sw/spack/apps/linux-rhel8-x86_64/gcc-10.3.0/python-3.10.1-qb/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/opt/sw/spack/apps/linux-rhel8-x86_64/gcc-10.3.0/python-3.10.1-qb/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 79.35 GiB total capacity; 55.32 GiB already allocated; 906.69 MiB free; 77.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1).
wandb: 
wandb: Run history:
wandb:      train/crs_att_loss █▇▇▆▇▇▇▇█▇▆▇▅▅▃▃▃▁
wandb:      train/dec_att_loss ▁▅▂▄▄▁▂█▃▅▅▇████▇▇
wandb:      train/dec_hid_loss ███▇▇▇▇▆▆▆▅▅▄▄▃▂▂▁
wandb:      train/enc_att_loss ▆█▃▄▅▂▁▁▁▂▃▂▄▄▆▄▄▄
wandb: train/enc_hid_last_loss ███▇▇▇▇▆▆▆▅▅▅▄▃▃▁▁
wandb:      train/enc_hid_loss ██▇▇▇▇▆▆▆▆▅▅▄▄▃▂▁▁
wandb:       train/logits_loss █▇▇▆▆▅▅▅▄▄▄▃▄▃▂▂▂▁
wandb:              train/loss ██▇▇▆▆▆▅▅▄▄▄▃▃▂▂▂▁
wandb:                train/lr ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██
wandb:              train/step ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██
wandb:         train/task_loss ██▇▇▆▆▅▅▄▄▄▃▃▃▂▂▁▁
wandb: 
wandb: Run summary:
wandb:      train/crs_att_loss 0.01472
wandb:      train/dec_att_loss 0.01109
wandb:      train/dec_hid_loss 1.09905
wandb:      train/enc_att_loss 0.00076
wandb: train/enc_hid_last_loss 1.44751
wandb:      train/enc_hid_loss 0.29866
wandb:       train/logits_loss 3.03146
wandb:              train/loss 15.25893
wandb:                train/lr 0.0
wandb:              train/step 359
wandb:         train/task_loss 9.36966
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/sshukla7/sshukla7/DQ_BART/wandb/offline-run-20230507_195109-qylrkq72
wandb: Find logs at: ./wandb/offline-run-20230507_195109-qylrkq72/logs
