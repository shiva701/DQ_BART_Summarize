Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pip in /home/sshukla7/.local/lib/python3.10/site-packages (23.1.2)
Defaulting to user installation because normal site-packages is not writeable
05/07/2023 19:51:08 - INFO - __main__ -   Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

05/07/2023 19:51:08 - WARNING - __main__ -   Namespace(dataset_name='cnn_dailymail', dataset_config_name='3.0.0', train_file=None, validation_file=None, ignore_pad_token_for_loss=True, max_source_length=1024, source_prefix=None, preprocessing_num_workers=None, overwrite_cache=None, max_target_length=128, val_max_target_length=None, pad_to_max_length=False, model_name_or_path='facebook/bart-large-cnn', config_name=None, tokenizer_name=None, text_column=None, summary_column=None, use_slow_tokenizer=False, per_device_train_batch_size=16, per_device_eval_batch_size=4, learning_rate=3e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=2, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.05, output_dir='./output_cnn_dailymail/8_8_6_6_20_3e-05_fp16', seed=42, model_type=None, teacher_model='facebook/bart-large-cnn', student_model='facebook/bart-large-cnn', pred_distill=True, intermediate_distill=True, weight_bits=8, input_bits=8, clip_val=2.5, length_penalty=2.0, max_length=142, min_length=56, num_beams=4, do_train=True, do_test=True, test_teacher=False, distill_encoder=6, distill_decoder=6, log_steps=20, local_rank=0, weighted=False, new_distill_map=False, task_weight=1, logits_weight=1, hid_weight=1)


the arguments are:  Namespace(dataset_name='cnn_dailymail', dataset_config_name='3.0.0', train_file=None, validation_file=None, ignore_pad_token_for_loss=True, max_source_length=1024, source_prefix=None, preprocessing_num_workers=None, overwrite_cache=None, max_target_length=128, val_max_target_length=None, pad_to_max_length=False, model_name_or_path='facebook/bart-large-cnn', config_name=None, tokenizer_name=None, text_column=None, summary_column=None, use_slow_tokenizer=False, per_device_train_batch_size=16, per_device_eval_batch_size=4, learning_rate=3e-05, weight_decay=0.0, num_train_epochs=20, max_train_steps=None, gradient_accumulation_steps=2, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.05, output_dir='./output_cnn_dailymail/8_8_6_6_20_3e-05_fp16', seed=42, model_type=None, teacher_model='facebook/bart-large-cnn', student_model='facebook/bart-large-cnn', pred_distill=True, intermediate_distill=True, weight_bits=8, input_bits=8, clip_val=2.5, length_penalty=2.0, max_length=142, min_length=56, num_beams=4, do_train=True, do_test=True, test_teacher=False, distill_encoder=6, distill_decoder=6, log_steps=20, local_rank=0, weighted=False, new_distill_map=False, task_weight=1, logits_weight=1, hid_weight=1)


the args output_dir is:  ./output_cnn_dailymail/8_8_6_6_20_3e-05_fp16
Downloading and preparing dataset cnn_dailymail/3.0.0 to /home/sshukla7/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de...
Dataset cnn_dailymail downloaded and prepared to /home/sshukla7/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de. Subsequent calls will reuse this data.
init quantize emb




 The raw datset is :  DatasetDict({
    train: Dataset({
        features: ['article', 'highlights', 'id'],
        num_rows: 287113
    })
    validation: Dataset({
        features: ['article', 'highlights', 'id'],
        num_rows: 13368
    })
    test: Dataset({
        features: ['article', 'highlights', 'id'],
        num_rows: 11490
    })
})
datasets size, train: 287113, validate: 13368, test: 11490
05/07/2023 19:54:35 - INFO - __main__ -   Sample 58369 of the training set: {'input_ids': [0, 1640, 16256, 43, 480, 83, 11259, 262, 4, 466, 8969, 2322, 160, 5, 3673, 9, 1353, 12132, 15, 307, 1559, 6, 2429, 379, 82, 8, 1618, 1510, 2581, 6, 270, 6284, 6603, 26, 15, 632, 2384, 4, 10829, 990, 36321, 860, 7, 146, 1519, 15, 49, 3551, 4247, 11, 18471, 6, 12132, 6, 71, 10, 670, 8969, 307, 4, 12132, 18, 5302, 8015, 636, 1113, 1012, 342, 5, 744, 5831, 23, 601, 4, 85, 969, 4338, 9, 1703, 4666, 11, 5, 812, 6, 18471, 6, 17980, 154, 19, 5, 15015, 4, 572, 960, 2294, 14375, 6, 5679, 2857, 58, 450, 36528, 7, 10, 693, 4, 20, 569, 67, 969, 26521, 9, 37351, 14, 56, 4491, 31, 3413, 4, 993, 18471, 1196, 58, 18769, 6721, 71, 5, 11030, 3662, 368, 6, 150, 643, 1382, 7, 28, 17587, 4, 22, 713, 34, 57, 5, 144, 15567, 676, 52, 348, 56, 60, 15151, 2459, 255, 4422, 1584, 324, 24772, 174, 3480, 31, 69, 184, 11, 18471, 4, 22, 243, 21, 2778, 251, 1666, 393, 11, 127, 301, 56, 38, 2984, 42, 251, 41, 8969, 60, 79, 26, 4, 1437, 4250, 5, 18471, 3313, 6190, 141, 82, 58, 128, 1193, 25754, 8, 17587, 108, 9313, 479, 1876, 82, 58, 751, 11, 5, 2827, 6, 79, 26, 6, 25, 5, 3188, 2449, 106, 14, 71, 1193, 6368, 115, 1407, 5, 15015, 4, 255, 4422, 1584, 324, 24772, 26, 79, 21, 2445, 11, 69, 512, 6, 6023, 7, 213, 124, 1025, 4, 22, 243, 21, 7724, 60, 26, 12615, 20419, 261, 6, 41, 470, 3918, 12132, 4, 22, 15216, 554, 9701, 480, 1159, 6, 3370, 4, 16225, 554, 878, 1706, 143, 5802, 980, 4, 16225, 21, 6023, 5, 3413, 58, 164, 7, 6277, 72, 22, 243, 18, 41, 11522, 676, 6, 142, 89, 18, 117, 2892, 60, 37, 26, 4, 20, 15015, 14774, 18109, 6815, 6, 25, 157, 25, 1947, 583, 5, 3673, 8, 5, 9787, 4, 345, 58, 476, 66, 3443, 11, 18471, 6, 1201, 431, 6, 8, 82, 2075, 88, 5, 2827, 11, 9810, 25, 5, 6110, 15188, 14774, 558, 3413, 4, 1876, 4711, 751, 6, 6023, 7, 213, 124, 20317, 71, 5, 8383, 9, 678, 71, 1193, 6368, 4, 2276, 6, 16309, 8383, 8, 11966, 1167, 71, 5, 15015, 13, 484, 1505, 8, 391, 470, 749, 58, 9491, 307, 363, 6, 25, 21, 10, 16309, 7640, 13, 6467, 4, 12132, 6, 8, 144, 9, 5, 391, 470, 3073, 2565, 6, 32, 15, 1424, 9, 80, 326, 9041, 10003, 12957, 35, 20, 391, 470, 5299, 6, 61, 1171, 144, 9, 5, 9183, 6, 8, 5, 14618, 3245, 5299, 6, 61, 14269, 420, 5, 3073, 552, 144, 9, 5, 3673, 4, 1437, 4250, 10, 5456, 9, 147, 5, 15015, 2322, 9313, 479, 20, 15015, 21, 1299, 13, 80, 728, 6, 309, 7, 2595, 32852, 433, 4, 28660, 6410, 58, 431, 11, 18471, 6, 8, 1830, 1028, 544, 21, 18525, 4, 20, 8095, 1139, 9, 221, 15069, 6, 59, 10572, 1788, 36, 31297, 8130, 43, 2077, 9, 18471, 6, 2092, 7, 28, 5, 11111, 12, 9591, 6, 8, 4382, 21, 66, 11, 5, 1139, 6, 4380, 4, 7866, 4677, 1075, 1696, 6, 471, 9, 12132, 18, 632, 2366, 1443, 14619, 6, 174, 3480, 1016, 4, 91, 26, 37, 115, 45, 4559, 143, 3257, 4, 20, 15015, 2322, 23, 231, 35, 4006, 181, 4, 119, 4, 36, 406, 35, 4006, 181, 4, 119, 4, 4799, 43, 8, 21, 14889, 564, 1788, 36, 5606, 8130, 43, 3072, 12, 25407, 10823, 9, 732, 3796, 102, 726, 4349, 6, 12132, 6, 8, 1814, 1788, 36, 28490, 8130, 43, 2077, 12, 29, 40440, 9, 18471, 6, 309, 7, 5, 121, 4, 104, 4, 26008, 11624, 4, 20, 9066, 11798, 21, 564, 1788, 36, 3706, 8130, 43, 874, 5, 3875, 18, 4084, 4, 901, 33704, 994, 1432, 4, 83, 11259, 195, 4, 398, 15015, 2756, 23, 262, 35, 4197, 181, 4, 119, 4, 36, 398, 35, 4197, 181, 4, 119, 4, 4799, 322, 85, 21, 18156, 18109, 6, 14889, 1510, 1788, 36, 19547, 8130, 43, 9489, 9, 732, 3796, 102, 726, 4349, 8, 11571, 1788, 36, 26340, 8130, 43, 3017, 12, 29, 40440, 9, 18471, 4, 178, 23, 262, 35, 1646, 181, 4, 119, 482, 277, 2735, 15015, 9, 195, 4, 466, 11259, 2756, 6, 14889, 389, 1788, 36, 3818, 8130, 43, 2077, 12, 25515, 10823, 9, 38, 3245, 6, 12132, 6, 8, 8963, 1788, 36, 24675, 8130, 43, 2077, 12, 29, 40440, 9, 18471, 4, 1437, 381, 12, 6380, 7, 10, 1441, 479, 1201, 3162, 7, 42, 266, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 5341, 35, 7303, 879, 5602, 8383, 8, 11966, 9491, 6, 25, 16, 6467, 18, 7640, 479, 50118, 3750, 513, 379, 82, 848, 6, 1510, 1710, 11, 15015, 479, 50118, 12444, 5113, 21, 1299, 13, 80, 728, 131, 82, 2075, 66, 9, 558, 3413, 11, 9810, 479, 50118, 26519, 6944, 6410, 6, 476, 66, 3443, 431, 11, 18471, 6, 12132, 18, 812, 479, 2]}.
Yes, cuda is available, Found device: NVIDIA A100-SXM-80GB, n_gpu: 1
05/07/2023 19:54:40 - INFO - __main__ -   ***** Running training *****
05/07/2023 19:54:40 - INFO - __main__ -     Num examples = 287113
05/07/2023 19:54:40 - INFO - __main__ -     Num Epochs = 20
05/07/2023 19:54:40 - INFO - __main__ -     Instantaneous batch size per device = 16
05/07/2023 19:54:40 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
05/07/2023 19:54:40 - INFO - __main__ -     Gradient Accumulation steps = 2
05/07/2023 19:54:40 - INFO - __main__ -     Total optimization steps = 179460
05/07/2023 19:54:40 - INFO - __main__ -     student encoder layers = 6
05/07/2023 19:54:40 - INFO - __main__ -     student decoder layers = 6
05/07/2023 19:54:40 - INFO - __main__ -     student encoder layers [0, 1, 2, 3, 4, 5] is mapped with teacher encoder layers [0, 1, 2, 3, 4, 5]
05/07/2023 19:54:40 - INFO - __main__ -     student decoder layers [0, 1, 2, 3, 4, 5] is mapped with teacher decoder layers [0, 1, 2, 3, 4, 5]
