
Lmod is automatically replacing "gnu9/9.3.0" with "gnu10/10.3.0-ya".


Inactive Modules:
  1) hwloc/2.1.0     2) libfabric     3) openmpi4     4) ucx

2023-05-01 19:32:50.447673: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-01 19:32:55.232571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: Tracking run with wandb version 0.15.0
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|          | 0/3 [00:00<?, ?it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.31it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.64it/s]
loading configuration file https://huggingface.co/ml6team/mbart-large-cc25-cnn-dailymail-xsum-nl/resolve/main/config.json from cache at /home/sshukla7/.cache/huggingface/transformers/59b78858281cd51b95e0e516d522745ea1b5cceddf393b9924adf18cffa18649.50d4c165622078f11e287b8feffe721d1b22c39ce210fa7b64055bea6d5b4d6c
Model config MBartConfig {
  "_name_or_path": "ml6team/mbart-large-cc25-cnn-dailymail-xsum-nl",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": true,
  "architectures": [
    "MBartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_length": 1024,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "normalize_before": true,
  "normalize_embedding": true,
  "num_beams": 5,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "scale_embedding": true,
  "static_position_embeddings": false,
  "task_specific_params": {
    "translation_en_to_ro": {
      "decoder_start_token_id": 250020
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 250027
}

Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file https://huggingface.co/facebook/mbart-large-cc25/resolve/main/config.json from cache at /home/sshukla7/.cache/huggingface/transformers/36135304685d914515720daa48fc1adae57803e32ab82d5bde85ef78479e9765.b548f7e307531070391a881374674824b374f829e5d8f68857012de63fe2681a
Model config MBartConfig {
  "_name_or_path": "facebook/mbart-large-cc25",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": true,
  "architectures": [
    "MBartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_length": 1024,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "normalize_before": true,
  "normalize_embedding": true,
  "num_beams": 5,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "scale_embedding": true,
  "static_position_embeddings": false,
  "task_specific_params": {
    "translation_en_to_ro": {
      "decoder_start_token_id": 250020
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 250027
}

loading file https://huggingface.co/facebook/mbart-large-cc25/resolve/main/sentencepiece.bpe.model from cache at /home/sshukla7/.cache/huggingface/transformers/83d419fb34e90155a8d95f7799f7a7316a327dc28c7ee6bee15b5a62d3c5ca6b.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading file https://huggingface.co/facebook/mbart-large-cc25/resolve/main/tokenizer.json from cache at /home/sshukla7/.cache/huggingface/transformers/16e85cac0e7a8c2938ac468199d0adff7483341305c7e848063b72dcf5f22538.39607a8bede9bcd2666ea442230a9d382f57e4fea127c9cc5b6fc6caf527d682
loading file https://huggingface.co/facebook/mbart-large-cc25/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/facebook/mbart-large-cc25/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/facebook/mbart-large-cc25/resolve/main/tokenizer_config.json from cache at None
loading configuration file https://huggingface.co/facebook/mbart-large-cc25/resolve/main/config.json from cache at /home/sshukla7/.cache/huggingface/transformers/36135304685d914515720daa48fc1adae57803e32ab82d5bde85ef78479e9765.b548f7e307531070391a881374674824b374f829e5d8f68857012de63fe2681a
Model config MBartConfig {
  "_name_or_path": "facebook/mbart-large-cc25",
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": true,
  "architectures": [
    "MBartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_length": 1024,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "normalize_before": true,
  "normalize_embedding": true,
  "num_beams": 5,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "scale_embedding": true,
  "static_position_embeddings": false,
  "task_specific_params": {
    "translation_en_to_ro": {
      "decoder_start_token_id": 250020
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 250027
}

Assigning ['ar_AR', 'cs_CZ', 'de_DE', 'en_XX', 'es_XX', 'et_EE', 'fi_FI', 'fr_XX', 'gu_IN', 'hi_IN', 'it_IT', 'ja_XX', 'kk_KZ', 'ko_KR', 'lt_LT', 'lv_LV', 'my_MM', 'ne_NP', 'nl_XX', 'ro_RO', 'ru_RU', 'si_LK', 'tr_TR', 'vi_VN', 'zh_CN'] to the additional_special_tokens key of the tokenizer
loading weights file https://huggingface.co/ml6team/mbart-large-cc25-cnn-dailymail-xsum-nl/resolve/main/pytorch_model.bin from cache at /home/sshukla7/.cache/huggingface/transformers/4b004e1d7bcef58422317f6ea8b15b75971e5f490ceeefad5baafc9fd756870f.95bef7eadd9669200273f2d88fa759398f7ca026a631169f2b27d530280273a8
All model checkpoint weights were used when initializing MBartForConditionalGeneration.

All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at ml6team/mbart-large-cc25-cnn-dailymail-xsum-nl.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.
/home/sshukla7/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:649: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   0%|          | 0/287113 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=10):   0%|          | 1000/287113 [00:03<14:48, 321.93 examples/s]Running tokenizer on dataset (num_proc=10):   1%|          | 2000/287113 [00:03<06:33, 724.33 examples/s]Running tokenizer on dataset (num_proc=10):   1%|â–         | 4000/287113 [00:03<02:36, 1812.92 examples/s]Running tokenizer on dataset (num_proc=10):   3%|â–Ž         | 9000/287113 [00:03<00:54, 5090.51 examples/s]Running tokenizer on dataset (num_proc=10):   4%|â–         | 11000/287113 [00:05<02:02, 2258.41 examples/s]Running tokenizer on dataset (num_proc=10):   5%|â–         | 13000/287113 [00:05<01:36, 2841.72 examples/s]Running tokenizer on dataset (num_proc=10):   6%|â–‹         | 18000/287113 [00:06<00:53, 5036.86 examples/s]Running tokenizer on dataset (num_proc=10):   7%|â–‹         | 20000/287113 [00:06<00:59, 4517.72 examples/s]Running tokenizer on dataset (num_proc=10):   8%|â–Š         | 22000/287113 [00:08<01:34, 2797.02 examples/s]Running tokenizer on dataset (num_proc=10):   9%|â–‰         | 27000/287113 [00:08<00:53, 4869.09 examples/s]Running tokenizer on dataset (num_proc=10):  10%|â–ˆ         | 29000/287113 [00:09<00:57, 4464.60 examples/s]Running tokenizer on dataset (num_proc=10):  11%|â–ˆ         | 31000/287113 [00:10<01:28, 2887.99 examples/s]Running tokenizer on dataset (num_proc=10):  11%|â–ˆâ–        | 33000/287113 [00:10<01:10, 3590.78 examples/s]Running tokenizer on dataset (num_proc=10):  12%|â–ˆâ–        | 35000/287113 [00:10<00:55, 4561.19 examples/s]Running tokenizer on dataset (num_proc=10):  13%|â–ˆâ–Ž        | 38000/287113 [00:11<00:42, 5813.93 examples/s]Running tokenizer on dataset (num_proc=10):  14%|â–ˆâ–        | 40000/287113 [00:11<00:58, 4249.04 examples/s]Running tokenizer on dataset (num_proc=10):  14%|â–ˆâ–        | 41000/287113 [00:13<01:29, 2755.19 examples/s]Running tokenizer on dataset (num_proc=10):  15%|â–ˆâ–        | 43000/287113 [00:13<01:05, 3722.15 examples/s]Running tokenizer on dataset (num_proc=10):  16%|â–ˆâ–Œ        | 45000/287113 [00:13<00:51, 4705.31 examples/s]Running tokenizer on dataset (num_proc=10):  17%|â–ˆâ–‹        | 48000/287113 [00:13<00:39, 6038.20 examples/s]Running tokenizer on dataset (num_proc=10):  17%|â–ˆâ–‹        | 50000/287113 [00:14<00:58, 4059.09 examples/s]Running tokenizer on dataset (num_proc=10):  18%|â–ˆâ–Š        | 51000/287113 [00:15<01:23, 2830.06 examples/s]Running tokenizer on dataset (num_proc=10):  19%|â–ˆâ–‰        | 54000/287113 [00:15<00:57, 4058.05 examples/s]Running tokenizer on dataset (num_proc=10):  20%|â–ˆâ–‰        | 57000/287113 [00:15<00:39, 5841.17 examples/s]Running tokenizer on dataset (num_proc=10):  21%|â–ˆâ–ˆ        | 59000/287113 [00:16<00:53, 4293.00 examples/s]Running tokenizer on dataset (num_proc=10):  21%|â–ˆâ–ˆ        | 60000/287113 [00:16<00:56, 4024.16 examples/s]Running tokenizer on dataset (num_proc=10):  21%|â–ˆâ–ˆ        | 61000/287113 [00:17<01:20, 2804.34 examples/s]Running tokenizer on dataset (num_proc=10):  22%|â–ˆâ–ˆâ–       | 64000/287113 [00:18<00:53, 4192.91 examples/s]Running tokenizer on dataset (num_proc=10):  23%|â–ˆâ–ˆâ–Ž       | 67000/287113 [00:18<00:36, 6046.31 examples/s]Running tokenizer on dataset (num_proc=10):  24%|â–ˆâ–ˆâ–       | 69000/287113 [00:19<00:53, 4054.74 examples/s]Running tokenizer on dataset (num_proc=10):  24%|â–ˆâ–ˆâ–       | 70000/287113 [00:19<00:53, 4029.38 examples/s]Running tokenizer on dataset (num_proc=10):  25%|â–ˆâ–ˆâ–       | 71000/287113 [00:20<01:14, 2887.86 examples/s]Running tokenizer on dataset (num_proc=10):  25%|â–ˆâ–ˆâ–Œ       | 73000/287113 [00:20<00:54, 3939.03 examples/s]Running tokenizer on dataset (num_proc=10):  26%|â–ˆâ–ˆâ–Œ       | 74000/287113 [00:20<00:50, 4234.65 examples/s]Running tokenizer on dataset (num_proc=10):  27%|â–ˆâ–ˆâ–‹       | 78000/287113 [00:20<00:30, 6773.13 examples/s]Running tokenizer on dataset (num_proc=10):  28%|â–ˆâ–ˆâ–Š       | 79000/287113 [00:21<00:55, 3769.27 examples/s]Running tokenizer on dataset (num_proc=10):  28%|â–ˆâ–ˆâ–Š       | 80000/287113 [00:21<00:52, 3929.12 examples/s]Running tokenizer on dataset (num_proc=10):  28%|â–ˆâ–ˆâ–Š       | 81000/287113 [00:22<01:09, 2945.05 examples/s]Running tokenizer on dataset (num_proc=10):  29%|â–ˆâ–ˆâ–‰       | 83000/287113 [00:22<00:52, 3899.76 examples/s]Running tokenizer on dataset (num_proc=10):  29%|â–ˆâ–ˆâ–‰       | 84000/287113 [00:22<00:46, 4329.35 examples/s]Running tokenizer on dataset (num_proc=10):  31%|â–ˆâ–ˆâ–ˆ       | 88000/287113 [00:23<00:30, 6497.33 examples/s]Running tokenizer on dataset (num_proc=10):  31%|â–ˆâ–ˆâ–ˆ       | 89000/287113 [00:24<00:56, 3481.96 examples/s]Running tokenizer on dataset (num_proc=10):  31%|â–ˆâ–ˆâ–ˆâ–      | 90000/287113 [00:24<00:49, 3962.37 examples/s]Running tokenizer on dataset (num_proc=10):  32%|â–ˆâ–ˆâ–ˆâ–      | 91000/287113 [00:24<01:00, 3250.22 examples/s]Running tokenizer on dataset (num_proc=10):  32%|â–ˆâ–ˆâ–ˆâ–      | 93000/287113 [00:25<00:47, 4068.99 examples/s]Running tokenizer on dataset (num_proc=10):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 94000/287113 [00:25<00:44, 4295.42 examples/s]Running tokenizer on dataset (num_proc=10):  34%|â–ˆâ–ˆâ–ˆâ–      | 98000/287113 [00:25<00:28, 6639.16 examples/s]Running tokenizer on dataset (num_proc=10):  34%|â–ˆâ–ˆâ–ˆâ–      | 99000/287113 [00:26<00:56, 3358.33 examples/s]Running tokenizer on dataset (num_proc=10):  35%|â–ˆâ–ˆâ–ˆâ–      | 100000/287113 [00:26<00:48, 3825.81 examples/s]Running tokenizer on dataset (num_proc=10):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 101000/287113 [00:27<00:54, 3409.12 examples/s]Running tokenizer on dataset (num_proc=10):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 103000/287113 [00:27<00:43, 4260.94 examples/s]Running tokenizer on dataset (num_proc=10):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 104000/287113 [00:27<00:41, 4431.18 examples/s]Running tokenizer on dataset (num_proc=10):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 108000/287113 [00:28<00:26, 6710.60 examples/s]Running tokenizer on dataset (num_proc=10):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 109000/287113 [00:29<00:56, 3178.84 examples/s]Running tokenizer on dataset (num_proc=10):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 111000/287113 [00:29<00:50, 3505.40 examples/s]Running tokenizer on dataset (num_proc=10):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 113000/287113 [00:29<00:41, 4222.59 examples/s]Running tokenizer on dataset (num_proc=10):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 114000/287113 [00:30<00:38, 4502.34 examples/s]Running tokenizer on dataset (num_proc=10):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 116000/287113 [00:30<00:30, 5679.51 examples/s]Running tokenizer on dataset (num_proc=10):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 118000/287113 [00:30<00:26, 6295.90 examples/s]Running tokenizer on dataset (num_proc=10):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119000/287113 [00:31<01:00, 2769.95 examples/s]Running tokenizer on dataset (num_proc=10):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121000/287113 [00:31<00:46, 3556.02 examples/s]Running tokenizer on dataset (num_proc=10):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 123000/287113 [00:32<00:40, 4036.45 examples/s]Running tokenizer on dataset (num_proc=10):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 124000/287113 [00:32<00:38, 4276.15 examples/s]Running tokenizer on dataset (num_proc=10):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 127000/287113 [00:32<00:29, 5405.11 examples/s]Running tokenizer on dataset (num_proc=10):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 129000/287113 [00:34<00:50, 3131.87 examples/s]Running tokenizer on dataset (num_proc=10):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130000/287113 [00:34<00:44, 3494.59 examples/s]Running tokenizer on dataset (num_proc=10):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 132000/287113 [00:34<00:32, 4704.16 examples/s]Running tokenizer on dataset (num_proc=10):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 133000/287113 [00:34<00:35, 4342.15 examples/s]Running tokenizer on dataset (num_proc=10):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 134000/287113 [00:34<00:33, 4572.61 examples/s]Running tokenizer on dataset (num_proc=10):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 136000/287113 [00:35<00:26, 5799.13 examples/s]Running tokenizer on dataset (num_proc=10):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 137000/287113 [00:35<00:29, 5058.89 examples/s]Running tokenizer on dataset (num_proc=10):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 138000/287113 [00:35<00:29, 5008.98 examples/s]Running tokenizer on dataset (num_proc=10):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 139000/287113 [00:36<00:59, 2507.73 examples/s]Running tokenizer on dataset (num_proc=10):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 140000/287113 [00:36<00:47, 3119.69 examples/s]Running tokenizer on dataset (num_proc=10):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 142000/287113 [00:36<00:30, 4723.99 examples/s]Running tokenizer on dataset (num_proc=10):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 143000/287113 [00:37<00:32, 4491.67 examples/s]Running tokenizer on dataset (num_proc=10):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 144000/287113 [00:37<00:29, 4783.76 examples/s]Running tokenizer on dataset (num_proc=10):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 145000/287113 [00:37<00:26, 5358.07 examples/s]Running tokenizer on dataset (num_proc=10):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 146000/287113 [00:37<00:24, 5789.00 examples/s]Running tokenizer on dataset (num_proc=10):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 147000/287113 [00:37<00:33, 4127.07 examples/s]Running tokenizer on dataset (num_proc=10):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148000/287113 [00:38<00:28, 4829.83 examples/s]Running tokenizer on dataset (num_proc=10):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 149000/287113 [00:39<00:58, 2349.51 examples/s]Running tokenizer on dataset (num_proc=10):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 151000/287113 [00:39<00:37, 3586.70 examples/s]Running tokenizer on dataset (num_proc=10):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 153000/287113 [00:39<00:32, 4083.01 examples/s]Running tokenizer on dataset (num_proc=10):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 155000/287113 [00:39<00:24, 5504.19 examples/s]Running tokenizer on dataset (num_proc=10):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156000/287113 [00:39<00:23, 5589.57 examples/s]Running tokenizer on dataset (num_proc=10):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 157000/287113 [00:40<00:29, 4354.34 examples/s]Running tokenizer on dataset (num_proc=10):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 158000/287113 [00:40<00:28, 4463.53 examples/s]Running tokenizer on dataset (num_proc=10):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 159000/287113 [00:41<00:50, 2539.48 examples/s]Running tokenizer on dataset (num_proc=10):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 161000/287113 [00:41<00:34, 3604.23 examples/s]Running tokenizer on dataset (num_proc=10):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 163000/287113 [00:42<00:30, 4068.71 examples/s]Running tokenizer on dataset (num_proc=10):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 165000/287113 [00:42<00:22, 5533.69 examples/s]Running tokenizer on dataset (num_proc=10):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 166000/287113 [00:42<00:21, 5667.58 examples/s]Running tokenizer on dataset (num_proc=10):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 167000/287113 [00:42<00:28, 4198.49 examples/s]Running tokenizer on dataset (num_proc=10):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 168000/287113 [00:42<00:27, 4293.06 examples/s]Running tokenizer on dataset (num_proc=10):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 169000/287113 [00:43<00:42, 2757.32 examples/s]Running tokenizer on dataset (num_proc=10):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 170000/287113 [00:43<00:38, 3050.41 examples/s]Running tokenizer on dataset (num_proc=10):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 171000/287113 [00:44<00:30, 3750.18 examples/s]Running tokenizer on dataset (num_proc=10):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 172000/287113 [00:44<00:26, 4269.31 examples/s]Running tokenizer on dataset (num_proc=10):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 173000/287113 [00:44<00:25, 4437.17 examples/s]Running tokenizer on dataset (num_proc=10):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 175000/287113 [00:44<00:18, 6106.83 examples/s]Running tokenizer on dataset (num_proc=10):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 176000/287113 [00:44<00:18, 5926.49 examples/s]Running tokenizer on dataset (num_proc=10):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 177000/287113 [00:45<00:25, 4319.94 examples/s]Running tokenizer on dataset (num_proc=10):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 178000/287113 [00:45<00:26, 4162.73 examples/s]Running tokenizer on dataset (num_proc=10):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179000/287113 [00:46<00:38, 2834.52 examples/s]Running tokenizer on dataset (num_proc=10):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 180000/287113 [00:46<00:36, 2963.09 examples/s]Running tokenizer on dataset (num_proc=10):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 182000/287113 [00:46<00:26, 3948.45 examples/s]Running tokenizer on dataset (num_proc=10):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 183000/287113 [00:46<00:23, 4472.39 examples/s]Running tokenizer on dataset (num_proc=10):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 185000/287113 [00:47<00:16, 6029.74 examples/s]Running tokenizer on dataset (num_proc=10):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 186000/287113 [00:47<00:18, 5502.94 examples/s]Running tokenizer on dataset (num_proc=10):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 187000/287113 [00:47<00:23, 4184.99 examples/s]Running tokenizer on dataset (num_proc=10):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 188000/287113 [00:47<00:25, 3955.05 examples/s]Running tokenizer on dataset (num_proc=10):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 189000/287113 [00:48<00:32, 3056.47 examples/s]Running tokenizer on dataset (num_proc=10):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 190000/287113 [00:48<00:33, 2921.74 examples/s]Running tokenizer on dataset (num_proc=10):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 192000/287113 [00:49<00:25, 3676.94 examples/s]Running tokenizer on dataset (num_proc=10):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 195000/287113 [00:49<00:15, 5858.23 examples/s]Running tokenizer on dataset (num_proc=10):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 196000/287113 [00:49<00:16, 5413.68 examples/s]Running tokenizer on dataset (num_proc=10):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 197000/287113 [00:50<00:21, 4118.22 examples/s]Running tokenizer on dataset (num_proc=10):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 198000/287113 [00:50<00:21, 4115.43 examples/s]Running tokenizer on dataset (num_proc=10):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 199000/287113 [00:50<00:26, 3352.27 examples/s]Running tokenizer on dataset (num_proc=10):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 200000/287113 [00:51<00:30, 2837.89 examples/s]Running tokenizer on dataset (num_proc=10):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 202000/287113 [00:51<00:22, 3704.43 examples/s]Running tokenizer on dataset (num_proc=10):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 204000/287113 [00:51<00:15, 5242.39 examples/s]Running tokenizer on dataset (num_proc=10):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 206000/287113 [00:52<00:14, 5462.46 examples/s]Running tokenizer on dataset (num_proc=10):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 207000/287113 [00:52<00:20, 3985.01 examples/s]Running tokenizer on dataset (num_proc=10):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 208000/287113 [00:52<00:17, 4494.80 examples/s]Running tokenizer on dataset (num_proc=10):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 209000/287113 [00:53<00:21, 3643.32 examples/s]Running tokenizer on dataset (num_proc=10):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 210000/287113 [00:53<00:25, 2980.56 examples/s]Running tokenizer on dataset (num_proc=10):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 211000/287113 [00:53<00:21, 3600.59 examples/s]Running tokenizer on dataset (num_proc=10):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 212000/287113 [00:54<00:19, 3855.74 examples/s]Running tokenizer on dataset (num_proc=10):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 214000/287113 [00:54<00:13, 5314.79 examples/s]Running tokenizer on dataset (num_proc=10):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 216000/287113 [00:54<00:11, 5962.03 examples/s]Running tokenizer on dataset (num_proc=10):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 217000/287113 [00:55<00:18, 3834.37 examples/s]Running tokenizer on dataset (num_proc=10):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 219000/287113 [00:55<00:15, 4348.48 examples/s]Running tokenizer on dataset (num_proc=10):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 220000/287113 [00:56<00:21, 3188.03 examples/s]Running tokenizer on dataset (num_proc=10):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 221000/287113 [00:56<00:18, 3503.09 examples/s]Running tokenizer on dataset (num_proc=10):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 222000/287113 [00:56<00:16, 3883.20 examples/s]Running tokenizer on dataset (num_proc=10):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 224000/287113 [00:56<00:11, 5319.00 examples/s]Running tokenizer on dataset (num_proc=10):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 225000/287113 [00:56<00:11, 5593.69 examples/s]Running tokenizer on dataset (num_proc=10):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 226000/287113 [00:56<00:09, 6156.87 examples/s]Running tokenizer on dataset (num_proc=10):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 227000/287113 [00:57<00:17, 3525.50 examples/s]Running tokenizer on dataset (num_proc=10):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 229000/287113 [00:57<00:13, 4367.47 examples/s]Running tokenizer on dataset (num_proc=10):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 230000/287113 [00:58<00:19, 2945.03 examples/s]Running tokenizer on dataset (num_proc=10):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 231000/287113 [00:58<00:16, 3334.77 examples/s]Running tokenizer on dataset (num_proc=10):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 232000/287113 [00:58<00:15, 3633.27 examples/s]Running tokenizer on dataset (num_proc=10):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 235000/287113 [00:59<00:09, 5220.36 examples/s]Running tokenizer on dataset (num_proc=10):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 237000/287113 [00:59<00:12, 4111.25 examples/s]Running tokenizer on dataset (num_proc=10):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 239000/287113 [01:00<00:12, 3980.37 examples/s]Running tokenizer on dataset (num_proc=10):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 240000/287113 [01:01<00:15, 3040.66 examples/s]Running tokenizer on dataset (num_proc=10):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 242000/287113 [01:01<00:12, 3498.91 examples/s]Running tokenizer on dataset (num_proc=10):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 245000/287113 [01:01<00:08, 4921.69 examples/s]Running tokenizer on dataset (num_proc=10):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 246000/287113 [01:01<00:07, 5296.36 examples/s]Running tokenizer on dataset (num_proc=10):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 247000/287113 [01:02<00:11, 3633.56 examples/s]Running tokenizer on dataset (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 249000/287113 [01:02<00:08, 4498.06 examples/s]Running tokenizer on dataset (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 250000/287113 [01:03<00:12, 2995.16 examples/s]Running tokenizer on dataset (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 251000/287113 [01:03<00:11, 3196.97 examples/s]Running tokenizer on dataset (num_proc=10):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 253000/287113 [01:04<00:07, 4483.33 examples/s]Running tokenizer on dataset (num_proc=10):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 255000/287113 [01:04<00:06, 4732.09 examples/s]Running tokenizer on dataset (num_proc=10):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 256000/287113 [01:04<00:06, 4720.87 examples/s]Running tokenizer on dataset (num_proc=10):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 257000/287113 [01:05<00:07, 3787.44 examples/s]Running tokenizer on dataset (num_proc=10):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 259000/287113 [01:05<00:05, 5428.63 examples/s]Running tokenizer on dataset (num_proc=10):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 260000/287113 [01:06<00:09, 2991.82 examples/s]Running tokenizer on dataset (num_proc=10):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 261000/287113 [01:06<00:08, 3251.20 examples/s]Running tokenizer on dataset (num_proc=10):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 263000/287113 [01:06<00:05, 4661.75 examples/s]Running tokenizer on dataset (num_proc=10):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 265000/287113 [01:06<00:04, 4935.12 examples/s]Running tokenizer on dataset (num_proc=10):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 266000/287113 [01:07<00:04, 4330.91 examples/s]Running tokenizer on dataset (num_proc=10):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 267000/287113 [01:07<00:05, 3746.66 examples/s]Running tokenizer on dataset (num_proc=10):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 270000/287113 [01:08<00:05, 3383.41 examples/s]Running tokenizer on dataset (num_proc=10):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 271000/287113 [01:08<00:04, 3686.21 examples/s]Running tokenizer on dataset (num_proc=10):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 272000/287113 [01:08<00:03, 4139.93 examples/s]Running tokenizer on dataset (num_proc=10):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 275000/287113 [01:09<00:02, 5233.56 examples/s]Running tokenizer on dataset (num_proc=10):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 276712/287113 [01:09<00:02, 4373.95 examples/s]Running tokenizer on dataset (num_proc=10):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 277712/287113 [01:10<00:02, 4388.61 examples/s]Running tokenizer on dataset (num_proc=10):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 279423/287113 [01:10<00:01, 4734.64 examples/s]Running tokenizer on dataset (num_proc=10):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 280135/287113 [01:10<00:01, 4383.16 examples/s]Running tokenizer on dataset (num_proc=10):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 280846/287113 [01:10<00:01, 4709.65 examples/s]Running tokenizer on dataset (num_proc=10):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 282269/287113 [01:10<00:00, 5041.00 examples/s]Running tokenizer on dataset (num_proc=10):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 283269/287113 [01:11<00:00, 5084.16 examples/s]Running tokenizer on dataset (num_proc=10):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 283980/287113 [01:11<00:01, 2825.91 examples/s]Running tokenizer on dataset (num_proc=10):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 284691/287113 [01:11<00:00, 3261.55 examples/s]Running tokenizer on dataset (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 285691/287113 [01:12<00:00, 2303.14 examples/s]Running tokenizer on dataset (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 286402/287113 [01:12<00:00, 2331.09 examples/s]Running tokenizer on dataset (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287113/287113 [01:14<00:00, 1083.06 examples/s]                                                                                                            Running tokenizer on dataset (num_proc=10):   0%|          | 0/13368 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=10):   7%|â–‹         | 1000/13368 [00:03<00:37, 333.17 examples/s]Running tokenizer on dataset (num_proc=10):  22%|â–ˆâ–ˆâ–       | 3000/13368 [00:03<00:08, 1214.53 examples/s]Running tokenizer on dataset (num_proc=10):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 6000/13368 [00:03<00:02, 2634.80 examples/s]Running tokenizer on dataset (num_proc=10):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 9000/13368 [00:03<00:00, 4390.87 examples/s]Running tokenizer on dataset (num_proc=10):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10337/13368 [00:03<00:00, 4601.60 examples/s]Running tokenizer on dataset (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 11683/13368 [00:04<00:00, 4973.87 examples/s]Running tokenizer on dataset (num_proc=10):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12694/13368 [00:04<00:00, 3955.52 examples/s]                                                                                                          Running tokenizer on dataset (num_proc=10):   0%|          | 0/11490 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=10):   9%|â–Š         | 1000/11490 [00:02<00:31, 333.92 examples/s]Running tokenizer on dataset (num_proc=10):  17%|â–ˆâ–‹        | 2000/11490 [00:03<00:12, 762.36 examples/s]Running tokenizer on dataset (num_proc=10):  27%|â–ˆâ–ˆâ–‹       | 3149/11490 [00:03<00:06, 1216.67 examples/s]Running tokenizer on dataset (num_proc=10):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 6149/11490 [00:03<00:01, 3190.25 examples/s]Running tokenizer on dataset (num_proc=10):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 8298/11490 [00:03<00:00, 4793.24 examples/s]Running tokenizer on dataset (num_proc=10):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10447/11490 [00:03<00:00, 6005.75 examples/s]                                                                                                          /home/sshukla7/.local/lib/python3.10/site-packages/accelerate/accelerator.py:498: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnDutch.py:675: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("rouge")
  0%|          | 0/179460 [00:00<?, ?it/s]  0%|          | 1/179460 [00:06<327:25:17,  6.57s/it]  0%|          | 2/179460 [00:10<240:16:38,  4.82s/it]Traceback (most recent call last):
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnDutch.py", line 953, in <module>
    main()
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnDutch.py", line 738, in main
    teacher_outputs = teacher_model(**batch, output_attentions=True, output_hidden_states=True)
  File "/opt/sw/spack/apps/linux-rhel8-x86_64/gcc-10.3.0/python-3.10.1-qb/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sshukla7/.local/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 1340, in forward
    lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias
  File "/opt/sw/spack/apps/linux-rhel8-x86_64/gcc-10.3.0/python-3.10.1-qb/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/sw/spack/apps/linux-rhel8-x86_64/gcc-10.3.0/python-3.10.1-qb/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 79.35 GiB total capacity; 69.10 GiB already allocated; 1.24 GiB free; 76.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1).
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/sshukla7/sshukla7/DQ_BART/wandb/offline-run-20230501_193305-ffy4mz7d
wandb: Find logs at: ./wandb/offline-run-20230501_193305-ffy4mz7d/logs
