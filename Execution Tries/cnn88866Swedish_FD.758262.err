
Lmod is automatically replacing "gnu9/9.3.0" with "gnu10/10.3.0-ya".


Inactive Modules:
  1) hwloc/2.1.0     2) libfabric     3) openmpi4     4) ucx

2023-05-02 22:23:57.229453: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-02 22:23:59.663032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: Tracking run with wandb version 0.15.0
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Downloading metadata:   0%|          | 0.00/983 [00:00<?, ?B/s]Downloading metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 983/983 [00:00<00:00, 2.34MB/s]
Downloading readme:   0%|          | 0.00/1.33k [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.33k/1.33k [00:00<00:00, 2.15MB/s]
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading data:   0%|          | 0.00/258M [00:00<?, ?B/s][A
Downloading data:   1%|          | 2.49M/258M [00:00<00:10, 24.8MB/s][A
Downloading data:   5%|â–         | 12.7M/258M [00:00<00:03, 70.5MB/s][A
Downloading data:  10%|â–ˆ         | 26.2M/258M [00:00<00:02, 99.8MB/s][A
Downloading data:  15%|â–ˆâ–Œ        | 39.5M/258M [00:00<00:01, 113MB/s] [A
Downloading data:  21%|â–ˆâ–ˆ        | 53.0M/258M [00:00<00:01, 121MB/s][A
Downloading data:  26%|â–ˆâ–ˆâ–Œ       | 66.5M/258M [00:00<00:01, 125MB/s][A
Downloading data:  31%|â–ˆâ–ˆâ–ˆ       | 80.0M/258M [00:00<00:01, 129MB/s][A
Downloading data:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 93.6M/258M [00:00<00:01, 131MB/s][A
Downloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 107M/258M [00:00<00:01, 133MB/s] [A
Downloading data:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 121M/258M [00:01<00:01, 134MB/s][A
Downloading data:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 135M/258M [00:01<00:00, 137MB/s][A
Downloading data:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 149M/258M [00:01<00:00, 136MB/s][A
Downloading data:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 163M/258M [00:01<00:00, 136MB/s][A
Downloading data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 176M/258M [00:01<00:00, 136MB/s][A
Downloading data:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 190M/258M [00:01<00:00, 136MB/s][A
Downloading data:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 204M/258M [00:01<00:00, 135MB/s][A
Downloading data:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 217M/258M [00:01<00:00, 135MB/s][A
Downloading data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 231M/258M [00:01<00:00, 135MB/s][A
Downloading data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 244M/258M [00:01<00:00, 135MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258M/258M [00:02<00:00, 128MB/s]

Downloading data:   0%|          | 0.00/274M [00:00<?, ?B/s][A
Downloading data:   4%|â–         | 10.8M/274M [00:00<00:02, 108MB/s][A
Downloading data:   9%|â–‰         | 24.6M/274M [00:00<00:01, 126MB/s][A
Downloading data:  14%|â–ˆâ–        | 38.4M/274M [00:00<00:01, 131MB/s][A
Downloading data:  19%|â–ˆâ–‰        | 52.4M/274M [00:00<00:01, 135MB/s][A
Downloading data:  24%|â–ˆâ–ˆâ–       | 66.4M/274M [00:00<00:01, 136MB/s][A
Downloading data:  29%|â–ˆâ–ˆâ–‰       | 80.2M/274M [00:00<00:01, 137MB/s][A
Downloading data:  34%|â–ˆâ–ˆâ–ˆâ–      | 94.1M/274M [00:00<00:01, 138MB/s][A
Downloading data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 108M/274M [00:00<00:01, 138MB/s] [A
Downloading data:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 122M/274M [00:00<00:01, 139MB/s][A
Downloading data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 136M/274M [00:01<00:00, 140MB/s][A
Downloading data:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 150M/274M [00:01<00:00, 136MB/s][A
Downloading data:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 164M/274M [00:01<00:00, 129MB/s][A
Downloading data:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 177M/274M [00:01<00:00, 129MB/s][A
Downloading data:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 190M/274M [00:01<00:00, 131MB/s][A
Downloading data:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 204M/274M [00:01<00:00, 133MB/s][A
Downloading data:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 218M/274M [00:01<00:00, 135MB/s][A
Downloading data:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 231M/274M [00:01<00:00, 132MB/s][A
Downloading data:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 245M/274M [00:01<00:00, 128MB/s][A
Downloading data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 258M/274M [00:01<00:00, 130MB/s][A
Downloading data:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 272M/274M [00:02<00:00, 133MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 274M/274M [00:02<00:00, 133MB/s]

Downloading data:   0%|          | 0.00/271M [00:00<?, ?B/s][A
Downloading data:   4%|â–         | 11.7M/271M [00:00<00:02, 117MB/s][A
Downloading data:   9%|â–‰         | 25.7M/271M [00:00<00:01, 131MB/s][A
Downloading data:  15%|â–ˆâ–        | 39.7M/271M [00:00<00:01, 135MB/s][A
Downloading data:  20%|â–ˆâ–‰        | 53.8M/271M [00:00<00:01, 137MB/s][A
Downloading data:  25%|â–ˆâ–ˆâ–Œ       | 68.1M/271M [00:00<00:01, 139MB/s][A
Downloading data:  30%|â–ˆâ–ˆâ–ˆ       | 82.2M/271M [00:00<00:01, 140MB/s][A
Downloading data:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96.5M/271M [00:00<00:01, 141MB/s][A
Downloading data:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111M/271M [00:00<00:01, 141MB/s] [A
Downloading data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 125M/271M [00:00<00:01, 137MB/s][A
Downloading data:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 139M/271M [00:01<00:00, 137MB/s][A
Downloading data:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153M/271M [00:01<00:00, 138MB/s][A
Downloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 167M/271M [00:01<00:00, 135MB/s][A
Downloading data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 180M/271M [00:01<00:00, 131MB/s][A
Downloading data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 193M/271M [00:01<00:00, 130MB/s][A
Downloading data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 207M/271M [00:01<00:00, 133MB/s][A
Downloading data:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 221M/271M [00:01<00:00, 135MB/s][A
Downloading data:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 235M/271M [00:01<00:00, 137MB/s][A
Downloading data:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 250M/271M [00:01<00:00, 138MB/s][A
Downloading data:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 264M/271M [00:01<00:00, 139MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 271M/271M [00:01<00:00, 136MB/s]
Downloading data files:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:07<00:14,  7.13s/it]
Downloading data:   0%|          | 0.00/31.1M [00:00<?, ?B/s][A
Downloading data:  34%|â–ˆâ–ˆâ–ˆâ–      | 10.5M/31.1M [00:00<00:00, 105MB/s][A
Downloading data:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23.9M/31.1M [00:00<00:00, 122MB/s][ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.1M/31.1M [00:00<00:00, 122MB/s]
Downloading data files:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:03,  3.19s/it]
Downloading data:   0%|          | 0.00/36.0M [00:00<?, ?B/s][A
Downloading data:  13%|â–ˆâ–Ž        | 4.83M/36.0M [00:00<00:00, 48.3MB/s][A
Downloading data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 18.0M/36.0M [00:00<00:00, 97.0MB/s][A
Downloading data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31.7M/36.0M [00:00<00:00, 115MB/s] [ADownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36.0M/36.0M [00:00<00:00, 109MB/s]
Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  1.96s/it]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.69s/it]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 42.07it/s]
Generating train split:   0%|          | 0/287108 [00:00<?, ? examples/s]Generating train split:   3%|â–Ž         | 10000/287108 [00:00<00:05, 49215.06 examples/s]Generating train split:   7%|â–‹         | 20000/287108 [00:00<00:03, 67026.22 examples/s]Generating train split:  10%|â–ˆ         | 30000/287108 [00:00<00:03, 77124.55 examples/s]Generating train split:  14%|â–ˆâ–        | 40000/287108 [00:00<00:02, 83785.17 examples/s]Generating train split:  17%|â–ˆâ–‹        | 50000/287108 [00:00<00:02, 85075.83 examples/s]Generating train split:  21%|â–ˆâ–ˆ        | 60000/287108 [00:00<00:02, 85132.12 examples/s]Generating train split:  24%|â–ˆâ–ˆâ–       | 70000/287108 [00:00<00:02, 85092.75 examples/s]Generating train split:  28%|â–ˆâ–ˆâ–Š       | 80000/287108 [00:00<00:02, 83958.06 examples/s]Generating train split:  31%|â–ˆâ–ˆâ–ˆâ–      | 90000/287108 [00:01<00:02, 81641.17 examples/s]Generating train split:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 105703/287108 [00:01<00:02, 82117.76 examples/s]Generating train split:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 115703/287108 [00:01<00:02, 82712.13 examples/s]Generating train split:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 125703/287108 [00:01<00:03, 44164.37 examples/s]Generating train split:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 145703/287108 [00:02<00:02, 57708.69 examples/s]Generating train split:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 155703/287108 [00:02<00:02, 61491.15 examples/s]Generating train split:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 165703/287108 [00:02<00:01, 66015.05 examples/s]Generating train split:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 175703/287108 [00:02<00:01, 69433.62 examples/s]Generating train split:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 185703/287108 [00:02<00:01, 73163.67 examples/s]Generating train split:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 201406/287108 [00:02<00:01, 74068.00 examples/s]Generating train split:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 211406/287108 [00:02<00:00, 76122.13 examples/s]Generating train split:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 221406/287108 [00:03<00:00, 78838.50 examples/s]Generating train split:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 231406/287108 [00:03<00:00, 80181.04 examples/s]Generating train split:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 241406/287108 [00:03<00:01, 42643.85 examples/s]Generating train split:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 251406/287108 [00:03<00:00, 50533.01 examples/s]Generating train split:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 261406/287108 [00:03<00:00, 56900.33 examples/s]Generating train split:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 271406/287108 [00:04<00:00, 64107.37 examples/s]Generating train split:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 281406/287108 [00:04<00:00, 71380.93 examples/s]                                                                                         Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]Generating test split:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 10000/11490 [00:00<00:00, 85443.54 examples/s]                                                                                      Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]Generating validation split:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10000/13368 [00:00<00:00, 91463.86 examples/s]                                                                                              0%|          | 0/3 [00:00<?, ?it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  2.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.73it/s]
Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04k/1.04k [00:00<00:00, 7.15MB/s]
loading configuration file config.json from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/config.json
Model config BartConfig {
  "_name_or_path": "Gabriel/bart-base-cnn-xsum-swe",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "torch_dtype": "float32",
  "transformers_version": "4.23.0",
  "use_cache": true,
  "vocab_size": 50185
}

Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/463 [00:00<?, ?B/s]Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 463/463 [00:00<00:00, 3.83MB/s]
Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.23M [00:00<?, ?B/s]Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.23M/2.23M [00:00<00:00, 156MB/s]
Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:00<00:00, 673kB/s]
loading file tokenizer.json from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/tokenizer_config.json
Downloading pytorch_model.bin:   0%|          | 0.00/558M [00:00<?, ?B/s]Downloading pytorch_model.bin:   2%|â–         | 10.5M/558M [00:00<00:08, 67.5MB/s]Downloading pytorch_model.bin:   6%|â–Œ         | 31.5M/558M [00:00<00:04, 116MB/s] Downloading pytorch_model.bin:   9%|â–‰         | 52.4M/558M [00:00<00:06, 74.2MB/s]Downloading pytorch_model.bin:  13%|â–ˆâ–Ž        | 73.4M/558M [00:00<00:04, 100MB/s] Downloading pytorch_model.bin:  17%|â–ˆâ–‹        | 94.4M/558M [00:01<00:06, 67.3MB/s]Downloading pytorch_model.bin:  19%|â–ˆâ–‰        | 105M/558M [00:01<00:07, 60.3MB/s] Downloading pytorch_model.bin:  21%|â–ˆâ–ˆ        | 115M/558M [00:01<00:07, 61.2MB/s]Downloading pytorch_model.bin:  24%|â–ˆâ–ˆâ–       | 136M/558M [00:02<00:07, 58.7MB/s]Downloading pytorch_model.bin:  28%|â–ˆâ–ˆâ–Š       | 157M/558M [00:02<00:06, 60.4MB/s]Downloading pytorch_model.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 189M/558M [00:02<00:05, 69.1MB/s]Downloading pytorch_model.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 210M/558M [00:02<00:04, 73.2MB/s]Downloading pytorch_model.bin:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 220M/558M [00:03<00:04, 69.7MB/s]Downloading pytorch_model.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 241M/558M [00:03<00:04, 68.4MB/s]Downloading pytorch_model.bin:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 262M/558M [00:03<00:04, 72.2MB/s]Downloading pytorch_model.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 273M/558M [00:03<00:03, 75.3MB/s]Downloading pytorch_model.bin:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 294M/558M [00:04<00:03, 74.3MB/s]Downloading pytorch_model.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 315M/558M [00:04<00:03, 67.6MB/s]Downloading pytorch_model.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 325M/558M [00:04<00:03, 66.1MB/s]Downloading pytorch_model.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 346M/558M [00:05<00:03, 60.0MB/s]Downloading pytorch_model.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 367M/558M [00:05<00:03, 57.3MB/s]Downloading pytorch_model.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 398M/558M [00:05<00:02, 65.8MB/s]Downloading pytorch_model.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 419M/558M [00:06<00:01, 72.9MB/s]Downloading pytorch_model.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 440M/558M [00:06<00:01, 88.9MB/s]Downloading pytorch_model.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 461M/558M [00:06<00:01, 59.2MB/s]Downloading pytorch_model.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 472M/558M [00:07<00:01, 55.0MB/s]Downloading pytorch_model.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 493M/558M [00:07<00:00, 71.9MB/s]Downloading pytorch_model.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 514M/558M [00:07<00:00, 80.0MB/s]Downloading pytorch_model.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 535M/558M [00:07<00:00, 76.6MB/s]Downloading pytorch_model.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 545M/558M [00:07<00:00, 80.3MB/s]Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 556M/558M [00:08<00:00, 66.3MB/s]Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558M/558M [00:08<00:00, 68.9MB/s]
loading weights file pytorch_model.bin from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/pytorch_model.bin
All model checkpoint weights were used when initializing BartForConditionalGeneration.

All the weights of BartForConditionalGeneration were initialized from the model checkpoint at Gabriel/bart-base-cnn-xsum-swe.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
/home/sshukla7/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:649: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'
  warnings.warn(
Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04k/1.04k [00:00<00:00, 4.94MB/s]
Running tokenizer on dataset (num_proc=10):   0%|          | 0/287108 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   0%|          | 1000/287108 [00:03<14:23, 331.43 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   1%|          | 3000/287108 [00:03<04:07, 1148.54 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   1%|â–         | 4000/287108 [00:03<03:11, 1479.83 examples/s]Running tokenizer on dataset (num_proc=10):   2%|â–         | 5000/287108 [00:03<02:19, 2025.61 examples/s]Running tokenizer on dataset (num_proc=10):   3%|â–Ž         | 8000/287108 [00:03<01:08, 4085.45 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   3%|â–Ž         | 9000/287108 [00:04<01:47, 2580.87 examples/s]Running tokenizer on dataset (num_proc=10):   3%|â–Ž         | 10000/287108 [00:05<01:55, 2389.57 examples/s]Running tokenizer on dataset (num_proc=10):   4%|â–         | 11000/287108 [00:05<02:09, 2128.28 examples/s]Running tokenizer on dataset (num_proc=10):   5%|â–         | 13000/287108 [00:06<01:27, 3116.36 examples/s]Running tokenizer on dataset (num_proc=10):   5%|â–         | 14000/287108 [00:07<02:00, 2271.33 examples/s]Running tokenizer on dataset (num_proc=10):   5%|â–Œ         | 15000/287108 [00:07<01:37, 2799.89 examples/s]Running tokenizer on dataset (num_proc=10):   6%|â–‹         | 18000/287108 [00:08<01:42, 2620.48 examples/s]Running tokenizer on dataset (num_proc=10):   7%|â–‹         | 19000/287108 [00:08<01:37, 2746.72 examples/s]Running tokenizer on dataset (num_proc=10):   7%|â–‹         | 20000/287108 [00:08<01:24, 3156.63 examples/s]Running tokenizer on dataset (num_proc=10):   7%|â–‹         | 21000/287108 [00:09<01:29, 2970.51 examples/s]Running tokenizer on dataset (num_proc=10):   8%|â–Š         | 22000/287108 [00:09<01:37, 2724.42 examples/s]Running tokenizer on dataset (num_proc=10):   8%|â–Š         | 23000/287108 [00:10<02:00, 2190.44 examples/s]Running tokenizer on dataset (num_proc=10):   8%|â–Š         | 24000/287108 [00:10<01:34, 2779.88 examples/s]Running tokenizer on dataset (num_proc=10):   9%|â–‰         | 27000/287108 [00:10<00:50, 5146.76 examples/s]Running tokenizer on dataset (num_proc=10):  10%|â–‰         | 28000/287108 [00:11<01:19, 3249.24 examples/s]Running tokenizer on dataset (num_proc=10):  10%|â–ˆ         | 29000/287108 [00:11<01:17, 3311.86 examples/s]Running tokenizer on dataset (num_proc=10):  11%|â–ˆ         | 31000/287108 [00:11<01:05, 3932.31 examples/s]Running tokenizer on dataset (num_proc=10):  11%|â–ˆ         | 32000/287108 [00:13<02:21, 1808.68 examples/s]Running tokenizer on dataset (num_proc=10):  11%|â–ˆâ–        | 33000/287108 [00:13<01:55, 2202.69 examples/s]Running tokenizer on dataset (num_proc=10):  12%|â–ˆâ–        | 35000/287108 [00:13<01:15, 3358.19 examples/s]Running tokenizer on dataset (num_proc=10):  13%|â–ˆâ–Ž        | 37000/287108 [00:14<00:54, 4592.45 examples/s]Running tokenizer on dataset (num_proc=10):  13%|â–ˆâ–Ž        | 38000/287108 [00:14<00:55, 4467.23 examples/s]Running tokenizer on dataset (num_proc=10):  14%|â–ˆâ–Ž        | 39000/287108 [00:14<00:58, 4273.50 examples/s]Running tokenizer on dataset (num_proc=10):  14%|â–ˆâ–        | 40000/287108 [00:14<01:03, 3904.70 examples/s]Running tokenizer on dataset (num_proc=10):  14%|â–ˆâ–        | 41000/287108 [00:15<00:58, 4238.11 examples/s]Running tokenizer on dataset (num_proc=10):  15%|â–ˆâ–        | 42000/287108 [00:16<02:46, 1474.89 examples/s]Running tokenizer on dataset (num_proc=10):  15%|â–ˆâ–        | 43000/287108 [00:17<02:09, 1889.64 examples/s]Running tokenizer on dataset (num_proc=10):  16%|â–ˆâ–Œ        | 45000/287108 [00:17<01:17, 3136.18 examples/s]Running tokenizer on dataset (num_proc=10):  17%|â–ˆâ–‹        | 48000/287108 [00:17<00:47, 5007.70 examples/s]Running tokenizer on dataset (num_proc=10):  17%|â–ˆâ–‹        | 50000/287108 [00:18<00:55, 4269.06 examples/s]Running tokenizer on dataset (num_proc=10):  18%|â–ˆâ–Š        | 52000/287108 [00:20<01:59, 1972.63 examples/s]Running tokenizer on dataset (num_proc=10):  19%|â–ˆâ–‰        | 54000/287108 [00:20<01:27, 2671.37 examples/s]Running tokenizer on dataset (num_proc=10):  20%|â–ˆâ–‰        | 57000/287108 [00:20<00:54, 4186.11 examples/s]Running tokenizer on dataset (num_proc=10):  21%|â–ˆâ–ˆ        | 59000/287108 [00:20<00:44, 5086.11 examples/s]Running tokenizer on dataset (num_proc=10):  21%|â–ˆâ–ˆ        | 61000/287108 [00:21<00:49, 4594.46 examples/s]Running tokenizer on dataset (num_proc=10):  22%|â–ˆâ–ˆâ–       | 63000/287108 [00:23<01:44, 2141.28 examples/s]Running tokenizer on dataset (num_proc=10):  23%|â–ˆâ–ˆâ–Ž       | 65000/287108 [00:23<01:18, 2841.71 examples/s]Running tokenizer on dataset (num_proc=10):  23%|â–ˆâ–ˆâ–Ž       | 66000/287108 [00:23<01:16, 2908.87 examples/s]Running tokenizer on dataset (num_proc=10):  24%|â–ˆâ–ˆâ–       | 69000/287108 [00:23<00:48, 4530.48 examples/s]Running tokenizer on dataset (num_proc=10):  25%|â–ˆâ–ˆâ–       | 71000/287108 [00:24<00:40, 5276.25 examples/s]Running tokenizer on dataset (num_proc=10):  25%|â–ˆâ–ˆâ–Œ       | 73000/287108 [00:26<01:38, 2180.97 examples/s]Running tokenizer on dataset (num_proc=10):  26%|â–ˆâ–ˆâ–Œ       | 74000/287108 [00:26<01:24, 2513.29 examples/s]Running tokenizer on dataset (num_proc=10):  26%|â–ˆâ–ˆâ–Œ       | 75000/287108 [00:26<01:15, 2795.76 examples/s]Running tokenizer on dataset (num_proc=10):  26%|â–ˆâ–ˆâ–‹       | 76000/287108 [00:26<01:09, 3019.17 examples/s]Running tokenizer on dataset (num_proc=10):  27%|â–ˆâ–ˆâ–‹       | 77000/287108 [00:27<00:59, 3528.24 examples/s]Running tokenizer on dataset (num_proc=10):  27%|â–ˆâ–ˆâ–‹       | 78000/287108 [00:27<00:49, 4215.70 examples/s]Running tokenizer on dataset (num_proc=10):  28%|â–ˆâ–ˆâ–Š       | 81000/287108 [00:27<00:36, 5577.40 examples/s]Running tokenizer on dataset (num_proc=10):  29%|â–ˆâ–ˆâ–Š       | 82000/287108 [00:28<01:24, 2440.84 examples/s]Running tokenizer on dataset (num_proc=10):  29%|â–ˆâ–ˆâ–‰       | 83000/287108 [00:29<01:32, 2212.74 examples/s]Running tokenizer on dataset (num_proc=10):  29%|â–ˆâ–ˆâ–‰       | 84000/287108 [00:29<01:17, 2626.73 examples/s]Running tokenizer on dataset (num_proc=10):  30%|â–ˆâ–ˆâ–‰       | 85000/287108 [00:29<01:07, 2989.61 examples/s]Running tokenizer on dataset (num_proc=10):  30%|â–ˆâ–ˆâ–ˆ       | 87000/287108 [00:30<01:02, 3216.78 examples/s]Running tokenizer on dataset (num_proc=10):  31%|â–ˆâ–ˆâ–ˆ       | 89000/287108 [00:30<00:44, 4473.13 examples/s]Running tokenizer on dataset (num_proc=10):  32%|â–ˆâ–ˆâ–ˆâ–      | 91000/287108 [00:30<00:42, 4619.44 examples/s]Running tokenizer on dataset (num_proc=10):  32%|â–ˆâ–ˆâ–ˆâ–      | 92000/287108 [00:31<01:03, 3086.97 examples/s]Running tokenizer on dataset (num_proc=10):  32%|â–ˆâ–ˆâ–ˆâ–      | 93000/287108 [00:32<01:28, 2196.39 examples/s]Running tokenizer on dataset (num_proc=10):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 95000/287108 [00:33<01:06, 2894.53 examples/s]Running tokenizer on dataset (num_proc=10):  34%|â–ˆâ–ˆâ–ˆâ–      | 97000/287108 [00:33<01:03, 3003.77 examples/s]Running tokenizer on dataset (num_proc=10):  34%|â–ˆâ–ˆâ–ˆâ–      | 98000/287108 [00:33<00:55, 3437.81 examples/s]Running tokenizer on dataset (num_proc=10):  34%|â–ˆâ–ˆâ–ˆâ–      | 99000/287108 [00:33<00:52, 3571.03 examples/s]Running tokenizer on dataset (num_proc=10):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 101000/287108 [00:34<00:49, 3783.66 examples/s]Running tokenizer on dataset (num_proc=10):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 102000/287108 [00:34<00:52, 3500.26 examples/s]Running tokenizer on dataset (num_proc=10):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 103000/287108 [00:35<01:22, 2218.40 examples/s]Running tokenizer on dataset (num_proc=10):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 104000/287108 [00:36<01:12, 2512.83 examples/s]Running tokenizer on dataset (num_proc=10):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 105000/287108 [00:36<00:59, 3060.95 examples/s]Running tokenizer on dataset (num_proc=10):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 106000/287108 [00:36<00:53, 3398.93 examples/s]Running tokenizer on dataset (num_proc=10):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 107000/287108 [00:36<00:51, 3467.45 examples/s]Running tokenizer on dataset (num_proc=10):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 108000/287108 [00:37<01:03, 2819.03 examples/s]Running tokenizer on dataset (num_proc=10):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 110000/287108 [00:37<00:46, 3776.72 examples/s]Running tokenizer on dataset (num_proc=10):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 112000/287108 [00:37<00:39, 4418.06 examples/s]Running tokenizer on dataset (num_proc=10):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 113000/287108 [00:39<01:19, 2185.52 examples/s]Running tokenizer on dataset (num_proc=10):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 114000/287108 [00:39<01:08, 2532.65 examples/s]Running tokenizer on dataset (num_proc=10):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 116000/287108 [00:39<00:47, 3640.59 examples/s]Running tokenizer on dataset (num_proc=10):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 117000/287108 [00:39<00:52, 3242.89 examples/s]Running tokenizer on dataset (num_proc=10):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119000/287108 [00:40<00:56, 2959.93 examples/s]Running tokenizer on dataset (num_proc=10):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120000/287108 [00:40<00:47, 3486.28 examples/s]Running tokenizer on dataset (num_proc=10):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121000/287108 [00:41<00:45, 3633.79 examples/s]Running tokenizer on dataset (num_proc=10):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 122000/287108 [00:41<00:51, 3185.60 examples/s]Running tokenizer on dataset (num_proc=10):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 123000/287108 [00:42<01:04, 2534.48 examples/s]Running tokenizer on dataset (num_proc=10):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 124000/287108 [00:42<00:58, 2781.96 examples/s]Running tokenizer on dataset (num_proc=10):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 126000/287108 [00:42<00:43, 3679.69 examples/s]Running tokenizer on dataset (num_proc=10):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 127000/287108 [00:42<00:42, 3738.65 examples/s]Running tokenizer on dataset (num_proc=10):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 129000/287108 [00:43<00:59, 2669.18 examples/s]Running tokenizer on dataset (num_proc=10):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 131000/287108 [00:44<00:48, 3224.40 examples/s]Running tokenizer on dataset (num_proc=10):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 133000/287108 [00:45<00:50, 3040.58 examples/s]Running tokenizer on dataset (num_proc=10):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 135000/287108 [00:45<00:38, 3979.15 examples/s]Running tokenizer on dataset (num_proc=10):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 136000/287108 [00:45<00:46, 3261.49 examples/s]Running tokenizer on dataset (num_proc=10):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 137000/287108 [00:46<00:41, 3618.65 examples/s]Running tokenizer on dataset (num_proc=10):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 138000/287108 [00:46<00:40, 3720.00 examples/s]Running tokenizer on dataset (num_proc=10):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 139000/287108 [00:47<01:00, 2454.79 examples/s]Running tokenizer on dataset (num_proc=10):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 140000/287108 [00:47<00:49, 2992.09 examples/s]Running tokenizer on dataset (num_proc=10):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 142000/287108 [00:47<00:38, 3747.31 examples/s]Running tokenizer on dataset (num_proc=10):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 143000/287108 [00:47<00:34, 4217.01 examples/s]Running tokenizer on dataset (num_proc=10):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 144000/287108 [00:48<00:44, 3251.14 examples/s]Running tokenizer on dataset (num_proc=10):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 146000/287108 [00:49<00:49, 2854.92 examples/s]Running tokenizer on dataset (num_proc=10):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148000/287108 [00:49<00:42, 3263.86 examples/s]Running tokenizer on dataset (num_proc=10):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 149000/287108 [00:49<00:46, 2968.15 examples/s]Running tokenizer on dataset (num_proc=10):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 150000/287108 [00:50<00:47, 2912.23 examples/s]Running tokenizer on dataset (num_proc=10):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 151000/287108 [00:50<00:39, 3458.74 examples/s]Running tokenizer on dataset (num_proc=10):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 152000/287108 [00:50<00:33, 4037.99 examples/s]Running tokenizer on dataset (num_proc=10):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 153000/287108 [00:50<00:33, 3967.66 examples/s]Running tokenizer on dataset (num_proc=10):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 154000/287108 [00:51<00:37, 3577.52 examples/s]Running tokenizer on dataset (num_proc=10):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156000/287108 [00:52<00:50, 2606.02 examples/s]Running tokenizer on dataset (num_proc=10):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 157000/287108 [00:52<00:42, 3084.63 examples/s]Running tokenizer on dataset (num_proc=10):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 158000/287108 [00:52<00:43, 2998.67 examples/s]Running tokenizer on dataset (num_proc=10):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 160000/287108 [00:52<00:31, 4016.89 examples/s]Running tokenizer on dataset (num_proc=10):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 161000/287108 [00:53<00:48, 2604.00 examples/s]Running tokenizer on dataset (num_proc=10):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 163000/287108 [00:54<00:34, 3631.84 examples/s]Running tokenizer on dataset (num_proc=10):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 164000/287108 [00:54<00:31, 3911.00 examples/s]Running tokenizer on dataset (num_proc=10):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 166000/287108 [00:55<00:42, 2819.74 examples/s]Running tokenizer on dataset (num_proc=10):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 167000/287108 [00:55<00:41, 2881.52 examples/s]Running tokenizer on dataset (num_proc=10):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 170000/287108 [00:55<00:27, 4217.67 examples/s]Running tokenizer on dataset (num_proc=10):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 171000/287108 [00:57<00:46, 2493.90 examples/s]Running tokenizer on dataset (num_proc=10):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 172000/287108 [00:57<00:39, 2913.60 examples/s]Running tokenizer on dataset (num_proc=10):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 174000/287108 [00:57<00:27, 4079.00 examples/s]Running tokenizer on dataset (num_proc=10):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 176000/287108 [00:58<00:38, 2859.54 examples/s]Running tokenizer on dataset (num_proc=10):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179000/287108 [00:58<00:28, 3811.37 examples/s]Running tokenizer on dataset (num_proc=10):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 180000/287108 [00:59<00:28, 3774.74 examples/s]Running tokenizer on dataset (num_proc=10):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 181000/287108 [00:59<00:39, 2654.99 examples/s]Running tokenizer on dataset (num_proc=10):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 182000/287108 [01:00<00:38, 2738.04 examples/s]Running tokenizer on dataset (num_proc=10):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 183000/287108 [01:00<00:32, 3184.24 examples/s]Running tokenizer on dataset (num_proc=10):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 184000/287108 [01:00<00:27, 3816.74 examples/s]Running tokenizer on dataset (num_proc=10):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 186000/287108 [01:01<00:30, 3305.16 examples/s]Running tokenizer on dataset (num_proc=10):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 187000/287108 [01:01<00:29, 3383.13 examples/s]Running tokenizer on dataset (num_proc=10):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 189000/287108 [01:02<00:30, 3238.46 examples/s]Running tokenizer on dataset (num_proc=10):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 191000/287108 [01:03<00:33, 2887.51 examples/s]Running tokenizer on dataset (num_proc=10):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 192000/287108 [01:03<00:34, 2746.48 examples/s]Running tokenizer on dataset (num_proc=10):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 193000/287108 [01:03<00:29, 3139.58 examples/s]Running tokenizer on dataset (num_proc=10):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 194000/287108 [01:03<00:27, 3426.01 examples/s]Running tokenizer on dataset (num_proc=10):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 196000/287108 [01:04<00:20, 4381.42 examples/s]Running tokenizer on dataset (num_proc=10):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 197000/287108 [01:04<00:22, 4039.31 examples/s]Running tokenizer on dataset (num_proc=10):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 198000/287108 [01:04<00:21, 4077.66 examples/s]Running tokenizer on dataset (num_proc=10):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 199000/287108 [01:05<00:34, 2585.18 examples/s]Running tokenizer on dataset (num_proc=10):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 200000/287108 [01:05<00:27, 3157.33 examples/s]Running tokenizer on dataset (num_proc=10):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 201000/287108 [01:06<00:34, 2518.32 examples/s]Running tokenizer on dataset (num_proc=10):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 202000/287108 [01:06<00:30, 2773.47 examples/s]Running tokenizer on dataset (num_proc=10):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 203000/287108 [01:06<00:33, 2537.68 examples/s]Running tokenizer on dataset (num_proc=10):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 204000/287108 [01:07<00:26, 3152.43 examples/s]Running tokenizer on dataset (num_proc=10):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 206000/287108 [01:07<00:17, 4750.84 examples/s]Running tokenizer on dataset (num_proc=10):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 207000/287108 [01:07<00:15, 5102.69 examples/s]Running tokenizer on dataset (num_proc=10):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 208000/287108 [01:07<00:21, 3678.60 examples/s]Running tokenizer on dataset (num_proc=10):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 209000/287108 [01:08<00:30, 2587.46 examples/s]Running tokenizer on dataset (num_proc=10):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 210000/287108 [01:08<00:28, 2691.86 examples/s]Running tokenizer on dataset (num_proc=10):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 211000/287108 [01:09<00:29, 2604.26 examples/s]Running tokenizer on dataset (num_proc=10):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 212000/287108 [01:09<00:23, 3214.97 examples/s]Running tokenizer on dataset (num_proc=10):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 213000/287108 [01:10<00:30, 2415.68 examples/s]Running tokenizer on dataset (num_proc=10):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 215000/287108 [01:10<00:19, 3723.36 examples/s]Running tokenizer on dataset (num_proc=10):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 216000/287108 [01:10<00:16, 4243.03 examples/s]Running tokenizer on dataset (num_proc=10):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 217000/287108 [01:10<00:14, 4683.42 examples/s]Running tokenizer on dataset (num_proc=10):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 218000/287108 [01:11<00:19, 3536.25 examples/s]Running tokenizer on dataset (num_proc=10):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 219000/287108 [01:11<00:28, 2406.93 examples/s]Running tokenizer on dataset (num_proc=10):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 220000/287108 [01:12<00:26, 2499.53 examples/s]Running tokenizer on dataset (num_proc=10):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 221000/287108 [01:12<00:22, 2968.08 examples/s]Running tokenizer on dataset (num_proc=10):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 223000/287108 [01:13<00:21, 2988.76 examples/s]Running tokenizer on dataset (num_proc=10):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 224000/287108 [01:13<00:19, 3266.86 examples/s]Running tokenizer on dataset (num_proc=10):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 225000/287108 [01:13<00:18, 3380.10 examples/s]Running tokenizer on dataset (num_proc=10):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 227000/287108 [01:13<00:15, 3877.69 examples/s]Running tokenizer on dataset (num_proc=10):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 228000/287108 [01:14<00:15, 3745.27 examples/s]Running tokenizer on dataset (num_proc=10):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 229000/287108 [01:15<00:23, 2503.26 examples/s]Running tokenizer on dataset (num_proc=10):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 230000/287108 [01:15<00:20, 2799.33 examples/s]Running tokenizer on dataset (num_proc=10):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 231000/287108 [01:15<00:17, 3186.96 examples/s]Running tokenizer on dataset (num_proc=10):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 232000/287108 [01:15<00:16, 3322.09 examples/s]Running tokenizer on dataset (num_proc=10):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 233000/287108 [01:15<00:14, 3643.94 examples/s]Running tokenizer on dataset (num_proc=10):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 234000/287108 [01:16<00:14, 3783.58 examples/s]Running tokenizer on dataset (num_proc=10):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 235000/287108 [01:16<00:13, 3970.33 examples/s]Running tokenizer on dataset (num_proc=10):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 236000/287108 [01:16<00:14, 3421.34 examples/s]Running tokenizer on dataset (num_proc=10):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 237000/287108 [01:17<00:16, 3050.75 examples/s]Running tokenizer on dataset (num_proc=10):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 238000/287108 [01:17<00:13, 3636.99 examples/s]Running tokenizer on dataset (num_proc=10):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 239000/287108 [01:18<00:21, 2281.82 examples/s]Running tokenizer on dataset (num_proc=10):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 240000/287108 [01:18<00:17, 2709.32 examples/s]Running tokenizer on dataset (num_proc=10):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 241000/287108 [01:18<00:17, 2703.07 examples/s]Running tokenizer on dataset (num_proc=10):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 242000/287108 [01:18<00:14, 3218.11 examples/s]Running tokenizer on dataset (num_proc=10):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 243000/287108 [01:19<00:11, 3892.32 examples/s]Running tokenizer on dataset (num_proc=10):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 244000/287108 [01:19<00:09, 4507.76 examples/s]Running tokenizer on dataset (num_proc=10):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 245000/287108 [01:19<00:10, 4188.81 examples/s]Running tokenizer on dataset (num_proc=10):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 246000/287108 [01:20<00:14, 2915.50 examples/s]Running tokenizer on dataset (num_proc=10):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 247000/287108 [01:20<00:16, 2374.12 examples/s]Running tokenizer on dataset (num_proc=10):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 248000/287108 [01:20<00:13, 2997.17 examples/s]Running tokenizer on dataset (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 248711/287108 [01:21<00:13, 2821.24 examples/s]Running tokenizer on dataset (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 249711/287108 [01:21<00:12, 2933.49 examples/s]Running tokenizer on dataset (num_proc=10):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 250711/287108 [01:21<00:10, 3486.67 examples/s]Running tokenizer on dataset (num_proc=10):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 252711/287108 [01:21<00:08, 3996.35 examples/s]Running tokenizer on dataset (num_proc=10):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 254711/287108 [01:22<00:10, 2984.19 examples/s]Running tokenizer on dataset (num_proc=10):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 255711/287108 [01:23<00:13, 2371.17 examples/s]Running tokenizer on dataset (num_proc=10):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 256711/287108 [01:23<00:11, 2605.23 examples/s]Running tokenizer on dataset (num_proc=10):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 257711/287108 [01:23<00:09, 3176.60 examples/s]Running tokenizer on dataset (num_proc=10):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 258711/287108 [01:24<00:11, 2514.76 examples/s]Running tokenizer on dataset (num_proc=10):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 259711/287108 [01:24<00:10, 2558.99 examples/s]Running tokenizer on dataset (num_proc=10):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 262711/287108 [01:25<00:04, 5053.09 examples/s]Running tokenizer on dataset (num_proc=10):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 264711/287108 [01:26<00:10, 2237.69 examples/s]Running tokenizer on dataset (num_proc=10):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 266422/287108 [01:27<00:07, 2738.16 examples/s]Running tokenizer on dataset (num_proc=10):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 268422/287108 [01:27<00:06, 3050.69 examples/s]Running tokenizer on dataset (num_proc=10):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 269422/287108 [01:28<00:05, 3042.73 examples/s]Running tokenizer on dataset (num_proc=10):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 270422/287108 [01:28<00:04, 3377.03 examples/s]Running tokenizer on dataset (num_proc=10):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 272422/287108 [01:29<00:06, 2385.66 examples/s]Running tokenizer on dataset (num_proc=10):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 273132/287108 [01:29<00:05, 2361.11 examples/s]Running tokenizer on dataset (num_proc=10):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 273843/287108 [01:30<00:06, 2195.74 examples/s]Running tokenizer on dataset (num_proc=10):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 275843/287108 [01:30<00:03, 2996.30 examples/s]Running tokenizer on dataset (num_proc=10):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 277843/287108 [01:31<00:03, 2529.68 examples/s]Running tokenizer on dataset (num_proc=10):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 279843/287108 [01:32<00:03, 2199.17 examples/s]Running tokenizer on dataset (num_proc=10):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 280554/287108 [01:32<00:02, 2372.78 examples/s]Running tokenizer on dataset (num_proc=10):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 281265/287108 [01:32<00:02, 2669.07 examples/s]Running tokenizer on dataset (num_proc=10):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 282265/287108 [01:33<00:02, 2305.58 examples/s]Running tokenizer on dataset (num_proc=10):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 283265/287108 [01:34<00:02, 1474.18 examples/s]Running tokenizer on dataset (num_proc=10):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 284265/287108 [01:35<00:01, 1901.62 examples/s]Running tokenizer on dataset (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 285687/287108 [01:35<00:00, 1804.29 examples/s]Running tokenizer on dataset (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 286398/287108 [01:37<00:00, 1148.87 examples/s]Running tokenizer on dataset (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287108/287108 [01:37<00:00, 1405.60 examples/s]                                                                                                            Running tokenizer on dataset (num_proc=10):   0%|          | 0/11490 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   9%|â–Š         | 1000/11490 [00:02<00:28, 369.77 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  17%|â–ˆâ–‹        | 2000/11490 [00:02<00:11, 808.80 examples/s]Running tokenizer on dataset (num_proc=10):  27%|â–ˆâ–ˆâ–‹       | 3149/11490 [00:03<00:05, 1420.12 examples/s]Running tokenizer on dataset (num_proc=10):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 4298/11490 [00:03<00:03, 2058.97 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 5596/11490 [00:03<00:02, 2054.31 examples/s]Running tokenizer on dataset (num_proc=10):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 7596/11490 [00:04<00:01, 3387.40 examples/s]Running tokenizer on dataset (num_proc=10):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 9596/11490 [00:04<00:00, 5014.89 examples/s]Running tokenizer on dataset (num_proc=10):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 10745/11490 [00:04<00:00, 4917.78 examples/s]                                                                                                          Running tokenizer on dataset (num_proc=10):   0%|          | 0/13368 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   7%|â–‹         | 1000/13368 [00:02<00:32, 376.96 examples/s]Running tokenizer on dataset (num_proc=10):  22%|â–ˆâ–ˆâ–       | 3000/13368 [00:02<00:08, 1258.94 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  32%|â–ˆâ–ˆâ–ˆâ–      | 4337/13368 [00:03<00:05, 1562.64 examples/s]Running tokenizer on dataset (num_proc=10):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5674/13368 [00:03<00:03, 2132.99 examples/s]Running tokenizer on dataset (num_proc=10):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 9674/13368 [00:03<00:00, 4903.96 examples/s]Running tokenizer on dataset (num_proc=10):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11348/13368 [00:04<00:00, 3767.78 examples/s]Running tokenizer on dataset (num_proc=10):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12359/13368 [00:04<00:00, 3505.34 examples/s]Running tokenizer on dataset (num_proc=10): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13368/13368 [00:05<00:00, 4015.77 examples/s]                                                                                                          /home/sshukla7/.local/lib/python3.10/site-packages/accelerate/accelerator.py:498: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnSwedish.py:683: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("rouge")
  0%|          | 0/179460 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnSwedish.py", line 961, in <module>
    main()
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnSwedish.py", line 742, in main
    student_outputs = student_model(**batch, output_attentions=True, output_hidden_states=True)
  File "/opt/sw/spack/apps/linux-rhel8-x86_64/gcc-10.3.0/python-3.10.1-qb/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartForConditionalGeneration.forward() got an unexpected keyword argument 'token_type_ids'
wandb: Waiting for W&B process to finish... (failed 1).
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/sshukla7/sshukla7/DQ_BART/wandb/offline-run-20230502_222406-1bebglno
wandb: Find logs at: ./wandb/offline-run-20230502_222406-1bebglno/logs
