
Lmod is automatically replacing "gnu9/9.3.0" with "gnu10/10.3.0-ya".


Inactive Modules:
  1) hwloc/2.1.0     2) libfabric     3) openmpi4     4) ucx

2023-05-02 22:23:57.229453: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-02 22:23:59.663032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: Tracking run with wandb version 0.15.0
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Downloading metadata:   0%|          | 0.00/983 [00:00<?, ?B/s]Downloading metadata: 100%|██████████| 983/983 [00:00<00:00, 2.34MB/s]
Downloading readme:   0%|          | 0.00/1.33k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 1.33k/1.33k [00:00<00:00, 2.15MB/s]
Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading data:   0%|          | 0.00/258M [00:00<?, ?B/s][A
Downloading data:   1%|          | 2.49M/258M [00:00<00:10, 24.8MB/s][A
Downloading data:   5%|▍         | 12.7M/258M [00:00<00:03, 70.5MB/s][A
Downloading data:  10%|█         | 26.2M/258M [00:00<00:02, 99.8MB/s][A
Downloading data:  15%|█▌        | 39.5M/258M [00:00<00:01, 113MB/s] [A
Downloading data:  21%|██        | 53.0M/258M [00:00<00:01, 121MB/s][A
Downloading data:  26%|██▌       | 66.5M/258M [00:00<00:01, 125MB/s][A
Downloading data:  31%|███       | 80.0M/258M [00:00<00:01, 129MB/s][A
Downloading data:  36%|███▋      | 93.6M/258M [00:00<00:01, 131MB/s][A
Downloading data:  42%|████▏     | 107M/258M [00:00<00:01, 133MB/s] [A
Downloading data:  47%|████▋     | 121M/258M [00:01<00:01, 134MB/s][A
Downloading data:  53%|█████▎    | 135M/258M [00:01<00:00, 137MB/s][A
Downloading data:  58%|█████▊    | 149M/258M [00:01<00:00, 136MB/s][A
Downloading data:  63%|██████▎   | 163M/258M [00:01<00:00, 136MB/s][A
Downloading data:  68%|██████▊   | 176M/258M [00:01<00:00, 136MB/s][A
Downloading data:  74%|███████▍  | 190M/258M [00:01<00:00, 136MB/s][A
Downloading data:  79%|███████▉  | 204M/258M [00:01<00:00, 135MB/s][A
Downloading data:  84%|████████▍ | 217M/258M [00:01<00:00, 135MB/s][A
Downloading data:  90%|████████▉ | 231M/258M [00:01<00:00, 135MB/s][A
Downloading data:  95%|█████████▍| 244M/258M [00:01<00:00, 135MB/s][ADownloading data: 100%|██████████| 258M/258M [00:02<00:00, 128MB/s]

Downloading data:   0%|          | 0.00/274M [00:00<?, ?B/s][A
Downloading data:   4%|▍         | 10.8M/274M [00:00<00:02, 108MB/s][A
Downloading data:   9%|▉         | 24.6M/274M [00:00<00:01, 126MB/s][A
Downloading data:  14%|█▍        | 38.4M/274M [00:00<00:01, 131MB/s][A
Downloading data:  19%|█▉        | 52.4M/274M [00:00<00:01, 135MB/s][A
Downloading data:  24%|██▍       | 66.4M/274M [00:00<00:01, 136MB/s][A
Downloading data:  29%|██▉       | 80.2M/274M [00:00<00:01, 137MB/s][A
Downloading data:  34%|███▍      | 94.1M/274M [00:00<00:01, 138MB/s][A
Downloading data:  39%|███▉      | 108M/274M [00:00<00:01, 138MB/s] [A
Downloading data:  45%|████▍     | 122M/274M [00:00<00:01, 139MB/s][A
Downloading data:  50%|████▉     | 136M/274M [00:01<00:00, 140MB/s][A
Downloading data:  55%|█████▍    | 150M/274M [00:01<00:00, 136MB/s][A
Downloading data:  60%|█████▉    | 164M/274M [00:01<00:00, 129MB/s][A
Downloading data:  64%|██████▍   | 177M/274M [00:01<00:00, 129MB/s][A
Downloading data:  69%|██████▉   | 190M/274M [00:01<00:00, 131MB/s][A
Downloading data:  74%|███████▍  | 204M/274M [00:01<00:00, 133MB/s][A
Downloading data:  79%|███████▉  | 218M/274M [00:01<00:00, 135MB/s][A
Downloading data:  84%|████████▍ | 231M/274M [00:01<00:00, 132MB/s][A
Downloading data:  89%|████████▉ | 245M/274M [00:01<00:00, 128MB/s][A
Downloading data:  94%|█████████▍| 258M/274M [00:01<00:00, 130MB/s][A
Downloading data:  99%|█████████▉| 272M/274M [00:02<00:00, 133MB/s][ADownloading data: 100%|██████████| 274M/274M [00:02<00:00, 133MB/s]

Downloading data:   0%|          | 0.00/271M [00:00<?, ?B/s][A
Downloading data:   4%|▍         | 11.7M/271M [00:00<00:02, 117MB/s][A
Downloading data:   9%|▉         | 25.7M/271M [00:00<00:01, 131MB/s][A
Downloading data:  15%|█▍        | 39.7M/271M [00:00<00:01, 135MB/s][A
Downloading data:  20%|█▉        | 53.8M/271M [00:00<00:01, 137MB/s][A
Downloading data:  25%|██▌       | 68.1M/271M [00:00<00:01, 139MB/s][A
Downloading data:  30%|███       | 82.2M/271M [00:00<00:01, 140MB/s][A
Downloading data:  36%|███▌      | 96.5M/271M [00:00<00:01, 141MB/s][A
Downloading data:  41%|████      | 111M/271M [00:00<00:01, 141MB/s] [A
Downloading data:  46%|████▌     | 125M/271M [00:00<00:01, 137MB/s][A
Downloading data:  51%|█████     | 139M/271M [00:01<00:00, 137MB/s][A
Downloading data:  56%|█████▋    | 153M/271M [00:01<00:00, 138MB/s][A
Downloading data:  61%|██████▏   | 167M/271M [00:01<00:00, 135MB/s][A
Downloading data:  66%|██████▋   | 180M/271M [00:01<00:00, 131MB/s][A
Downloading data:  71%|███████   | 193M/271M [00:01<00:00, 130MB/s][A
Downloading data:  76%|███████▋  | 207M/271M [00:01<00:00, 133MB/s][A
Downloading data:  82%|████████▏ | 221M/271M [00:01<00:00, 135MB/s][A
Downloading data:  87%|████████▋ | 235M/271M [00:01<00:00, 137MB/s][A
Downloading data:  92%|█████████▏| 250M/271M [00:01<00:00, 138MB/s][A
Downloading data:  97%|█████████▋| 264M/271M [00:01<00:00, 139MB/s][ADownloading data: 100%|██████████| 271M/271M [00:01<00:00, 136MB/s]
Downloading data files:  33%|███▎      | 1/3 [00:07<00:14,  7.13s/it]
Downloading data:   0%|          | 0.00/31.1M [00:00<?, ?B/s][A
Downloading data:  34%|███▍      | 10.5M/31.1M [00:00<00:00, 105MB/s][A
Downloading data:  77%|███████▋  | 23.9M/31.1M [00:00<00:00, 122MB/s][ADownloading data: 100%|██████████| 31.1M/31.1M [00:00<00:00, 122MB/s]
Downloading data files:  67%|██████▋   | 2/3 [00:07<00:03,  3.19s/it]
Downloading data:   0%|          | 0.00/36.0M [00:00<?, ?B/s][A
Downloading data:  13%|█▎        | 4.83M/36.0M [00:00<00:00, 48.3MB/s][A
Downloading data:  50%|████▉     | 18.0M/36.0M [00:00<00:00, 97.0MB/s][A
Downloading data:  88%|████████▊ | 31.7M/36.0M [00:00<00:00, 115MB/s] [ADownloading data: 100%|██████████| 36.0M/36.0M [00:00<00:00, 109MB/s]
Downloading data files: 100%|██████████| 3/3 [00:08<00:00,  1.96s/it]Downloading data files: 100%|██████████| 3/3 [00:08<00:00,  2.69s/it]
Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 42.07it/s]
Generating train split:   0%|          | 0/287108 [00:00<?, ? examples/s]Generating train split:   3%|▎         | 10000/287108 [00:00<00:05, 49215.06 examples/s]Generating train split:   7%|▋         | 20000/287108 [00:00<00:03, 67026.22 examples/s]Generating train split:  10%|█         | 30000/287108 [00:00<00:03, 77124.55 examples/s]Generating train split:  14%|█▍        | 40000/287108 [00:00<00:02, 83785.17 examples/s]Generating train split:  17%|█▋        | 50000/287108 [00:00<00:02, 85075.83 examples/s]Generating train split:  21%|██        | 60000/287108 [00:00<00:02, 85132.12 examples/s]Generating train split:  24%|██▍       | 70000/287108 [00:00<00:02, 85092.75 examples/s]Generating train split:  28%|██▊       | 80000/287108 [00:00<00:02, 83958.06 examples/s]Generating train split:  31%|███▏      | 90000/287108 [00:01<00:02, 81641.17 examples/s]Generating train split:  37%|███▋      | 105703/287108 [00:01<00:02, 82117.76 examples/s]Generating train split:  40%|████      | 115703/287108 [00:01<00:02, 82712.13 examples/s]Generating train split:  44%|████▍     | 125703/287108 [00:01<00:03, 44164.37 examples/s]Generating train split:  51%|█████     | 145703/287108 [00:02<00:02, 57708.69 examples/s]Generating train split:  54%|█████▍    | 155703/287108 [00:02<00:02, 61491.15 examples/s]Generating train split:  58%|█████▊    | 165703/287108 [00:02<00:01, 66015.05 examples/s]Generating train split:  61%|██████    | 175703/287108 [00:02<00:01, 69433.62 examples/s]Generating train split:  65%|██████▍   | 185703/287108 [00:02<00:01, 73163.67 examples/s]Generating train split:  70%|███████   | 201406/287108 [00:02<00:01, 74068.00 examples/s]Generating train split:  74%|███████▎  | 211406/287108 [00:02<00:00, 76122.13 examples/s]Generating train split:  77%|███████▋  | 221406/287108 [00:03<00:00, 78838.50 examples/s]Generating train split:  81%|████████  | 231406/287108 [00:03<00:00, 80181.04 examples/s]Generating train split:  84%|████████▍ | 241406/287108 [00:03<00:01, 42643.85 examples/s]Generating train split:  88%|████████▊ | 251406/287108 [00:03<00:00, 50533.01 examples/s]Generating train split:  91%|█████████ | 261406/287108 [00:03<00:00, 56900.33 examples/s]Generating train split:  95%|█████████▍| 271406/287108 [00:04<00:00, 64107.37 examples/s]Generating train split:  98%|█████████▊| 281406/287108 [00:04<00:00, 71380.93 examples/s]                                                                                         Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]Generating test split:  87%|████████▋ | 10000/11490 [00:00<00:00, 85443.54 examples/s]                                                                                      Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]Generating validation split:  75%|███████▍  | 10000/13368 [00:00<00:00, 91463.86 examples/s]                                                                                              0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  2.10it/s]100%|██████████| 3/3 [00:00<00:00,  5.73it/s]
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.04k/1.04k [00:00<00:00, 7.15MB/s]
loading configuration file config.json from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/config.json
Model config BartConfig {
  "_name_or_path": "Gabriel/bart-base-cnn-xsum-swe",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "torch_dtype": "float32",
  "transformers_version": "4.23.0",
  "use_cache": true,
  "vocab_size": 50185
}

Downloading (…)okenizer_config.json:   0%|          | 0.00/463 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 463/463 [00:00<00:00, 3.83MB/s]
Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.23M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|██████████| 2.23M/2.23M [00:00<00:00, 156MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 673kB/s]
loading file tokenizer.json from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/tokenizer_config.json
Downloading pytorch_model.bin:   0%|          | 0.00/558M [00:00<?, ?B/s]Downloading pytorch_model.bin:   2%|▏         | 10.5M/558M [00:00<00:08, 67.5MB/s]Downloading pytorch_model.bin:   6%|▌         | 31.5M/558M [00:00<00:04, 116MB/s] Downloading pytorch_model.bin:   9%|▉         | 52.4M/558M [00:00<00:06, 74.2MB/s]Downloading pytorch_model.bin:  13%|█▎        | 73.4M/558M [00:00<00:04, 100MB/s] Downloading pytorch_model.bin:  17%|█▋        | 94.4M/558M [00:01<00:06, 67.3MB/s]Downloading pytorch_model.bin:  19%|█▉        | 105M/558M [00:01<00:07, 60.3MB/s] Downloading pytorch_model.bin:  21%|██        | 115M/558M [00:01<00:07, 61.2MB/s]Downloading pytorch_model.bin:  24%|██▍       | 136M/558M [00:02<00:07, 58.7MB/s]Downloading pytorch_model.bin:  28%|██▊       | 157M/558M [00:02<00:06, 60.4MB/s]Downloading pytorch_model.bin:  34%|███▍      | 189M/558M [00:02<00:05, 69.1MB/s]Downloading pytorch_model.bin:  38%|███▊      | 210M/558M [00:02<00:04, 73.2MB/s]Downloading pytorch_model.bin:  39%|███▉      | 220M/558M [00:03<00:04, 69.7MB/s]Downloading pytorch_model.bin:  43%|████▎     | 241M/558M [00:03<00:04, 68.4MB/s]Downloading pytorch_model.bin:  47%|████▋     | 262M/558M [00:03<00:04, 72.2MB/s]Downloading pytorch_model.bin:  49%|████▉     | 273M/558M [00:03<00:03, 75.3MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 294M/558M [00:04<00:03, 74.3MB/s]Downloading pytorch_model.bin:  56%|█████▋    | 315M/558M [00:04<00:03, 67.6MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 325M/558M [00:04<00:03, 66.1MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 346M/558M [00:05<00:03, 60.0MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 367M/558M [00:05<00:03, 57.3MB/s]Downloading pytorch_model.bin:  71%|███████▏  | 398M/558M [00:05<00:02, 65.8MB/s]Downloading pytorch_model.bin:  75%|███████▌  | 419M/558M [00:06<00:01, 72.9MB/s]Downloading pytorch_model.bin:  79%|███████▉  | 440M/558M [00:06<00:01, 88.9MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 461M/558M [00:06<00:01, 59.2MB/s]Downloading pytorch_model.bin:  85%|████████▍ | 472M/558M [00:07<00:01, 55.0MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 493M/558M [00:07<00:00, 71.9MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 514M/558M [00:07<00:00, 80.0MB/s]Downloading pytorch_model.bin:  96%|█████████▌| 535M/558M [00:07<00:00, 76.6MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 545M/558M [00:07<00:00, 80.3MB/s]Downloading pytorch_model.bin: 100%|█████████▉| 556M/558M [00:08<00:00, 66.3MB/s]Downloading pytorch_model.bin: 100%|██████████| 558M/558M [00:08<00:00, 68.9MB/s]
loading weights file pytorch_model.bin from cache at /home/sshukla7/.cache/huggingface/hub/models--Gabriel--bart-base-cnn-xsum-swe/snapshots/9b280fa682c64893fe4ead88fcb532028677bd9c/pytorch_model.bin
All model checkpoint weights were used when initializing BartForConditionalGeneration.

All the weights of BartForConditionalGeneration were initialized from the model checkpoint at Gabriel/bart-base-cnn-xsum-swe.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
/home/sshukla7/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:649: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'
  warnings.warn(
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.04k/1.04k [00:00<00:00, 4.94MB/s]
Running tokenizer on dataset (num_proc=10):   0%|          | 0/287108 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   0%|          | 1000/287108 [00:03<14:23, 331.43 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   1%|          | 3000/287108 [00:03<04:07, 1148.54 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   1%|▏         | 4000/287108 [00:03<03:11, 1479.83 examples/s]Running tokenizer on dataset (num_proc=10):   2%|▏         | 5000/287108 [00:03<02:19, 2025.61 examples/s]Running tokenizer on dataset (num_proc=10):   3%|▎         | 8000/287108 [00:03<01:08, 4085.45 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   3%|▎         | 9000/287108 [00:04<01:47, 2580.87 examples/s]Running tokenizer on dataset (num_proc=10):   3%|▎         | 10000/287108 [00:05<01:55, 2389.57 examples/s]Running tokenizer on dataset (num_proc=10):   4%|▍         | 11000/287108 [00:05<02:09, 2128.28 examples/s]Running tokenizer on dataset (num_proc=10):   5%|▍         | 13000/287108 [00:06<01:27, 3116.36 examples/s]Running tokenizer on dataset (num_proc=10):   5%|▍         | 14000/287108 [00:07<02:00, 2271.33 examples/s]Running tokenizer on dataset (num_proc=10):   5%|▌         | 15000/287108 [00:07<01:37, 2799.89 examples/s]Running tokenizer on dataset (num_proc=10):   6%|▋         | 18000/287108 [00:08<01:42, 2620.48 examples/s]Running tokenizer on dataset (num_proc=10):   7%|▋         | 19000/287108 [00:08<01:37, 2746.72 examples/s]Running tokenizer on dataset (num_proc=10):   7%|▋         | 20000/287108 [00:08<01:24, 3156.63 examples/s]Running tokenizer on dataset (num_proc=10):   7%|▋         | 21000/287108 [00:09<01:29, 2970.51 examples/s]Running tokenizer on dataset (num_proc=10):   8%|▊         | 22000/287108 [00:09<01:37, 2724.42 examples/s]Running tokenizer on dataset (num_proc=10):   8%|▊         | 23000/287108 [00:10<02:00, 2190.44 examples/s]Running tokenizer on dataset (num_proc=10):   8%|▊         | 24000/287108 [00:10<01:34, 2779.88 examples/s]Running tokenizer on dataset (num_proc=10):   9%|▉         | 27000/287108 [00:10<00:50, 5146.76 examples/s]Running tokenizer on dataset (num_proc=10):  10%|▉         | 28000/287108 [00:11<01:19, 3249.24 examples/s]Running tokenizer on dataset (num_proc=10):  10%|█         | 29000/287108 [00:11<01:17, 3311.86 examples/s]Running tokenizer on dataset (num_proc=10):  11%|█         | 31000/287108 [00:11<01:05, 3932.31 examples/s]Running tokenizer on dataset (num_proc=10):  11%|█         | 32000/287108 [00:13<02:21, 1808.68 examples/s]Running tokenizer on dataset (num_proc=10):  11%|█▏        | 33000/287108 [00:13<01:55, 2202.69 examples/s]Running tokenizer on dataset (num_proc=10):  12%|█▏        | 35000/287108 [00:13<01:15, 3358.19 examples/s]Running tokenizer on dataset (num_proc=10):  13%|█▎        | 37000/287108 [00:14<00:54, 4592.45 examples/s]Running tokenizer on dataset (num_proc=10):  13%|█▎        | 38000/287108 [00:14<00:55, 4467.23 examples/s]Running tokenizer on dataset (num_proc=10):  14%|█▎        | 39000/287108 [00:14<00:58, 4273.50 examples/s]Running tokenizer on dataset (num_proc=10):  14%|█▍        | 40000/287108 [00:14<01:03, 3904.70 examples/s]Running tokenizer on dataset (num_proc=10):  14%|█▍        | 41000/287108 [00:15<00:58, 4238.11 examples/s]Running tokenizer on dataset (num_proc=10):  15%|█▍        | 42000/287108 [00:16<02:46, 1474.89 examples/s]Running tokenizer on dataset (num_proc=10):  15%|█▍        | 43000/287108 [00:17<02:09, 1889.64 examples/s]Running tokenizer on dataset (num_proc=10):  16%|█▌        | 45000/287108 [00:17<01:17, 3136.18 examples/s]Running tokenizer on dataset (num_proc=10):  17%|█▋        | 48000/287108 [00:17<00:47, 5007.70 examples/s]Running tokenizer on dataset (num_proc=10):  17%|█▋        | 50000/287108 [00:18<00:55, 4269.06 examples/s]Running tokenizer on dataset (num_proc=10):  18%|█▊        | 52000/287108 [00:20<01:59, 1972.63 examples/s]Running tokenizer on dataset (num_proc=10):  19%|█▉        | 54000/287108 [00:20<01:27, 2671.37 examples/s]Running tokenizer on dataset (num_proc=10):  20%|█▉        | 57000/287108 [00:20<00:54, 4186.11 examples/s]Running tokenizer on dataset (num_proc=10):  21%|██        | 59000/287108 [00:20<00:44, 5086.11 examples/s]Running tokenizer on dataset (num_proc=10):  21%|██        | 61000/287108 [00:21<00:49, 4594.46 examples/s]Running tokenizer on dataset (num_proc=10):  22%|██▏       | 63000/287108 [00:23<01:44, 2141.28 examples/s]Running tokenizer on dataset (num_proc=10):  23%|██▎       | 65000/287108 [00:23<01:18, 2841.71 examples/s]Running tokenizer on dataset (num_proc=10):  23%|██▎       | 66000/287108 [00:23<01:16, 2908.87 examples/s]Running tokenizer on dataset (num_proc=10):  24%|██▍       | 69000/287108 [00:23<00:48, 4530.48 examples/s]Running tokenizer on dataset (num_proc=10):  25%|██▍       | 71000/287108 [00:24<00:40, 5276.25 examples/s]Running tokenizer on dataset (num_proc=10):  25%|██▌       | 73000/287108 [00:26<01:38, 2180.97 examples/s]Running tokenizer on dataset (num_proc=10):  26%|██▌       | 74000/287108 [00:26<01:24, 2513.29 examples/s]Running tokenizer on dataset (num_proc=10):  26%|██▌       | 75000/287108 [00:26<01:15, 2795.76 examples/s]Running tokenizer on dataset (num_proc=10):  26%|██▋       | 76000/287108 [00:26<01:09, 3019.17 examples/s]Running tokenizer on dataset (num_proc=10):  27%|██▋       | 77000/287108 [00:27<00:59, 3528.24 examples/s]Running tokenizer on dataset (num_proc=10):  27%|██▋       | 78000/287108 [00:27<00:49, 4215.70 examples/s]Running tokenizer on dataset (num_proc=10):  28%|██▊       | 81000/287108 [00:27<00:36, 5577.40 examples/s]Running tokenizer on dataset (num_proc=10):  29%|██▊       | 82000/287108 [00:28<01:24, 2440.84 examples/s]Running tokenizer on dataset (num_proc=10):  29%|██▉       | 83000/287108 [00:29<01:32, 2212.74 examples/s]Running tokenizer on dataset (num_proc=10):  29%|██▉       | 84000/287108 [00:29<01:17, 2626.73 examples/s]Running tokenizer on dataset (num_proc=10):  30%|██▉       | 85000/287108 [00:29<01:07, 2989.61 examples/s]Running tokenizer on dataset (num_proc=10):  30%|███       | 87000/287108 [00:30<01:02, 3216.78 examples/s]Running tokenizer on dataset (num_proc=10):  31%|███       | 89000/287108 [00:30<00:44, 4473.13 examples/s]Running tokenizer on dataset (num_proc=10):  32%|███▏      | 91000/287108 [00:30<00:42, 4619.44 examples/s]Running tokenizer on dataset (num_proc=10):  32%|███▏      | 92000/287108 [00:31<01:03, 3086.97 examples/s]Running tokenizer on dataset (num_proc=10):  32%|███▏      | 93000/287108 [00:32<01:28, 2196.39 examples/s]Running tokenizer on dataset (num_proc=10):  33%|███▎      | 95000/287108 [00:33<01:06, 2894.53 examples/s]Running tokenizer on dataset (num_proc=10):  34%|███▍      | 97000/287108 [00:33<01:03, 3003.77 examples/s]Running tokenizer on dataset (num_proc=10):  34%|███▍      | 98000/287108 [00:33<00:55, 3437.81 examples/s]Running tokenizer on dataset (num_proc=10):  34%|███▍      | 99000/287108 [00:33<00:52, 3571.03 examples/s]Running tokenizer on dataset (num_proc=10):  35%|███▌      | 101000/287108 [00:34<00:49, 3783.66 examples/s]Running tokenizer on dataset (num_proc=10):  36%|███▌      | 102000/287108 [00:34<00:52, 3500.26 examples/s]Running tokenizer on dataset (num_proc=10):  36%|███▌      | 103000/287108 [00:35<01:22, 2218.40 examples/s]Running tokenizer on dataset (num_proc=10):  36%|███▌      | 104000/287108 [00:36<01:12, 2512.83 examples/s]Running tokenizer on dataset (num_proc=10):  37%|███▋      | 105000/287108 [00:36<00:59, 3060.95 examples/s]Running tokenizer on dataset (num_proc=10):  37%|███▋      | 106000/287108 [00:36<00:53, 3398.93 examples/s]Running tokenizer on dataset (num_proc=10):  37%|███▋      | 107000/287108 [00:36<00:51, 3467.45 examples/s]Running tokenizer on dataset (num_proc=10):  38%|███▊      | 108000/287108 [00:37<01:03, 2819.03 examples/s]Running tokenizer on dataset (num_proc=10):  38%|███▊      | 110000/287108 [00:37<00:46, 3776.72 examples/s]Running tokenizer on dataset (num_proc=10):  39%|███▉      | 112000/287108 [00:37<00:39, 4418.06 examples/s]Running tokenizer on dataset (num_proc=10):  39%|███▉      | 113000/287108 [00:39<01:19, 2185.52 examples/s]Running tokenizer on dataset (num_proc=10):  40%|███▉      | 114000/287108 [00:39<01:08, 2532.65 examples/s]Running tokenizer on dataset (num_proc=10):  40%|████      | 116000/287108 [00:39<00:47, 3640.59 examples/s]Running tokenizer on dataset (num_proc=10):  41%|████      | 117000/287108 [00:39<00:52, 3242.89 examples/s]Running tokenizer on dataset (num_proc=10):  41%|████▏     | 119000/287108 [00:40<00:56, 2959.93 examples/s]Running tokenizer on dataset (num_proc=10):  42%|████▏     | 120000/287108 [00:40<00:47, 3486.28 examples/s]Running tokenizer on dataset (num_proc=10):  42%|████▏     | 121000/287108 [00:41<00:45, 3633.79 examples/s]Running tokenizer on dataset (num_proc=10):  42%|████▏     | 122000/287108 [00:41<00:51, 3185.60 examples/s]Running tokenizer on dataset (num_proc=10):  43%|████▎     | 123000/287108 [00:42<01:04, 2534.48 examples/s]Running tokenizer on dataset (num_proc=10):  43%|████▎     | 124000/287108 [00:42<00:58, 2781.96 examples/s]Running tokenizer on dataset (num_proc=10):  44%|████▍     | 126000/287108 [00:42<00:43, 3679.69 examples/s]Running tokenizer on dataset (num_proc=10):  44%|████▍     | 127000/287108 [00:42<00:42, 3738.65 examples/s]Running tokenizer on dataset (num_proc=10):  45%|████▍     | 129000/287108 [00:43<00:59, 2669.18 examples/s]Running tokenizer on dataset (num_proc=10):  46%|████▌     | 131000/287108 [00:44<00:48, 3224.40 examples/s]Running tokenizer on dataset (num_proc=10):  46%|████▋     | 133000/287108 [00:45<00:50, 3040.58 examples/s]Running tokenizer on dataset (num_proc=10):  47%|████▋     | 135000/287108 [00:45<00:38, 3979.15 examples/s]Running tokenizer on dataset (num_proc=10):  47%|████▋     | 136000/287108 [00:45<00:46, 3261.49 examples/s]Running tokenizer on dataset (num_proc=10):  48%|████▊     | 137000/287108 [00:46<00:41, 3618.65 examples/s]Running tokenizer on dataset (num_proc=10):  48%|████▊     | 138000/287108 [00:46<00:40, 3720.00 examples/s]Running tokenizer on dataset (num_proc=10):  48%|████▊     | 139000/287108 [00:47<01:00, 2454.79 examples/s]Running tokenizer on dataset (num_proc=10):  49%|████▉     | 140000/287108 [00:47<00:49, 2992.09 examples/s]Running tokenizer on dataset (num_proc=10):  49%|████▉     | 142000/287108 [00:47<00:38, 3747.31 examples/s]Running tokenizer on dataset (num_proc=10):  50%|████▉     | 143000/287108 [00:47<00:34, 4217.01 examples/s]Running tokenizer on dataset (num_proc=10):  50%|█████     | 144000/287108 [00:48<00:44, 3251.14 examples/s]Running tokenizer on dataset (num_proc=10):  51%|█████     | 146000/287108 [00:49<00:49, 2854.92 examples/s]Running tokenizer on dataset (num_proc=10):  52%|█████▏    | 148000/287108 [00:49<00:42, 3263.86 examples/s]Running tokenizer on dataset (num_proc=10):  52%|█████▏    | 149000/287108 [00:49<00:46, 2968.15 examples/s]Running tokenizer on dataset (num_proc=10):  52%|█████▏    | 150000/287108 [00:50<00:47, 2912.23 examples/s]Running tokenizer on dataset (num_proc=10):  53%|█████▎    | 151000/287108 [00:50<00:39, 3458.74 examples/s]Running tokenizer on dataset (num_proc=10):  53%|█████▎    | 152000/287108 [00:50<00:33, 4037.99 examples/s]Running tokenizer on dataset (num_proc=10):  53%|█████▎    | 153000/287108 [00:50<00:33, 3967.66 examples/s]Running tokenizer on dataset (num_proc=10):  54%|█████▎    | 154000/287108 [00:51<00:37, 3577.52 examples/s]Running tokenizer on dataset (num_proc=10):  54%|█████▍    | 156000/287108 [00:52<00:50, 2606.02 examples/s]Running tokenizer on dataset (num_proc=10):  55%|█████▍    | 157000/287108 [00:52<00:42, 3084.63 examples/s]Running tokenizer on dataset (num_proc=10):  55%|█████▌    | 158000/287108 [00:52<00:43, 2998.67 examples/s]Running tokenizer on dataset (num_proc=10):  56%|█████▌    | 160000/287108 [00:52<00:31, 4016.89 examples/s]Running tokenizer on dataset (num_proc=10):  56%|█████▌    | 161000/287108 [00:53<00:48, 2604.00 examples/s]Running tokenizer on dataset (num_proc=10):  57%|█████▋    | 163000/287108 [00:54<00:34, 3631.84 examples/s]Running tokenizer on dataset (num_proc=10):  57%|█████▋    | 164000/287108 [00:54<00:31, 3911.00 examples/s]Running tokenizer on dataset (num_proc=10):  58%|█████▊    | 166000/287108 [00:55<00:42, 2819.74 examples/s]Running tokenizer on dataset (num_proc=10):  58%|█████▊    | 167000/287108 [00:55<00:41, 2881.52 examples/s]Running tokenizer on dataset (num_proc=10):  59%|█████▉    | 170000/287108 [00:55<00:27, 4217.67 examples/s]Running tokenizer on dataset (num_proc=10):  60%|█████▉    | 171000/287108 [00:57<00:46, 2493.90 examples/s]Running tokenizer on dataset (num_proc=10):  60%|█████▉    | 172000/287108 [00:57<00:39, 2913.60 examples/s]Running tokenizer on dataset (num_proc=10):  61%|██████    | 174000/287108 [00:57<00:27, 4079.00 examples/s]Running tokenizer on dataset (num_proc=10):  61%|██████▏   | 176000/287108 [00:58<00:38, 2859.54 examples/s]Running tokenizer on dataset (num_proc=10):  62%|██████▏   | 179000/287108 [00:58<00:28, 3811.37 examples/s]Running tokenizer on dataset (num_proc=10):  63%|██████▎   | 180000/287108 [00:59<00:28, 3774.74 examples/s]Running tokenizer on dataset (num_proc=10):  63%|██████▎   | 181000/287108 [00:59<00:39, 2654.99 examples/s]Running tokenizer on dataset (num_proc=10):  63%|██████▎   | 182000/287108 [01:00<00:38, 2738.04 examples/s]Running tokenizer on dataset (num_proc=10):  64%|██████▎   | 183000/287108 [01:00<00:32, 3184.24 examples/s]Running tokenizer on dataset (num_proc=10):  64%|██████▍   | 184000/287108 [01:00<00:27, 3816.74 examples/s]Running tokenizer on dataset (num_proc=10):  65%|██████▍   | 186000/287108 [01:01<00:30, 3305.16 examples/s]Running tokenizer on dataset (num_proc=10):  65%|██████▌   | 187000/287108 [01:01<00:29, 3383.13 examples/s]Running tokenizer on dataset (num_proc=10):  66%|██████▌   | 189000/287108 [01:02<00:30, 3238.46 examples/s]Running tokenizer on dataset (num_proc=10):  67%|██████▋   | 191000/287108 [01:03<00:33, 2887.51 examples/s]Running tokenizer on dataset (num_proc=10):  67%|██████▋   | 192000/287108 [01:03<00:34, 2746.48 examples/s]Running tokenizer on dataset (num_proc=10):  67%|██████▋   | 193000/287108 [01:03<00:29, 3139.58 examples/s]Running tokenizer on dataset (num_proc=10):  68%|██████▊   | 194000/287108 [01:03<00:27, 3426.01 examples/s]Running tokenizer on dataset (num_proc=10):  68%|██████▊   | 196000/287108 [01:04<00:20, 4381.42 examples/s]Running tokenizer on dataset (num_proc=10):  69%|██████▊   | 197000/287108 [01:04<00:22, 4039.31 examples/s]Running tokenizer on dataset (num_proc=10):  69%|██████▉   | 198000/287108 [01:04<00:21, 4077.66 examples/s]Running tokenizer on dataset (num_proc=10):  69%|██████▉   | 199000/287108 [01:05<00:34, 2585.18 examples/s]Running tokenizer on dataset (num_proc=10):  70%|██████▉   | 200000/287108 [01:05<00:27, 3157.33 examples/s]Running tokenizer on dataset (num_proc=10):  70%|███████   | 201000/287108 [01:06<00:34, 2518.32 examples/s]Running tokenizer on dataset (num_proc=10):  70%|███████   | 202000/287108 [01:06<00:30, 2773.47 examples/s]Running tokenizer on dataset (num_proc=10):  71%|███████   | 203000/287108 [01:06<00:33, 2537.68 examples/s]Running tokenizer on dataset (num_proc=10):  71%|███████   | 204000/287108 [01:07<00:26, 3152.43 examples/s]Running tokenizer on dataset (num_proc=10):  72%|███████▏  | 206000/287108 [01:07<00:17, 4750.84 examples/s]Running tokenizer on dataset (num_proc=10):  72%|███████▏  | 207000/287108 [01:07<00:15, 5102.69 examples/s]Running tokenizer on dataset (num_proc=10):  72%|███████▏  | 208000/287108 [01:07<00:21, 3678.60 examples/s]Running tokenizer on dataset (num_proc=10):  73%|███████▎  | 209000/287108 [01:08<00:30, 2587.46 examples/s]Running tokenizer on dataset (num_proc=10):  73%|███████▎  | 210000/287108 [01:08<00:28, 2691.86 examples/s]Running tokenizer on dataset (num_proc=10):  73%|███████▎  | 211000/287108 [01:09<00:29, 2604.26 examples/s]Running tokenizer on dataset (num_proc=10):  74%|███████▍  | 212000/287108 [01:09<00:23, 3214.97 examples/s]Running tokenizer on dataset (num_proc=10):  74%|███████▍  | 213000/287108 [01:10<00:30, 2415.68 examples/s]Running tokenizer on dataset (num_proc=10):  75%|███████▍  | 215000/287108 [01:10<00:19, 3723.36 examples/s]Running tokenizer on dataset (num_proc=10):  75%|███████▌  | 216000/287108 [01:10<00:16, 4243.03 examples/s]Running tokenizer on dataset (num_proc=10):  76%|███████▌  | 217000/287108 [01:10<00:14, 4683.42 examples/s]Running tokenizer on dataset (num_proc=10):  76%|███████▌  | 218000/287108 [01:11<00:19, 3536.25 examples/s]Running tokenizer on dataset (num_proc=10):  76%|███████▋  | 219000/287108 [01:11<00:28, 2406.93 examples/s]Running tokenizer on dataset (num_proc=10):  77%|███████▋  | 220000/287108 [01:12<00:26, 2499.53 examples/s]Running tokenizer on dataset (num_proc=10):  77%|███████▋  | 221000/287108 [01:12<00:22, 2968.08 examples/s]Running tokenizer on dataset (num_proc=10):  78%|███████▊  | 223000/287108 [01:13<00:21, 2988.76 examples/s]Running tokenizer on dataset (num_proc=10):  78%|███████▊  | 224000/287108 [01:13<00:19, 3266.86 examples/s]Running tokenizer on dataset (num_proc=10):  78%|███████▊  | 225000/287108 [01:13<00:18, 3380.10 examples/s]Running tokenizer on dataset (num_proc=10):  79%|███████▉  | 227000/287108 [01:13<00:15, 3877.69 examples/s]Running tokenizer on dataset (num_proc=10):  79%|███████▉  | 228000/287108 [01:14<00:15, 3745.27 examples/s]Running tokenizer on dataset (num_proc=10):  80%|███████▉  | 229000/287108 [01:15<00:23, 2503.26 examples/s]Running tokenizer on dataset (num_proc=10):  80%|████████  | 230000/287108 [01:15<00:20, 2799.33 examples/s]Running tokenizer on dataset (num_proc=10):  80%|████████  | 231000/287108 [01:15<00:17, 3186.96 examples/s]Running tokenizer on dataset (num_proc=10):  81%|████████  | 232000/287108 [01:15<00:16, 3322.09 examples/s]Running tokenizer on dataset (num_proc=10):  81%|████████  | 233000/287108 [01:15<00:14, 3643.94 examples/s]Running tokenizer on dataset (num_proc=10):  82%|████████▏ | 234000/287108 [01:16<00:14, 3783.58 examples/s]Running tokenizer on dataset (num_proc=10):  82%|████████▏ | 235000/287108 [01:16<00:13, 3970.33 examples/s]Running tokenizer on dataset (num_proc=10):  82%|████████▏ | 236000/287108 [01:16<00:14, 3421.34 examples/s]Running tokenizer on dataset (num_proc=10):  83%|████████▎ | 237000/287108 [01:17<00:16, 3050.75 examples/s]Running tokenizer on dataset (num_proc=10):  83%|████████▎ | 238000/287108 [01:17<00:13, 3636.99 examples/s]Running tokenizer on dataset (num_proc=10):  83%|████████▎ | 239000/287108 [01:18<00:21, 2281.82 examples/s]Running tokenizer on dataset (num_proc=10):  84%|████████▎ | 240000/287108 [01:18<00:17, 2709.32 examples/s]Running tokenizer on dataset (num_proc=10):  84%|████████▍ | 241000/287108 [01:18<00:17, 2703.07 examples/s]Running tokenizer on dataset (num_proc=10):  84%|████████▍ | 242000/287108 [01:18<00:14, 3218.11 examples/s]Running tokenizer on dataset (num_proc=10):  85%|████████▍ | 243000/287108 [01:19<00:11, 3892.32 examples/s]Running tokenizer on dataset (num_proc=10):  85%|████████▍ | 244000/287108 [01:19<00:09, 4507.76 examples/s]Running tokenizer on dataset (num_proc=10):  85%|████████▌ | 245000/287108 [01:19<00:10, 4188.81 examples/s]Running tokenizer on dataset (num_proc=10):  86%|████████▌ | 246000/287108 [01:20<00:14, 2915.50 examples/s]Running tokenizer on dataset (num_proc=10):  86%|████████▌ | 247000/287108 [01:20<00:16, 2374.12 examples/s]Running tokenizer on dataset (num_proc=10):  86%|████████▋ | 248000/287108 [01:20<00:13, 2997.17 examples/s]Running tokenizer on dataset (num_proc=10):  87%|████████▋ | 248711/287108 [01:21<00:13, 2821.24 examples/s]Running tokenizer on dataset (num_proc=10):  87%|████████▋ | 249711/287108 [01:21<00:12, 2933.49 examples/s]Running tokenizer on dataset (num_proc=10):  87%|████████▋ | 250711/287108 [01:21<00:10, 3486.67 examples/s]Running tokenizer on dataset (num_proc=10):  88%|████████▊ | 252711/287108 [01:21<00:08, 3996.35 examples/s]Running tokenizer on dataset (num_proc=10):  89%|████████▊ | 254711/287108 [01:22<00:10, 2984.19 examples/s]Running tokenizer on dataset (num_proc=10):  89%|████████▉ | 255711/287108 [01:23<00:13, 2371.17 examples/s]Running tokenizer on dataset (num_proc=10):  89%|████████▉ | 256711/287108 [01:23<00:11, 2605.23 examples/s]Running tokenizer on dataset (num_proc=10):  90%|████████▉ | 257711/287108 [01:23<00:09, 3176.60 examples/s]Running tokenizer on dataset (num_proc=10):  90%|█████████ | 258711/287108 [01:24<00:11, 2514.76 examples/s]Running tokenizer on dataset (num_proc=10):  90%|█████████ | 259711/287108 [01:24<00:10, 2558.99 examples/s]Running tokenizer on dataset (num_proc=10):  92%|█████████▏| 262711/287108 [01:25<00:04, 5053.09 examples/s]Running tokenizer on dataset (num_proc=10):  92%|█████████▏| 264711/287108 [01:26<00:10, 2237.69 examples/s]Running tokenizer on dataset (num_proc=10):  93%|█████████▎| 266422/287108 [01:27<00:07, 2738.16 examples/s]Running tokenizer on dataset (num_proc=10):  93%|█████████▎| 268422/287108 [01:27<00:06, 3050.69 examples/s]Running tokenizer on dataset (num_proc=10):  94%|█████████▍| 269422/287108 [01:28<00:05, 3042.73 examples/s]Running tokenizer on dataset (num_proc=10):  94%|█████████▍| 270422/287108 [01:28<00:04, 3377.03 examples/s]Running tokenizer on dataset (num_proc=10):  95%|█████████▍| 272422/287108 [01:29<00:06, 2385.66 examples/s]Running tokenizer on dataset (num_proc=10):  95%|█████████▌| 273132/287108 [01:29<00:05, 2361.11 examples/s]Running tokenizer on dataset (num_proc=10):  95%|█████████▌| 273843/287108 [01:30<00:06, 2195.74 examples/s]Running tokenizer on dataset (num_proc=10):  96%|█████████▌| 275843/287108 [01:30<00:03, 2996.30 examples/s]Running tokenizer on dataset (num_proc=10):  97%|█████████▋| 277843/287108 [01:31<00:03, 2529.68 examples/s]Running tokenizer on dataset (num_proc=10):  97%|█████████▋| 279843/287108 [01:32<00:03, 2199.17 examples/s]Running tokenizer on dataset (num_proc=10):  98%|█████████▊| 280554/287108 [01:32<00:02, 2372.78 examples/s]Running tokenizer on dataset (num_proc=10):  98%|█████████▊| 281265/287108 [01:32<00:02, 2669.07 examples/s]Running tokenizer on dataset (num_proc=10):  98%|█████████▊| 282265/287108 [01:33<00:02, 2305.58 examples/s]Running tokenizer on dataset (num_proc=10):  99%|█████████▊| 283265/287108 [01:34<00:02, 1474.18 examples/s]Running tokenizer on dataset (num_proc=10):  99%|█████████▉| 284265/287108 [01:35<00:01, 1901.62 examples/s]Running tokenizer on dataset (num_proc=10): 100%|█████████▉| 285687/287108 [01:35<00:00, 1804.29 examples/s]Running tokenizer on dataset (num_proc=10): 100%|█████████▉| 286398/287108 [01:37<00:00, 1148.87 examples/s]Running tokenizer on dataset (num_proc=10): 100%|██████████| 287108/287108 [01:37<00:00, 1405.60 examples/s]                                                                                                            Running tokenizer on dataset (num_proc=10):   0%|          | 0/11490 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   9%|▊         | 1000/11490 [00:02<00:28, 369.77 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  17%|█▋        | 2000/11490 [00:02<00:11, 808.80 examples/s]Running tokenizer on dataset (num_proc=10):  27%|██▋       | 3149/11490 [00:03<00:05, 1420.12 examples/s]Running tokenizer on dataset (num_proc=10):  37%|███▋      | 4298/11490 [00:03<00:03, 2058.97 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  49%|████▊     | 5596/11490 [00:03<00:02, 2054.31 examples/s]Running tokenizer on dataset (num_proc=10):  66%|██████▌   | 7596/11490 [00:04<00:01, 3387.40 examples/s]Running tokenizer on dataset (num_proc=10):  84%|████████▎ | 9596/11490 [00:04<00:00, 5014.89 examples/s]Running tokenizer on dataset (num_proc=10):  94%|█████████▎| 10745/11490 [00:04<00:00, 4917.78 examples/s]                                                                                                          Running tokenizer on dataset (num_proc=10):   0%|          | 0/13368 [00:00<?, ? examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):   7%|▋         | 1000/13368 [00:02<00:32, 376.96 examples/s]Running tokenizer on dataset (num_proc=10):  22%|██▏       | 3000/13368 [00:02<00:08, 1258.94 examples/s]/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Running tokenizer on dataset (num_proc=10):  32%|███▏      | 4337/13368 [00:03<00:05, 1562.64 examples/s]Running tokenizer on dataset (num_proc=10):  42%|████▏     | 5674/13368 [00:03<00:03, 2132.99 examples/s]Running tokenizer on dataset (num_proc=10):  72%|███████▏  | 9674/13368 [00:03<00:00, 4903.96 examples/s]Running tokenizer on dataset (num_proc=10):  85%|████████▍ | 11348/13368 [00:04<00:00, 3767.78 examples/s]Running tokenizer on dataset (num_proc=10):  92%|█████████▏| 12359/13368 [00:04<00:00, 3505.34 examples/s]Running tokenizer on dataset (num_proc=10): 100%|██████████| 13368/13368 [00:05<00:00, 4015.77 examples/s]                                                                                                          /home/sshukla7/.local/lib/python3.10/site-packages/accelerate/accelerator.py:498: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
/home/sshukla7/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnSwedish.py:683: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("rouge")
  0%|          | 0/179460 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnSwedish.py", line 961, in <module>
    main()
  File "/home/sshukla7/sshukla7/DQ_BART/run_summarization_no_trainer_cnnSwedish.py", line 742, in main
    student_outputs = student_model(**batch, output_attentions=True, output_hidden_states=True)
  File "/opt/sw/spack/apps/linux-rhel8-x86_64/gcc-10.3.0/python-3.10.1-qb/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: BartForConditionalGeneration.forward() got an unexpected keyword argument 'token_type_ids'
wandb: Waiting for W&B process to finish... (failed 1).
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/sshukla7/sshukla7/DQ_BART/wandb/offline-run-20230502_222406-1bebglno
wandb: Find logs at: ./wandb/offline-run-20230502_222406-1bebglno/logs
